<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>model_selection_slides</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            max-width: 1200px;
            margin: 0 auto;
            padding: 20px;
            background-color: #f5f5f5;
        }
        .page {
            background-color: white;
            margin: 20px 0;
            padding: 40px;
            box-shadow: 0 2px 8px rgba(0,0,0,0.1);
            border-radius: 4px;
            min-height: 400px;
        }
        .page-number {
            color: #666;
            font-size: 14px;
            margin-bottom: 20px;
            border-bottom: 1px solid #ddd;
            padding-bottom: 10px;
        }
        h1, h2, h3 {
            color: #333;
        }
        .page-content {
            line-height: 1.8;
            white-space: pre-wrap;
            font-family: 'Courier New', monospace;
            font-size: 12pt;
        }
        .note {
            color: #666;
            font-style: italic;
            margin: 10px 0;
            padding: 10px;
            background-color: #f9f9f9;
            border-left: 3px solid #ccc;
        }
    </style>
</head>
<body>
<div class="note">
    This is a text-only version of the slides. For the full visual experience with images and formatting, please refer to the PDF version.
</div>

<div class="page">
<div class="page-number">Page 1 of 45</div>
<div class="page-content">
Model Selection and Evaluation
CMSC 173 - Machine Learning
Course Lecture
1

</div>
</div>
<div class="page">
<div class="page-number">Page 2 of 45</div>
<div class="page-content">
Outline
Introduction to Model Selection
Bias-Variance Decomposition
Model Validation and Evaluation
Evaluation Metrics
Regularization
Best Practices and Common Pitfalls
Summary and Key Takeaways
2

</div>
</div>
<div class="page">
<div class="page-number">Page 3 of 45</div>
<div class="page-content">
Introduction to Model Selection

</div>
</div>
<div class="page">
<div class="page-number">Page 4 of 45</div>
<div class="page-content">
The Model Selection Problem
Central Question: How do we choose the best model?
Challenges
• Multiple algorithms available
• Different hyperparameters
• Trade-offs between complexity and performance
• Avoiding overfitting
• Generalization to unseen data
Goals
• Select optimal model architecture
• Tune hyperparameters effectively
• Ensure reliable performance
• Balance bias and variance
• Maximize generalization
Key Insight
Model selection is not just about training performance, but about how well the model generalizes to new,
unseen data.
3

</div>
</div>
<div class="page">
<div class="page-number">Page 5 of 45</div>
<div class="page-content">
Model Selection Pipeline
Data
Train/Val/Test Split
Train Models
Validate &amp; Select
Test Final Model
• Split: Divide data into training, validation, and test sets
• Train: Fit multiple candidate models
• Validate: Compare models on validation set
• Select: Choose best performing model
• Test: Final evaluation on held-out test set
4

</div>
</div>
<div class="page">
<div class="page-number">Page 6 of 45</div>
<div class="page-content">
Train-Validation-Test Split
Training Set
• Model fitting
• Learning parameters
• 60-70% of data
Validation Set
• Model selection
• Hyperparameter tuning
• 15-20% of data
Test Set
• Final evaluation
• Unbiased estimate
• 15-20% of data
5

</div>
</div>
<div class="page">
<div class="page-number">Page 7 of 45</div>
<div class="page-content">
Bias-Variance Decomposition

</div>
</div>
<div class="page">
<div class="page-number">Page 8 of 45</div>
<div class="page-content">
Understanding Prediction Error
For a regression problem, the expected prediction error can be decomposed:
Error Decomposition
E[(y −ˆf (x))2] = Bias2[ˆf (x)] + Var[ˆf (x)] + σ2
Bias
Bias[ˆf ] = E[ˆf ] −f
Error from wrong assumptions in
the learning algorithm
Variance
Var[ˆf ] = E[(ˆf −E[ˆf ])2]
Error from sensitivity to training
set variations
Irreducible Error
σ2 = Var[ϵ]
Noise in the data that cannot be
reduced
6

</div>
</div>
<div class="page">
<div class="page-number">Page 9 of 45</div>
<div class="page-content">
The Bias-Variance Tradeoff
Key Insight
• As model complexity increases, bias decreases but variance increases
• The optimal model minimizes the total error (bias2 + variance)
• There exists a sweet spot that balances both sources of error
7

</div>
</div>
<div class="page">
<div class="page-number">Page 10 of 45</div>
<div class="page-content">
High Bias vs High Variance
High Bias (Underfitting)
Characteristics:
• Overly simple model
• Poor training performance
• Poor test performance
• Cannot capture data patterns
Solutions:
• Increase model complexity
• Add more features
• Reduce regularization
• Train longer
High Variance (Overfitting)
Characteristics:
• Overly complex model
• Excellent training performance
• Poor test performance
• Memorizes training data
Solutions:
• Simplify model
• Get more training data
• Increase regularization
• Use early stopping
8

</div>
</div>
<div class="page">
<div class="page-number">Page 11 of 45</div>
<div class="page-content">
Visualizing Underfitting and Overfitting
• Left: Underfitting - linear model cannot capture nonlinear relationship
• Center: Good fit - balanced complexity captures true pattern
• Right: Overfitting - high-degree polynomial fits noise
9

</div>
</div>
<div class="page">
<div class="page-number">Page 12 of 45</div>
<div class="page-content">
Model Complexity and Error
Observations
• Training error decreases monotonically with complexity
• Validation error has a U-shaped curve
• Gap between curves indicates overfitting
• Optimal complexity minimizes validation error
10

</div>
</div>
<div class="page">
<div class="page-number">Page 13 of 45</div>
<div class="page-content">
Model Validation and Evaluation

</div>
</div>
<div class="page">
<div class="page-number">Page 14 of 45</div>
<div class="page-content">
Why Do We Need Validation?
The Fundamental Problem
We cannot evaluate model performance on the same data used
for training!
Training Error is Optimistic
• Model has seen the training data
• Can memorize patterns and noise
• Does not reflect generalization
• Always decreases with complexity
Validation Error is Realistic
• Model has not seen validation data
• Measures true generalization
• Enables fair model comparison
• Guides hyperparameter selection
11

</div>
</div>
<div class="page">
<div class="page-number">Page 15 of 45</div>
<div class="page-content">
Learning Curves
• Underfitting: Both errors high, converge to high value
• Well-fitted: Both errors low, small gap between them
• Overfitting: Large gap between training and validation error
12

</div>
</div>
<div class="page">
<div class="page-number">Page 16 of 45</div>
<div class="page-content">
Cross-Validation: Motivation
Problem with Single Train-Val Split
• Results depend on random split
• Some data points never used for training
• Some never used for validation
• High variance in performance estimates
Cross-Validation Solution
• Use multiple train-validation splits
• Every data point used for both training and validation
• Average results across splits for robust estimate
• Reduces variance in performance evaluation
13

</div>
</div>
<div class="page">
<div class="page-number">Page 17 of 45</div>
<div class="page-content">
Cross-Validation Schemes
K-Fold CV
Stratified K-Fold
14

</div>
</div>
<div class="page">
<div class="page-number">Page 18 of 45</div>
<div class="page-content">
K-Fold Cross-Validation Algorithm
Algorithm 1 K-Fold Cross-Validation
Require: Dataset D, Model M, Number of folds K
Ensure: Cross-validation score
1: Randomly partition D into K equal-sized subsets D1, D2, . . . , DK
2: Initialize scores = []
3: for i = 1 to K do
4:
Dval ←Di
5:
Dtrain ←D \ Di
6:
Train model ˆ
M on Dtrain
7:
si ←Evaluate( ˆ
M, Dval)
8:
Append si to scores
9: end for
10: return
1
K
PK
i=1 si
Common Choices
K = 5 or K = 10 are typical values balancing computational cost and variance reduction.
15

</div>
</div>
<div class="page">
<div class="page-number">Page 19 of 45</div>
<div class="page-content">
Validation Curve
Using Validation Curves
• Plot training and validation scores vs. hyperparameter values
• Identify optimal hyperparameter setting
• Diagnose underfitting and overfitting regions
• Select model with best validation performance
16

</div>
</div>
<div class="page">
<div class="page-number">Page 20 of 45</div>
<div class="page-content">
Evaluation Metrics

</div>
</div>
<div class="page">
<div class="page-number">Page 21 of 45</div>
<div class="page-content">
Classification Metrics: Confusion Matrix
Definitions
• TP: True Positives
• TN: True Negatives
• FP: False Positives (Type I error)
• FN: False Negatives (Type II error)
Key Metrics
Accuracy =
TP + TN
TP + TN + FP + FN
Precision =
TP
TP + FP
Recall =
TP
TP + FN
17

</div>
</div>
<div class="page">
<div class="page-number">Page 22 of 45</div>
<div class="page-content">
Classification Metrics Comparison
Accuracy
Overall correctness; can be misleading with
imbalanced classes
F1-Score
Harmonic mean of precision and recall:
F1 = 2·Precision·Recall
Precision+Recall
18

</div>
</div>
<div class="page">
<div class="page-number">Page 23 of 45</div>
<div class="page-content">
ROC Curve and AUC
ROC Curve
• Plots TPR vs FPR
• Shows performance across thresholds
• Diagonal = random classifier
• Upper-left corner = perfect
AUC Score
• Area Under ROC Curve
• Range: [0, 1]
• 0.5 = random
• 1.0 = perfect
• Threshold-independent
19

</div>
</div>
<div class="page">
<div class="page-number">Page 24 of 45</div>
<div class="page-content">
Precision-Recall Curve
When to Use
• Imbalanced datasets
• Care about positive class
• False positives costly
• Alternative to ROC
Interpretation
• High area = good performance
• Trade-off between precision and recall
• Choose threshold based on application
needs
20

</div>
</div>
<div class="page">
<div class="page-number">Page 25 of 45</div>
<div class="page-content">
Regression Metrics
Mean Squared Error (MSE)
MSE = 1
n
n
X
i=1
(yi −ˆyi)2
• Penalizes large errors heavily
• Same units as y2
• Always non-negative
• Lower is better
Root Mean Squared Error
RMSE =
√
MSE
• Same units as y
• More interpretable than MSE
Mean Absolute Error (MAE)
MAE = 1
n
n
X
i=1
|yi −ˆyi|
• Robust to outliers
• Same units as y
• Easy to interpret
R-Squared (R2)
R2 = 1 −
P(yi −ˆyi)2
P(yi −¯y)2
• Proportion of variance explained
• Range: (−∞, 1]
• 1 = perfect predictions
21

</div>
</div>
<div class="page">
<div class="page-number">Page 26 of 45</div>
<div class="page-content">
Regularization

</div>
</div>
<div class="page">
<div class="page-number">Page 27 of 45</div>
<div class="page-content">
What is Regularization?
Definition
Regularization is a technique to prevent overfitting by adding a penalty term to the loss function that
discourages complex models.
General Form
Lossregularized = Lossdata + λ · Penalty(parameters)
where λ ≥0 is the regularization parameter controlling the strength of regularization.
Benefits
• Reduces overfitting
• Improves generalization
• Encourages simpler models
• Can perform feature selection
Trade-off
• λ too small: underfitting
• λ too large: underfitting
• Must tune λ via validation
22

</div>
</div>
<div class="page">
<div class="page-number">Page 28 of 45</div>
<div class="page-content">
Ridge Regression (L2 Regularization)
Objective Function
min
w
n
X
i=1
(yi −wT xi)2 + λ∥w∥2
2
where ∥w∥2
2 = Pd
j=1 w2
j is the L2 norm.
Characteristics
• Shrinks coefficients towards zero
• Does not set coefficients exactly to zero
• Has closed-form solution
• Stable and computationally efficient
• Preferred when all features are relevant
Solution
ˆw = (XT X + λI)−1XT y
23

</div>
</div>
<div class="page">
<div class="page-number">Page 29 of 45</div>
<div class="page-content">
Lasso Regression (L1 Regularization)
Objective Function
min
w
n
X
i=1
(yi −wT xi)2 + λ∥w∥1
where ∥w∥1 = Pd
j=1 |wj| is the L1 norm.
Characteristics
• Can set coefficients exactly to zero
• Performs automatic feature selection
• Produces sparse models
• No closed-form solution (use optimization)
• Preferred with many irrelevant features
Sparsity Property
Lasso’s ability to zero out coefficients makes it ideal for
interpretable models and high-dimensional data.
24

</div>
</div>
<div class="page">
<div class="page-number">Page 30 of 45</div>
<div class="page-content">
L1 vs L2 Regularization: Geometric Interpretation
• L2 (Ridge): Circular constraint region - solution rarely at axes (non-sparse)
• L1 (Lasso): Diamond constraint region - corners encourage sparse solutions
• Contours represent loss function, constraint region represents penalty
25

</div>
</div>
<div class="page">
<div class="page-number">Page 31 of 45</div>
<div class="page-content">
Regularization Paths
Observations
• Ridge: Coefficients shrink smoothly but never reach exactly zero
• Lasso: Coefficients can become exactly zero at finite λ
• As λ →∞, all coefficients approach zero
• Different coefficients zero out at different λ values in Lasso
26

</div>
</div>
<div class="page">
<div class="page-number">Page 32 of 45</div>
<div class="page-content">
Elastic Net: Combining L1 and L2
Objective Function
min
w
n
X
i=1
(yi −wT xi)2 + λ1∥w∥1 + λ2∥w∥2
2
Alternatively parameterized with mixing parameter α ∈[0, 1]:
Penalty = λ

α∥w∥1 + (1 −α)∥w∥2
2

Advantages
• Combines benefits of Ridge and Lasso
• Handles correlated features better than Lasso
• Can select groups of correlated features
• More stable than Lasso
When to Use
• Many correlated features
• Want feature selection and grouping
• Lasso is too aggressive
• Ridge is not sparse enough
27

</div>
</div>
<div class="page">
<div class="page-number">Page 33 of 45</div>
<div class="page-content">
Comparing Regularization Methods
• All methods converge to similar training error with strong regularization
• Test error differences reveal generalization capabilities
• Optimal λ differs across methods
28

</div>
</div>
<div class="page">
<div class="page-number">Page 34 of 45</div>
<div class="page-content">
Regularization in Other Models
Neural Networks
• Weight decay: L2 penalty on weights
• Dropout: Randomly drop neurons during
training
• Early stopping: Stop training before overfitting
• Batch normalization: Normalize activations
Support Vector Machines
• C parameter controls regularization
• Small C = strong regularization
• Large C = weak regularization
Decision Trees/Forests
• Max depth
• Min samples per leaf
• Max number of features
• Pruning
General Strategies
• Data augmentation
• Feature selection
• Ensemble methods
• Cross-validation for tuning
29

</div>
</div>
<div class="page">
<div class="page-number">Page 35 of 45</div>
<div class="page-content">
Best Practices and Common Pitfalls

</div>
</div>
<div class="page">
<div class="page-number">Page 36 of 45</div>
<div class="page-content">
Model Selection Best Practices
Do’s
• Always use separate train/validation/test sets
• Use cross-validation for robust estimates
• Tune hyperparameters only on validation data
• Report final performance on test set (once!)
• Standardize/normalize features appropriately
• Use stratified splits for classification
• Track both training and validation metrics
• Document all preprocessing steps
30

</div>
</div>
<div class="page">
<div class="page-number">Page 37 of 45</div>
<div class="page-content">
Common Pitfalls to Avoid
Don’ts
• Data leakage: Including test data in preprocessing
• Peeking at test set: Multiple evaluations on test set
• Ignoring class imbalance: Using accuracy on imbalanced data
• Not checking assumptions: Assuming i.i.d. data
• Overfitting validation set: Excessive hyperparameter tuning
• Cherry-picking results: Reporting only best-case performance
• Inadequate splitting: Too small validation/test sets
• Comparing on training data: Always compare on validation
31

</div>
</div>
<div class="page">
<div class="page-number">Page 38 of 45</div>
<div class="page-content">
Data Leakage: A Critical Issue
What is Data Leakage?
Information from the test/validation set leaking into the training process, leading to overly optimistic
performance estimates.
Common Sources
• Normalization using all data
• Feature selection on all data
• Imputation using all data
• Temporal data ordering issues
• Duplicate samples across splits
Prevention
• Split data FIRST
• Fit preprocessing only on training
• Transform validation/test separately
• Use pipelines
• Be careful with time series
Example: Correct Order
1. Split data →2. Fit scaler on train →3. Transform train/val/test →4. Train model
32

</div>
</div>
<div class="page">
<div class="page-number">Page 39 of 45</div>
<div class="page-content">
Hyperparameter Tuning Strategies
Grid Search
• Exhaustive search over grid
• Guarantees finding best in grid
• Exponential in # parameters
• Good for few parameters
Random Search
• Randomly sample combinations
• Often finds good solutions faster
• Better for many parameters
• Can set computational budget
Bayesian Optimization
• Models objective function
• Guides search intelligently
• Most sample-efficient
• Good for expensive models
Practical Tips
• Start with coarse grid
• Refine around best values
• Use log scale for λ
• Parallelize when possible
33

</div>
</div>
<div class="page">
<div class="page-number">Page 40 of 45</div>
<div class="page-content">
Nested Cross-Validation
Problem
Using CV for both model selection and performance
estimation gives biased results!
Solution: Nested CV
• Outer loop: Estimates true performance
• Inner loop: Selects hyperparameters
• Provides unbiased performance estimate
• More computationally expensive
Structure
For each outer fold:
1. Set aside test fold
2. Use inner CV to select hyperparameters
3. Train final model with best hyperparameters
4. Evaluate on test fold
Average outer fold results
Outer Fold 1
Outer Fold 2
Outer Fold 3
Inner 1
Inner 2
Inner 3
Outer
Inner
34

</div>
</div>
<div class="page">
<div class="page-number">Page 41 of 45</div>
<div class="page-content">
Model Selection Checklist
Before Training
□Understand the problem and data
□Check for class imbalance
□Handle missing values
□Split data properly
□Standardize/normalize features
□Choose appropriate metrics
During Training
□Use cross-validation
□Track train and validation metrics
□Try multiple model types
□Tune hyperparameters systematically
□Check for overfitting
After Training
□Evaluate on test set (once!)
□Compare multiple metrics
□Analyze errors/confusion matrix
□Check for biases
□Document results
□Assess computational requirements
Golden Rule
Never touch the test set until final evaluation, and
evaluate on it only once!
35

</div>
</div>
<div class="page">
<div class="page-number">Page 42 of 45</div>
<div class="page-content">
Summary and Key Takeaways

</div>
</div>
<div class="page">
<div class="page-number">Page 43 of 45</div>
<div class="page-content">
Summary: Key Concepts
1. Bias-Variance Tradeoff
• Balance between model complexity and generalization
• Underfitting (high bias) vs Overfitting (high variance)
2. Model Validation
• Always use separate train/validation/test sets
• Cross-validation provides robust performance estimates
• Learning curves diagnose fitting issues
3. Evaluation Metrics
• Choose metrics appropriate for the problem
• Classification: accuracy, precision, recall, F1, ROC-AUC
• Regression: MSE, RMSE, MAE, R2
4. Regularization
• Ridge (L2): shrinks coefficients, keeps all features
• Lasso (L1): feature selection via sparsity
• Elastic Net: combines L1 and L2
36

</div>
</div>
<div class="page">
<div class="page-number">Page 44 of 45</div>
<div class="page-content">
Key Takeaways
Critical Principles
• Generalization is the goal - training performance is not enough
• Avoid data leakage - fit preprocessing only on training data
• Use proper validation - cross-validation for robust estimates
• Test set is sacred - evaluate on it only once at the end
• Choose appropriate metrics - align with business/research goals
• Regularize when needed - prevent overfitting proactively
• Document everything - ensure reproducibility
Next Steps
Practice model selection and evaluation on real datasets using cross-validation, regularization, and proper
evaluation protocols.
37

</div>
</div>
<div class="page">
<div class="page-number">Page 45 of 45</div>
<div class="page-content">
Additional Resources
Textbooks
• Hastie, Tibshirani, Friedman - The Elements of Statistical Learning
• Bishop - Pattern Recognition and Machine Learning
• James et al. - An Introduction to Statistical Learning
Online Resources
• scikit-learn documentation: Model selection and evaluation
• Coursera: Machine Learning by Andrew Ng
• Fast.ai: Practical Deep Learning for Coders
Thank you!
38

</div>
</div>

</body>
</html>
