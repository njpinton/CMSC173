{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kernel Methods Tutorial\n",
    "## CMSC 173 - Machine Learning\n",
    "\n",
    "This notebook accompanies the kernel methods slides and provides hands-on implementation and visualization of key concepts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.svm import SVC, SVR\n",
    "from sklearn.datasets import make_classification, make_regression, make_circles, make_moons\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.multiclass import OneVsRestClassifier, OneVsOneClassifier\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.style.use('seaborn-v0_8')\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction & Motivation\n",
    "\n",
    "Kernel methods solve the fundamental problem of learning non-linear patterns in data by implicitly mapping features to higher-dimensional spaces. This section demonstrates why we need kernel methods through visualization of linear vs non-linear separability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create linearly separable data\n",
    "X_linear, y_linear = make_classification(n_samples=200, n_features=2, n_redundant=0, \n",
    "                                        n_informative=2, n_clusters_per_class=1, \n",
    "                                        class_sep=2, random_state=42)\n",
    "\n",
    "# Create non-linearly separable data\n",
    "X_circles, y_circles = make_circles(n_samples=200, noise=0.1, factor=0.3, random_state=42)\n",
    "X_moons, y_moons = make_moons(n_samples=200, noise=0.1, random_state=42)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Plot linearly separable data\n",
    "axes[0].scatter(X_linear[y_linear==0, 0], X_linear[y_linear==0, 1], c='red', alpha=0.7, label='Class 0')\n",
    "axes[0].scatter(X_linear[y_linear==1, 0], X_linear[y_linear==1, 1], c='blue', alpha=0.7, label='Class 1')\n",
    "axes[0].set_title('Linearly Separable')\n",
    "axes[0].legend()\n",
    "\n",
    "# Plot circles data\n",
    "axes[1].scatter(X_circles[y_circles==0, 0], X_circles[y_circles==0, 1], c='red', alpha=0.7, label='Class 0')\n",
    "axes[1].scatter(X_circles[y_circles==1, 0], X_circles[y_circles==1, 1], c='blue', alpha=0.7, label='Class 1')\n",
    "axes[1].set_title('Circles (Non-linear)')\n",
    "axes[1].legend()\n",
    "\n",
    "# Plot moons data\n",
    "axes[2].scatter(X_moons[y_moons==0, 0], X_moons[y_moons==0, 1], c='red', alpha=0.7, label='Class 0')\n",
    "axes[2].scatter(X_moons[y_moons==1, 0], X_moons[y_moons==1, 1], c='blue', alpha=0.7, label='Class 1')\n",
    "axes[2].set_title('Moons (Non-linear)')\n",
    "axes[2].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Support Vector Machines - Linear Case\n",
    "\n",
    "Support Vector Machines find the optimal decision boundary by maximizing the margin between classes. This section demonstrates the fundamental SVM concepts using linearly separable data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_svm_decision_boundary(X, y, model, title):\n",
    "    \"\"\"Plot SVM decision boundary with support vectors highlighted.\"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Create mesh for decision boundary\n",
    "    h = 0.02\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "    \n",
    "    # Predict on mesh\n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    # Plot decision boundary\n",
    "    plt.contourf(xx, yy, Z, alpha=0.3, cmap=plt.cm.RdYlBu)\n",
    "    \n",
    "    # Plot data points\n",
    "    scatter = plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.RdYlBu, edgecolors='black')\n",
    "    \n",
    "    # Highlight support vectors\n",
    "    plt.scatter(model.support_vectors_[:, 0], model.support_vectors_[:, 1],\n",
    "                s=100, facecolors='none', edgecolors='green', linewidths=2,\n",
    "                label=f'Support Vectors ({len(model.support_vectors_)})')\n",
    "    \n",
    "    plt.title(f'{title} - Margin: {2/np.linalg.norm(model.coef_):.3f}')\n",
    "    plt.xlabel('Feature 1')\n",
    "    plt.ylabel('Feature 2')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Train linear SVM on linearly separable data\n",
    "svm_linear = SVC(kernel='linear', C=1.0)\n",
    "svm_linear.fit(X_linear, y_linear)\n",
    "\n",
    "plot_svm_decision_boundary(X_linear, y_linear, svm_linear, 'Linear SVM')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Hard vs Soft Margin\n",
    "\n",
    "Real-world data is rarely perfectly separable. Soft margin SVM allows some misclassifications by introducing slack variables, controlled by the regularization parameter C."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add noise to make data not perfectly separable\n",
    "X_noisy = X_linear + np.random.normal(0, 0.3, X_linear.shape)\n",
    "\n",
    "# Compare different C values\n",
    "C_values = [0.1, 1.0, 10.0]\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "for i, C in enumerate(C_values):\n",
    "    svm = SVC(kernel='linear', C=C)\n",
    "    svm.fit(X_noisy, y_linear)\n",
    "    \n",
    "    # Create mesh\n",
    "    h = 0.02\n",
    "    x_min, x_max = X_noisy[:, 0].min() - 1, X_noisy[:, 0].max() + 1\n",
    "    y_min, y_max = X_noisy[:, 1].min() - 1, X_noisy[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "    \n",
    "    Z = svm.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    axes[i].contourf(xx, yy, Z, alpha=0.3, cmap=plt.cm.RdYlBu)\n",
    "    axes[i].scatter(X_noisy[:, 0], X_noisy[:, 1], c=y_linear, cmap=plt.cm.RdYlBu, edgecolors='black')\n",
    "    axes[i].scatter(svm.support_vectors_[:, 0], svm.support_vectors_[:, 1],\n",
    "                   s=100, facecolors='none', edgecolors='green', linewidths=2)\n",
    "    \n",
    "    accuracy = svm.score(X_noisy, y_linear)\n",
    "    axes[i].set_title(f'C = {C}, Accuracy: {accuracy:.3f}\\nSupport Vectors: {len(svm.support_vectors_)}')\n",
    "    axes[i].set_xlabel('Feature 1')\n",
    "    axes[i].set_ylabel('Feature 2')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. The Kernel Trick\n",
    "\n",
    "The kernel trick allows SVMs to find non-linear decision boundaries by implicitly mapping data to higher-dimensional spaces. This section demonstrates how different kernels handle non-linear data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_kernels(X, y, title_prefix):\n",
    "    \"\"\"Compare different kernel functions on the same dataset.\"\"\"\n",
    "    kernels = ['linear', 'poly', 'rbf', 'sigmoid']\n",
    "    kernel_names = ['Linear', 'Polynomial (degree=3)', 'RBF (Î³=auto)', 'Sigmoid']\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    axes = axes.ravel()\n",
    "    \n",
    "    for i, (kernel, name) in enumerate(zip(kernels, kernel_names)):\n",
    "        if kernel == 'poly':\n",
    "            svm = SVC(kernel=kernel, degree=3, C=1.0)\n",
    "        else:\n",
    "            svm = SVC(kernel=kernel, C=1.0)\n",
    "        \n",
    "        svm.fit(X, y)\n",
    "        \n",
    "        # Create mesh\n",
    "        h = 0.02\n",
    "        x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
    "        y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
    "        xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                             np.arange(y_min, y_max, h))\n",
    "        \n",
    "        Z = svm.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "        Z = Z.reshape(xx.shape)\n",
    "        \n",
    "        axes[i].contourf(xx, yy, Z, alpha=0.3, cmap=plt.cm.RdYlBu)\n",
    "        axes[i].scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.RdYlBu, edgecolors='black')\n",
    "        axes[i].scatter(svm.support_vectors_[:, 0], svm.support_vectors_[:, 1],\n",
    "                       s=100, facecolors='none', edgecolors='green', linewidths=2)\n",
    "        \n",
    "        accuracy = svm.score(X, y)\n",
    "        axes[i].set_title(f'{name}\\nAccuracy: {accuracy:.3f}, SVs: {len(svm.support_vectors_)}')\n",
    "        axes[i].set_xlabel('Feature 1')\n",
    "        axes[i].set_ylabel('Feature 2')\n",
    "    \n",
    "    plt.suptitle(f'{title_prefix} - Kernel Comparison', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Compare kernels on circles dataset\n",
    "compare_kernels(X_circles, y_circles, 'Circles Dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare kernels on moons dataset\n",
    "compare_kernels(X_moons, y_moons, 'Moons Dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. RBF Kernel Parameter Tuning\n",
    "\n",
    "The RBF (Radial Basis Function) kernel is the most popular kernel for SVMs. Its performance depends heavily on two parameters: C (regularization) and Î³ (kernel coefficient). This section explores their effects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter grid for RBF kernel\n",
    "C_range = [0.1, 1, 10, 100]\n",
    "gamma_range = [0.001, 0.01, 0.1, 1]\n",
    "\n",
    "fig, axes = plt.subplots(len(gamma_range), len(C_range), figsize=(20, 16))\n",
    "\n",
    "for i, gamma in enumerate(gamma_range):\n",
    "    for j, C in enumerate(C_range):\n",
    "        svm = SVC(kernel='rbf', C=C, gamma=gamma)\n",
    "        svm.fit(X_circles, y_circles)\n",
    "        \n",
    "        # Create mesh\n",
    "        h = 0.02\n",
    "        x_min, x_max = X_circles[:, 0].min() - 0.5, X_circles[:, 0].max() + 0.5\n",
    "        y_min, y_max = X_circles[:, 1].min() - 0.5, X_circles[:, 1].max() + 0.5\n",
    "        xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                             np.arange(y_min, y_max, h))\n",
    "        \n",
    "        Z = svm.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "        Z = Z.reshape(xx.shape)\n",
    "        \n",
    "        axes[i, j].contourf(xx, yy, Z, alpha=0.3, cmap=plt.cm.RdYlBu)\n",
    "        axes[i, j].scatter(X_circles[:, 0], X_circles[:, 1], c=y_circles, \n",
    "                          cmap=plt.cm.RdYlBu, edgecolors='black', s=30)\n",
    "        \n",
    "        accuracy = svm.score(X_circles, y_circles)\n",
    "        axes[i, j].set_title(f'C={C}, Î³={gamma}\\nAcc: {accuracy:.3f}')\n",
    "        axes[i, j].set_xticks([])\n",
    "        axes[i, j].set_yticks([])\n",
    "\n",
    "plt.suptitle('RBF Kernel: Effect of C and Î³ Parameters', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Multi-class Classification\n",
    "\n",
    "SVMs are inherently binary classifiers. For multi-class problems, we use strategies like One-vs-Rest (OvR) and One-vs-One (OvO). This section compares these approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create multi-class dataset\n",
    "X_multi, y_multi = make_classification(n_samples=300, n_features=2, n_classes=4, \n",
    "                                      n_redundant=0, n_informative=2, \n",
    "                                      n_clusters_per_class=1, class_sep=1.5, \n",
    "                                      random_state=42)\n",
    "\n",
    "# Compare multi-class strategies\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Native multi-class SVM\n",
    "svm_native = SVC(kernel='rbf', C=1, gamma='scale', decision_function_shape='ovr')\n",
    "svm_native.fit(X_multi, y_multi)\n",
    "\n",
    "# One-vs-Rest\n",
    "svm_ovr = OneVsRestClassifier(SVC(kernel='rbf', C=1, gamma='scale'))\n",
    "svm_ovr.fit(X_multi, y_multi)\n",
    "\n",
    "# One-vs-One\n",
    "svm_ovo = OneVsOneClassifier(SVC(kernel='rbf', C=1, gamma='scale'))\n",
    "svm_ovo.fit(X_multi, y_multi)\n",
    "\n",
    "models = [svm_native, svm_ovr, svm_ovo]\n",
    "titles = ['Native Multi-class', 'One-vs-Rest', 'One-vs-One']\n",
    "\n",
    "for i, (model, title) in enumerate(zip(models, titles)):\n",
    "    # Create mesh\n",
    "    h = 0.02\n",
    "    x_min, x_max = X_multi[:, 0].min() - 1, X_multi[:, 0].max() + 1\n",
    "    y_min, y_max = X_multi[:, 1].min() - 1, X_multi[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "    \n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    axes[i].contourf(xx, yy, Z, alpha=0.3, cmap=plt.cm.Set3)\n",
    "    scatter = axes[i].scatter(X_multi[:, 0], X_multi[:, 1], c=y_multi, \n",
    "                             cmap=plt.cm.Set3, edgecolors='black')\n",
    "    \n",
    "    accuracy = model.score(X_multi, y_multi)\n",
    "    axes[i].set_title(f'{title}\\nAccuracy: {accuracy:.3f}')\n",
    "    axes[i].set_xlabel('Feature 1')\n",
    "    axes[i].set_ylabel('Feature 2')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Support Vector Regression (SVR)\n",
    "\n",
    "Support Vector Regression extends SVM concepts to regression problems. It uses Îµ-insensitive loss function and can handle non-linear relationships using kernels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate regression data with noise\n",
    "def generate_regression_data(n_samples=100, noise=0.3):\n",
    "    X = np.linspace(0, 4, n_samples).reshape(-1, 1)\n",
    "    y = np.sin(2 * X).ravel() + np.random.normal(0, noise, X.shape[0])\n",
    "    return X, y\n",
    "\n",
    "X_reg, y_reg = generate_regression_data()\n",
    "\n",
    "# Compare different SVR kernels\n",
    "kernels = ['linear', 'poly', 'rbf']\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Create fine grid for smooth curves\n",
    "X_plot = np.linspace(0, 4, 300).reshape(-1, 1)\n",
    "\n",
    "for i, kernel in enumerate(kernels):\n",
    "    if kernel == 'poly':\n",
    "        svr = SVR(kernel=kernel, degree=3, C=100, epsilon=0.1)\n",
    "    else:\n",
    "        svr = SVR(kernel=kernel, C=100, epsilon=0.1)\n",
    "    \n",
    "    svr.fit(X_reg, y_reg)\n",
    "    y_pred = svr.predict(X_plot)\n",
    "    \n",
    "    axes[i].scatter(X_reg, y_reg, alpha=0.6, label='Data')\n",
    "    axes[i].plot(X_plot, y_pred, color='red', linewidth=2, label='SVR prediction')\n",
    "    axes[i].plot(X_plot, np.sin(2 * X_plot).ravel(), color='green', \n",
    "                linewidth=1, linestyle='--', label='True function')\n",
    "    \n",
    "    # Highlight support vectors\n",
    "    axes[i].scatter(X_reg[svr.support_], y_reg[svr.support_], \n",
    "                   s=100, facecolors='none', edgecolors='orange', \n",
    "                   linewidths=2, label=f'Support Vectors ({len(svr.support_)})')\n",
    "    \n",
    "    score = svr.score(X_reg, y_reg)\n",
    "    axes[i].set_title(f'{kernel.upper()} SVR\\nRÂ² Score: {score:.3f}')\n",
    "    axes[i].set_xlabel('X')\n",
    "    axes[i].set_ylabel('y')\n",
    "    axes[i].legend()\n",
    "    axes[i].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Epsilon Parameter in SVR\n",
    "\n",
    "The Îµ parameter in SVR defines the margin of tolerance where no penalty is given to errors. Points within this margin are not considered support vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Effect of epsilon parameter\n",
    "epsilons = [0.01, 0.1, 0.5, 1.0]\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i, eps in enumerate(epsilons):\n",
    "    svr = SVR(kernel='rbf', C=100, epsilon=eps, gamma='scale')\n",
    "    svr.fit(X_reg, y_reg)\n",
    "    y_pred = svr.predict(X_plot)\n",
    "    \n",
    "    axes[i].scatter(X_reg, y_reg, alpha=0.6, label='Data')\n",
    "    axes[i].plot(X_plot, y_pred, color='red', linewidth=2, label='SVR prediction')\n",
    "    \n",
    "    # Show epsilon tube\n",
    "    axes[i].fill_between(X_plot.ravel(), y_pred - eps, y_pred + eps, \n",
    "                        alpha=0.2, color='red', label=f'Îµ-tube (Îµ={eps})')\n",
    "    \n",
    "    # Highlight support vectors\n",
    "    axes[i].scatter(X_reg[svr.support_], y_reg[svr.support_], \n",
    "                   s=100, facecolors='none', edgecolors='orange', linewidths=2)\n",
    "    \n",
    "    score = svr.score(X_reg, y_reg)\n",
    "    axes[i].set_title(f'Îµ = {eps}\\nSupport Vectors: {len(svr.support_)}\\nRÂ² Score: {score:.3f}')\n",
    "    axes[i].set_xlabel('X')\n",
    "    axes[i].set_ylabel('y')\n",
    "    axes[i].legend()\n",
    "    axes[i].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Kernel Ridge Regression Comparison\n",
    "\n",
    "Kernel Ridge Regression is another kernel method for regression that uses L2 regularization instead of the Îµ-insensitive loss. This section compares it with SVR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.kernel_ridge import KernelRidge\n",
    "\n",
    "# Compare SVR vs Kernel Ridge Regression\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# SVR\n",
    "svr = SVR(kernel='rbf', C=100, epsilon=0.1, gamma='scale')\n",
    "svr.fit(X_reg, y_reg)\n",
    "y_svr = svr.predict(X_plot)\n",
    "\n",
    "# Kernel Ridge Regression\n",
    "krr = KernelRidge(kernel='rbf', alpha=0.01, gamma=None)\n",
    "krr.fit(X_reg, y_reg)\n",
    "y_krr = krr.predict(X_plot)\n",
    "\n",
    "# Plot SVR\n",
    "axes[0].scatter(X_reg, y_reg, alpha=0.6, label='Data')\n",
    "axes[0].plot(X_plot, y_svr, color='red', linewidth=2, label='SVR')\n",
    "axes[0].plot(X_plot, np.sin(2 * X_plot).ravel(), color='green', \n",
    "            linewidth=1, linestyle='--', label='True function')\n",
    "axes[0].scatter(X_reg[svr.support_], y_reg[svr.support_], \n",
    "               s=100, facecolors='none', edgecolors='orange', linewidths=2,\n",
    "               label=f'Support Vectors ({len(svr.support_)})')\n",
    "axes[0].set_title(f'Support Vector Regression\\nRÂ² Score: {svr.score(X_reg, y_reg):.3f}')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot Kernel Ridge\n",
    "axes[1].scatter(X_reg, y_reg, alpha=0.6, label='Data')\n",
    "axes[1].plot(X_plot, y_krr, color='blue', linewidth=2, label='Kernel Ridge')\n",
    "axes[1].plot(X_plot, np.sin(2 * X_plot).ravel(), color='green', \n",
    "            linewidth=1, linestyle='--', label='True function')\n",
    "axes[1].set_title(f'Kernel Ridge Regression\\nRÂ² Score: {krr.score(X_reg, y_reg):.3f}')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "for ax in axes:\n",
    "    ax.set_xlabel('X')\n",
    "    ax.set_ylabel('y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Hyperparameter Optimization\n",
    "\n",
    "Finding optimal hyperparameters is crucial for SVM performance. This section demonstrates systematic parameter tuning using grid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid search for optimal parameters\n",
    "from sklearn.model_selection import validation_curve\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_circles, y_circles, \n",
    "                                                    test_size=0.3, random_state=42)\n",
    "\n",
    "# Validation curve for C parameter\n",
    "C_range = np.logspace(-2, 3, 10)\n",
    "train_scores_C, val_scores_C = validation_curve(\n",
    "    SVC(kernel='rbf', gamma='scale'), X_train, y_train, \n",
    "    param_name='C', param_range=C_range, cv=5, scoring='accuracy'\n",
    ")\n",
    "\n",
    "# Validation curve for gamma parameter\n",
    "gamma_range = np.logspace(-4, 1, 10)\n",
    "train_scores_gamma, val_scores_gamma = validation_curve(\n",
    "    SVC(kernel='rbf', C=1.0), X_train, y_train, \n",
    "    param_name='gamma', param_range=gamma_range, cv=5, scoring='accuracy'\n",
    ")\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Plot C parameter validation curve\n",
    "train_mean_C = np.mean(train_scores_C, axis=1)\n",
    "train_std_C = np.std(train_scores_C, axis=1)\n",
    "val_mean_C = np.mean(val_scores_C, axis=1)\n",
    "val_std_C = np.std(val_scores_C, axis=1)\n",
    "\n",
    "axes[0].semilogx(C_range, train_mean_C, 'o-', color='blue', label='Training accuracy')\n",
    "axes[0].fill_between(C_range, train_mean_C - train_std_C, train_mean_C + train_std_C, \n",
    "                    alpha=0.2, color='blue')\n",
    "axes[0].semilogx(C_range, val_mean_C, 'o-', color='red', label='Validation accuracy')\n",
    "axes[0].fill_between(C_range, val_mean_C - val_std_C, val_mean_C + val_std_C, \n",
    "                    alpha=0.2, color='red')\n",
    "axes[0].set_xlabel('C')\n",
    "axes[0].set_ylabel('Accuracy')\n",
    "axes[0].set_title('Validation Curve - C Parameter')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot gamma parameter validation curve\n",
    "train_mean_gamma = np.mean(train_scores_gamma, axis=1)\n",
    "train_std_gamma = np.std(train_scores_gamma, axis=1)\n",
    "val_mean_gamma = np.mean(val_scores_gamma, axis=1)\n",
    "val_std_gamma = np.std(val_scores_gamma, axis=1)\n",
    "\n",
    "axes[1].semilogx(gamma_range, train_mean_gamma, 'o-', color='blue', label='Training accuracy')\n",
    "axes[1].fill_between(gamma_range, train_mean_gamma - train_std_gamma, \n",
    "                    train_mean_gamma + train_std_gamma, alpha=0.2, color='blue')\n",
    "axes[1].semilogx(gamma_range, val_mean_gamma, 'o-', color='red', label='Validation accuracy')\n",
    "axes[1].fill_between(gamma_range, val_mean_gamma - val_std_gamma, \n",
    "                    val_mean_gamma + val_std_gamma, alpha=0.2, color='red')\n",
    "axes[1].set_xlabel('Î³')\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "axes[1].set_title('Validation Curve - Î³ Parameter')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid search for best combination\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'gamma': [0.001, 0.01, 0.1, 1]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(SVC(kernel='rbf'), param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Test the best model\n",
    "best_svm = grid_search.best_estimator_\n",
    "test_accuracy = best_svm.score(X_test, y_test)\n",
    "\n",
    "print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best cross-validation score: {grid_search.best_score_:.3f}\")\n",
    "print(f\"Test accuracy: {test_accuracy:.3f}\")\n",
    "\n",
    "# Visualize the best model\n",
    "plot_svm_decision_boundary(X_test, y_test, best_svm, 'Optimized RBF SVM')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated key concepts in kernel methods:\n",
    "\n",
    "1. **Linear vs Non-linear Separability**: Understanding when kernel methods are needed\n",
    "2. **Support Vector Machines**: Maximum margin classification with support vectors\n",
    "3. **Soft Margins**: Handling non-separable data with regularization parameter C\n",
    "4. **Kernel Trick**: Implicit mapping to higher dimensions using different kernels\n",
    "5. **Parameter Tuning**: Effect of C and Î³ on model complexity and performance\n",
    "6. **Multi-class Extensions**: One-vs-Rest and One-vs-One strategies\n",
    "7. **Support Vector Regression**: Extending SVM concepts to regression problems\n",
    "8. **Hyperparameter Optimization**: Systematic approach to finding optimal parameters\n",
    "\n",
    "**Key Takeaways:**\n",
    "- RBF kernel works well for most non-linear problems\n",
    "- Parameter tuning is crucial for optimal performance\n",
    "- SVMs are powerful but require careful preprocessing and parameter selection\n",
    "- Kernel methods provide elegant solutions to non-linear problems through implicit feature mapping"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}