{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kernel Methods Tutorial\n",
    "## CMSC 173 - Machine Learning\n",
    "\n",
    "This notebook accompanies the kernel methods slides and provides hands-on implementation and visualization of key concepts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.svm import SVC, SVR\n",
    "from sklearn.datasets import make_classification, make_regression, make_circles, make_moons\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.multiclass import OneVsRestClassifier, OneVsOneClassifier\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.style.use('seaborn-v0_8')\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction & Motivation\n",
    "\n",
    "Kernel methods solve the fundamental problem of learning non-linear patterns in data by implicitly mapping features to higher-dimensional spaces. This section demonstrates why we need kernel methods through visualization of linear vs non-linear separability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create linearly separable data\n",
    "X_linear, y_linear = make_classification(n_samples=200, n_features=2, n_redundant=0, \n",
    "                                        n_informative=2, n_clusters_per_class=1, \n",
    "                                        class_sep=2, random_state=42)\n",
    "\n",
    "# Create non-linearly separable data\n",
    "X_circles, y_circles = make_circles(n_samples=200, noise=0.1, factor=0.3, random_state=42)\n",
    "X_moons, y_moons = make_moons(n_samples=200, noise=0.1, random_state=42)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Plot linearly separable data\n",
    "axes[0].scatter(X_linear[y_linear==0, 0], X_linear[y_linear==0, 1], c='red', alpha=0.7, label='Class 0')\n",
    "axes[0].scatter(X_linear[y_linear==1, 0], X_linear[y_linear==1, 1], c='blue', alpha=0.7, label='Class 1')\n",
    "axes[0].set_title('Linearly Separable')\n",
    "axes[0].legend()\n",
    "\n",
    "# Plot circles data\n",
    "axes[1].scatter(X_circles[y_circles==0, 0], X_circles[y_circles==0, 1], c='red', alpha=0.7, label='Class 0')\n",
    "axes[1].scatter(X_circles[y_circles==1, 0], X_circles[y_circles==1, 1], c='blue', alpha=0.7, label='Class 1')\n",
    "axes[1].set_title('Circles (Non-linear)')\n",
    "axes[1].legend()\n",
    "\n",
    "# Plot moons data\n",
    "axes[2].scatter(X_moons[y_moons==0, 0], X_moons[y_moons==0, 1], c='red', alpha=0.7, label='Class 0')\n",
    "axes[2].scatter(X_moons[y_moons==1, 0], X_moons[y_moons==1, 1], c='blue', alpha=0.7, label='Class 1')\n",
    "axes[2].set_title('Moons (Non-linear)')\n",
    "axes[2].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Support Vector Machines - Linear Case\n",
    "\n",
    "Support Vector Machines find the optimal decision boundary by maximizing the margin between classes. This section demonstrates the fundamental SVM concepts using linearly separable data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_svm_decision_boundary(X, y, model, title):\n",
    "    \"\"\"Plot SVM decision boundary with support vectors highlighted.\"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Create mesh for decision boundary\n",
    "    h = 0.02\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "    \n",
    "    # Predict on mesh\n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    # Plot decision boundary\n",
    "    plt.contourf(xx, yy, Z, alpha=0.3, cmap=plt.cm.RdYlBu)\n",
    "    \n",
    "    # Plot data points\n",
    "    scatter = plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.RdYlBu, edgecolors='black')\n",
    "    \n",
    "    # Highlight support vectors\n",
    "    plt.scatter(model.support_vectors_[:, 0], model.support_vectors_[:, 1],\n",
    "                s=100, facecolors='none', edgecolors='green', linewidths=2,\n",
    "                label=f'Support Vectors ({len(model.support_vectors_)})')\n",
    "    \n",
    "    plt.title(f'{title} - Margin: {2/np.linalg.norm(model.coef_):.3f}')\n",
    "    plt.xlabel('Feature 1')\n",
    "    plt.ylabel('Feature 2')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Train linear SVM on linearly separable data\n",
    "svm_linear = SVC(kernel='linear', C=1.0)\n",
    "svm_linear.fit(X_linear, y_linear)\n",
    "\n",
    "plot_svm_decision_boundary(X_linear, y_linear, svm_linear, 'Linear SVM')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Hard vs Soft Margin\n",
    "\n",
    "Real-world data is rarely perfectly separable. Soft margin SVM allows some misclassifications by introducing slack variables, controlled by the regularization parameter C."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add noise to make data not perfectly separable\n",
    "X_noisy = X_linear + np.random.normal(0, 0.3, X_linear.shape)\n",
    "\n",
    "# Compare different C values\n",
    "C_values = [0.1, 1.0, 10.0]\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "for i, C in enumerate(C_values):\n",
    "    svm = SVC(kernel='linear', C=C)\n",
    "    svm.fit(X_noisy, y_linear)\n",
    "    \n",
    "    # Create mesh\n",
    "    h = 0.02\n",
    "    x_min, x_max = X_noisy[:, 0].min() - 1, X_noisy[:, 0].max() + 1\n",
    "    y_min, y_max = X_noisy[:, 1].min() - 1, X_noisy[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "    \n",
    "    Z = svm.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    axes[i].contourf(xx, yy, Z, alpha=0.3, cmap=plt.cm.RdYlBu)\n",
    "    axes[i].scatter(X_noisy[:, 0], X_noisy[:, 1], c=y_linear, cmap=plt.cm.RdYlBu, edgecolors='black')\n",
    "    axes[i].scatter(svm.support_vectors_[:, 0], svm.support_vectors_[:, 1],\n",
    "                   s=100, facecolors='none', edgecolors='green', linewidths=2)\n",
    "    \n",
    "    accuracy = svm.score(X_noisy, y_linear)\n",
    "    axes[i].set_title(f'C = {C}, Accuracy: {accuracy:.3f}\\nSupport Vectors: {len(svm.support_vectors_)}')\n",
    "    axes[i].set_xlabel('Feature 1')\n",
    "    axes[i].set_ylabel('Feature 2')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. The Kernel Trick\n",
    "\n",
    "The kernel trick allows SVMs to find non-linear decision boundaries by implicitly mapping data to higher-dimensional spaces. This section demonstrates how different kernels handle non-linear data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_kernels(X, y, title_prefix):\n",
    "    \"\"\"Compare different kernel functions on the same dataset.\"\"\"\n",
    "    kernels = ['linear', 'poly', 'rbf', 'sigmoid']\n",
    "    kernel_names = ['Linear', 'Polynomial (degree=3)', 'RBF (γ=auto)', 'Sigmoid']\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    axes = axes.ravel()\n",
    "    \n",
    "    for i, (kernel, name) in enumerate(zip(kernels, kernel_names)):\n",
    "        if kernel == 'poly':\n",
    "            svm = SVC(kernel=kernel, degree=3, C=1.0)\n",
    "        else:\n",
    "            svm = SVC(kernel=kernel, C=1.0)\n",
    "        \n",
    "        svm.fit(X, y)\n",
    "        \n",
    "        # Create mesh\n",
    "        h = 0.02\n",
    "        x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
    "        y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
    "        xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                             np.arange(y_min, y_max, h))\n",
    "        \n",
    "        Z = svm.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "        Z = Z.reshape(xx.shape)\n",
    "        \n",
    "        axes[i].contourf(xx, yy, Z, alpha=0.3, cmap=plt.cm.RdYlBu)\n",
    "        axes[i].scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.RdYlBu, edgecolors='black')\n",
    "        axes[i].scatter(svm.support_vectors_[:, 0], svm.support_vectors_[:, 1],\n",
    "                       s=100, facecolors='none', edgecolors='green', linewidths=2)\n",
    "        \n",
    "        accuracy = svm.score(X, y)\n",
    "        axes[i].set_title(f'{name}\\nAccuracy: {accuracy:.3f}, SVs: {len(svm.support_vectors_)}')\n",
    "        axes[i].set_xlabel('Feature 1')\n",
    "        axes[i].set_ylabel('Feature 2')\n",
    "    \n",
    "    plt.suptitle(f'{title_prefix} - Kernel Comparison', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Compare kernels on circles dataset\n",
    "compare_kernels(X_circles, y_circles, 'Circles Dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare kernels on moons dataset\n",
    "compare_kernels(X_moons, y_moons, 'Moons Dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. RBF Kernel Parameter Tuning\n",
    "\n",
    "The RBF (Radial Basis Function) kernel is the most popular kernel for SVMs. Its performance depends heavily on two parameters: C (regularization) and γ (kernel coefficient). This section explores their effects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter grid for RBF kernel\n",
    "C_range = [0.1, 1, 10, 100]\n",
    "gamma_range = [0.001, 0.01, 0.1, 1]\n",
    "\n",
    "fig, axes = plt.subplots(len(gamma_range), len(C_range), figsize=(20, 16))\n",
    "\n",
    "for i, gamma in enumerate(gamma_range):\n",
    "    for j, C in enumerate(C_range):\n",
    "        svm = SVC(kernel='rbf', C=C, gamma=gamma)\n",
    "        svm.fit(X_circles, y_circles)\n",
    "        \n",
    "        # Create mesh\n",
    "        h = 0.02\n",
    "        x_min, x_max = X_circles[:, 0].min() - 0.5, X_circles[:, 0].max() + 0.5\n",
    "        y_min, y_max = X_circles[:, 1].min() - 0.5, X_circles[:, 1].max() + 0.5\n",
    "        xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                             np.arange(y_min, y_max, h))\n",
    "        \n",
    "        Z = svm.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "        Z = Z.reshape(xx.shape)\n",
    "        \n",
    "        axes[i, j].contourf(xx, yy, Z, alpha=0.3, cmap=plt.cm.RdYlBu)\n",
    "        axes[i, j].scatter(X_circles[:, 0], X_circles[:, 1], c=y_circles, \n",
    "                          cmap=plt.cm.RdYlBu, edgecolors='black', s=30)\n",
    "        \n",
    "        accuracy = svm.score(X_circles, y_circles)\n",
    "        axes[i, j].set_title(f'C={C}, γ={gamma}\\nAcc: {accuracy:.3f}')\n",
    "        axes[i, j].set_xticks([])\n",
    "        axes[i, j].set_yticks([])\n",
    "\n",
    "plt.suptitle('RBF Kernel: Effect of C and γ Parameters', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Multi-class Classification\n",
    "\n",
    "SVMs are inherently binary classifiers. For multi-class problems, we use strategies like One-vs-Rest (OvR) and One-vs-One (OvO). This section compares these approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create multi-class dataset\n",
    "X_multi, y_multi = make_classification(n_samples=300, n_features=2, n_classes=4, \n",
    "                                      n_redundant=0, n_informative=2, \n",
    "                                      n_clusters_per_class=1, class_sep=1.5, \n",
    "                                      random_state=42)\n",
    "\n",
    "# Compare multi-class strategies\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Native multi-class SVM\n",
    "svm_native = SVC(kernel='rbf', C=1, gamma='scale', decision_function_shape='ovr')\n",
    "svm_native.fit(X_multi, y_multi)\n",
    "\n",
    "# One-vs-Rest\n",
    "svm_ovr = OneVsRestClassifier(SVC(kernel='rbf', C=1, gamma='scale'))\n",
    "svm_ovr.fit(X_multi, y_multi)\n",
    "\n",
    "# One-vs-One\n",
    "svm_ovo = OneVsOneClassifier(SVC(kernel='rbf', C=1, gamma='scale'))\n",
    "svm_ovo.fit(X_multi, y_multi)\n",
    "\n",
    "models = [svm_native, svm_ovr, svm_ovo]\n",
    "titles = ['Native Multi-class', 'One-vs-Rest', 'One-vs-One']\n",
    "\n",
    "for i, (model, title) in enumerate(zip(models, titles)):\n",
    "    # Create mesh\n",
    "    h = 0.02\n",
    "    x_min, x_max = X_multi[:, 0].min() - 1, X_multi[:, 0].max() + 1\n",
    "    y_min, y_max = X_multi[:, 1].min() - 1, X_multi[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "    \n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    axes[i].contourf(xx, yy, Z, alpha=0.3, cmap=plt.cm.Set3)\n",
    "    scatter = axes[i].scatter(X_multi[:, 0], X_multi[:, 1], c=y_multi, \n",
    "                             cmap=plt.cm.Set3, edgecolors='black')\n",
    "    \n",
    "    accuracy = model.score(X_multi, y_multi)\n",
    "    axes[i].set_title(f'{title}\\nAccuracy: {accuracy:.3f}')\n",
    "    axes[i].set_xlabel('Feature 1')\n",
    "    axes[i].set_ylabel('Feature 2')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Support Vector Regression (SVR)\n",
    "\n",
    "Support Vector Regression extends SVM concepts to regression problems. It uses ε-insensitive loss function and can handle non-linear relationships using kernels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate regression data with noise\n",
    "def generate_regression_data(n_samples=100, noise=0.3):\n",
    "    X = np.linspace(0, 4, n_samples).reshape(-1, 1)\n",
    "    y = np.sin(2 * X).ravel() + np.random.normal(0, noise, X.shape[0])\n",
    "    return X, y\n",
    "\n",
    "X_reg, y_reg = generate_regression_data()\n",
    "\n",
    "# Compare different SVR kernels\n",
    "kernels = ['linear', 'poly', 'rbf']\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Create fine grid for smooth curves\n",
    "X_plot = np.linspace(0, 4, 300).reshape(-1, 1)\n",
    "\n",
    "for i, kernel in enumerate(kernels):\n",
    "    if kernel == 'poly':\n",
    "        svr = SVR(kernel=kernel, degree=3, C=100, epsilon=0.1)\n",
    "    else:\n",
    "        svr = SVR(kernel=kernel, C=100, epsilon=0.1)\n",
    "    \n",
    "    svr.fit(X_reg, y_reg)\n",
    "    y_pred = svr.predict(X_plot)\n",
    "    \n",
    "    axes[i].scatter(X_reg, y_reg, alpha=0.6, label='Data')\n",
    "    axes[i].plot(X_plot, y_pred, color='red', linewidth=2, label='SVR prediction')\n",
    "    axes[i].plot(X_plot, np.sin(2 * X_plot).ravel(), color='green', \n",
    "                linewidth=1, linestyle='--', label='True function')\n",
    "    \n",
    "    # Highlight support vectors\n",
    "    axes[i].scatter(X_reg[svr.support_], y_reg[svr.support_], \n",
    "                   s=100, facecolors='none', edgecolors='orange', \n",
    "                   linewidths=2, label=f'Support Vectors ({len(svr.support_)})')\n",
    "    \n",
    "    score = svr.score(X_reg, y_reg)\n",
    "    axes[i].set_title(f'{kernel.upper()} SVR\\nR² Score: {score:.3f}')\n",
    "    axes[i].set_xlabel('X')\n",
    "    axes[i].set_ylabel('y')\n",
    "    axes[i].legend()\n",
    "    axes[i].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Epsilon Parameter in SVR\n",
    "\n",
    "The ε parameter in SVR defines the margin of tolerance where no penalty is given to errors. Points within this margin are not considered support vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Effect of epsilon parameter\n",
    "epsilons = [0.01, 0.1, 0.5, 1.0]\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i, eps in enumerate(epsilons):\n",
    "    svr = SVR(kernel='rbf', C=100, epsilon=eps, gamma='scale')\n",
    "    svr.fit(X_reg, y_reg)\n",
    "    y_pred = svr.predict(X_plot)\n",
    "    \n",
    "    axes[i].scatter(X_reg, y_reg, alpha=0.6, label='Data')\n",
    "    axes[i].plot(X_plot, y_pred, color='red', linewidth=2, label='SVR prediction')\n",
    "    \n",
    "    # Show epsilon tube\n",
    "    axes[i].fill_between(X_plot.ravel(), y_pred - eps, y_pred + eps, \n",
    "                        alpha=0.2, color='red', label=f'ε-tube (ε={eps})')\n",
    "    \n",
    "    # Highlight support vectors\n",
    "    axes[i].scatter(X_reg[svr.support_], y_reg[svr.support_], \n",
    "                   s=100, facecolors='none', edgecolors='orange', linewidths=2)\n",
    "    \n",
    "    score = svr.score(X_reg, y_reg)\n",
    "    axes[i].set_title(f'ε = {eps}\\nSupport Vectors: {len(svr.support_)}\\nR² Score: {score:.3f}')\n",
    "    axes[i].set_xlabel('X')\n",
    "    axes[i].set_ylabel('y')\n",
    "    axes[i].legend()\n",
    "    axes[i].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Kernel Ridge Regression Comparison\n",
    "\n",
    "Kernel Ridge Regression is another kernel method for regression that uses L2 regularization instead of the ε-insensitive loss. This section compares it with SVR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.kernel_ridge import KernelRidge\n",
    "\n",
    "# Compare SVR vs Kernel Ridge Regression\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# SVR\n",
    "svr = SVR(kernel='rbf', C=100, epsilon=0.1, gamma='scale')\n",
    "svr.fit(X_reg, y_reg)\n",
    "y_svr = svr.predict(X_plot)\n",
    "\n",
    "# Kernel Ridge Regression\n",
    "krr = KernelRidge(kernel='rbf', alpha=0.01, gamma=None)\n",
    "krr.fit(X_reg, y_reg)\n",
    "y_krr = krr.predict(X_plot)\n",
    "\n",
    "# Plot SVR\n",
    "axes[0].scatter(X_reg, y_reg, alpha=0.6, label='Data')\n",
    "axes[0].plot(X_plot, y_svr, color='red', linewidth=2, label='SVR')\n",
    "axes[0].plot(X_plot, np.sin(2 * X_plot).ravel(), color='green', \n",
    "            linewidth=1, linestyle='--', label='True function')\n",
    "axes[0].scatter(X_reg[svr.support_], y_reg[svr.support_], \n",
    "               s=100, facecolors='none', edgecolors='orange', linewidths=2,\n",
    "               label=f'Support Vectors ({len(svr.support_)})')\n",
    "axes[0].set_title(f'Support Vector Regression\\nR² Score: {svr.score(X_reg, y_reg):.3f}')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot Kernel Ridge\n",
    "axes[1].scatter(X_reg, y_reg, alpha=0.6, label='Data')\n",
    "axes[1].plot(X_plot, y_krr, color='blue', linewidth=2, label='Kernel Ridge')\n",
    "axes[1].plot(X_plot, np.sin(2 * X_plot).ravel(), color='green', \n",
    "            linewidth=1, linestyle='--', label='True function')\n",
    "axes[1].set_title(f'Kernel Ridge Regression\\nR² Score: {krr.score(X_reg, y_reg):.3f}')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "for ax in axes:\n",
    "    ax.set_xlabel('X')\n",
    "    ax.set_ylabel('y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Hyperparameter Optimization\n",
    "\n",
    "Finding optimal hyperparameters is crucial for SVM performance. This section demonstrates systematic parameter tuning using grid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid search for optimal parameters\n",
    "from sklearn.model_selection import validation_curve\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_circles, y_circles, \n",
    "                                                    test_size=0.3, random_state=42)\n",
    "\n",
    "# Validation curve for C parameter\n",
    "C_range = np.logspace(-2, 3, 10)\n",
    "train_scores_C, val_scores_C = validation_curve(\n",
    "    SVC(kernel='rbf', gamma='scale'), X_train, y_train, \n",
    "    param_name='C', param_range=C_range, cv=5, scoring='accuracy'\n",
    ")\n",
    "\n",
    "# Validation curve for gamma parameter\n",
    "gamma_range = np.logspace(-4, 1, 10)\n",
    "train_scores_gamma, val_scores_gamma = validation_curve(\n",
    "    SVC(kernel='rbf', C=1.0), X_train, y_train, \n",
    "    param_name='gamma', param_range=gamma_range, cv=5, scoring='accuracy'\n",
    ")\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Plot C parameter validation curve\n",
    "train_mean_C = np.mean(train_scores_C, axis=1)\n",
    "train_std_C = np.std(train_scores_C, axis=1)\n",
    "val_mean_C = np.mean(val_scores_C, axis=1)\n",
    "val_std_C = np.std(val_scores_C, axis=1)\n",
    "\n",
    "axes[0].semilogx(C_range, train_mean_C, 'o-', color='blue', label='Training accuracy')\n",
    "axes[0].fill_between(C_range, train_mean_C - train_std_C, train_mean_C + train_std_C, \n",
    "                    alpha=0.2, color='blue')\n",
    "axes[0].semilogx(C_range, val_mean_C, 'o-', color='red', label='Validation accuracy')\n",
    "axes[0].fill_between(C_range, val_mean_C - val_std_C, val_mean_C + val_std_C, \n",
    "                    alpha=0.2, color='red')\n",
    "axes[0].set_xlabel('C')\n",
    "axes[0].set_ylabel('Accuracy')\n",
    "axes[0].set_title('Validation Curve - C Parameter')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot gamma parameter validation curve\n",
    "train_mean_gamma = np.mean(train_scores_gamma, axis=1)\n",
    "train_std_gamma = np.std(train_scores_gamma, axis=1)\n",
    "val_mean_gamma = np.mean(val_scores_gamma, axis=1)\n",
    "val_std_gamma = np.std(val_scores_gamma, axis=1)\n",
    "\n",
    "axes[1].semilogx(gamma_range, train_mean_gamma, 'o-', color='blue', label='Training accuracy')\n",
    "axes[1].fill_between(gamma_range, train_mean_gamma - train_std_gamma, \n",
    "                    train_mean_gamma + train_std_gamma, alpha=0.2, color='blue')\n",
    "axes[1].semilogx(gamma_range, val_mean_gamma, 'o-', color='red', label='Validation accuracy')\n",
    "axes[1].fill_between(gamma_range, val_mean_gamma - val_std_gamma, \n",
    "                    val_mean_gamma + val_std_gamma, alpha=0.2, color='red')\n",
    "axes[1].set_xlabel('γ')\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "axes[1].set_title('Validation Curve - γ Parameter')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid search for best combination\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'gamma': [0.001, 0.01, 0.1, 1]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(SVC(kernel='rbf'), param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Test the best model\n",
    "best_svm = grid_search.best_estimator_\n",
    "test_accuracy = best_svm.score(X_test, y_test)\n",
    "\n",
    "print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best cross-validation score: {grid_search.best_score_:.3f}\")\n",
    "print(f\"Test accuracy: {test_accuracy:.3f}\")\n",
    "\n",
    "# Visualize the best model\n",
    "plot_svm_decision_boundary(X_test, y_test, best_svm, 'Optimized RBF SVM')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated key concepts in kernel methods:\n",
    "\n",
    "1. **Linear vs Non-linear Separability**: Understanding when kernel methods are needed\n",
    "2. **Support Vector Machines**: Maximum margin classification with support vectors\n",
    "3. **Soft Margins**: Handling non-separable data with regularization parameter C\n",
    "4. **Kernel Trick**: Implicit mapping to higher dimensions using different kernels\n",
    "5. **Parameter Tuning**: Effect of C and γ on model complexity and performance\n",
    "6. **Multi-class Extensions**: One-vs-Rest and One-vs-One strategies\n",
    "7. **Support Vector Regression**: Extending SVM concepts to regression problems\n",
    "8. **Hyperparameter Optimization**: Systematic approach to finding optimal parameters\n",
    "\n",
    "**Key Takeaways:**\n",
    "- RBF kernel works well for most non-linear problems\n",
    "- Parameter tuning is crucial for optimal performance\n",
    "- SVMs are powerful but require careful preprocessing and parameter selection\n",
    "- Kernel methods provide elegant solutions to non-linear problems through implicit feature mapping"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}