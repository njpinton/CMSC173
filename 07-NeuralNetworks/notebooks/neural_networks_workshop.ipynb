{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks Workshop\n",
    "**CMSC 173 - Machine Learning**\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "By the end of this workshop, you will be able to:\n",
    "1. Build and train neural networks from scratch\n",
    "2. Understand forward and backward propagation\n",
    "3. Implement different activation functions\n",
    "4. Apply regularization techniques\n",
    "5. Evaluate neural network performance\n",
    "\n",
    "---\n",
    "\n",
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import make_classification, make_circles\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for consistent figures\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"âœ… Setup complete! Ready to build neural networks.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Understanding Activation Functions\n",
    "\n",
    "Let's start by implementing and visualizing different activation functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActivationFunctions:\n",
    "    \"\"\"Collection of activation functions and their derivatives\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def sigmoid(x):\n",
    "        \"\"\"Sigmoid activation function\"\"\"\n",
    "        return 1 / (1 + np.exp(-np.clip(x, -500, 500)))  # Clip to prevent overflow\n",
    "    \n",
    "    @staticmethod\n",
    "    def sigmoid_derivative(x):\n",
    "        \"\"\"Derivative of sigmoid function\"\"\"\n",
    "        s = ActivationFunctions.sigmoid(x)\n",
    "        return s * (1 - s)\n",
    "    \n",
    "    @staticmethod\n",
    "    def tanh(x):\n",
    "        \"\"\"Hyperbolic tangent activation function\"\"\"\n",
    "        return np.tanh(x)\n",
    "    \n",
    "    @staticmethod\n",
    "    def tanh_derivative(x):\n",
    "        \"\"\"Derivative of tanh function\"\"\"\n",
    "        return 1 - np.tanh(x)**2\n",
    "    \n",
    "    @staticmethod\n",
    "    def relu(x):\n",
    "        \"\"\"ReLU activation function\"\"\"\n",
    "        return np.maximum(0, x)\n",
    "    \n",
    "    @staticmethod\n",
    "    def relu_derivative(x):\n",
    "        \"\"\"Derivative of ReLU function\"\"\"\n",
    "        return (x > 0).astype(float)\n",
    "\n",
    "# Test the activation functions\n",
    "x = np.linspace(-5, 5, 100)\n",
    "activations = ActivationFunctions()\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Sigmoid\n",
    "axes[0].plot(x, activations.sigmoid(x), 'b-', linewidth=2, label='Sigmoid')\n",
    "axes[0].plot(x, activations.sigmoid_derivative(x), 'b--', linewidth=2, label='Derivative')\n",
    "axes[0].set_title('Sigmoid Function')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].legend()\n",
    "\n",
    "# Tanh\n",
    "axes[1].plot(x, activations.tanh(x), 'r-', linewidth=2, label='Tanh')\n",
    "axes[1].plot(x, activations.tanh_derivative(x), 'r--', linewidth=2, label='Derivative')\n",
    "axes[1].set_title('Tanh Function')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].legend()\n",
    "\n",
    "# ReLU\n",
    "axes[2].plot(x, activations.relu(x), 'g-', linewidth=2, label='ReLU')\n",
    "axes[2].plot(x, activations.relu_derivative(x), 'g--', linewidth=2, label='Derivative')\n",
    "axes[2].set_title('ReLU Function')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "axes[2].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ¤” Discussion Questions\n",
    "1. Which activation function has the problem of vanishing gradients?\n",
    "2. Why is ReLU popular in deep networks?\n",
    "3. What are the output ranges of each activation function?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Building a Neural Network from Scratch\n",
    "\n",
    "Now let's implement a simple neural network class with forward and backward propagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNeuralNetwork:\n",
    "    \"\"\"A simple 2-layer neural network implementation\"\"\"\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, output_size, activation='sigmoid'):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        # Initialize weights and biases\n",
    "        self.W1 = np.random.randn(self.input_size, self.hidden_size) * 0.5\n",
    "        self.b1 = np.zeros((1, self.hidden_size))\n",
    "        self.W2 = np.random.randn(self.hidden_size, self.output_size) * 0.5\n",
    "        self.b2 = np.zeros((1, self.output_size))\n",
    "        \n",
    "        # Set activation function\n",
    "        self.activations = ActivationFunctions()\n",
    "        if activation == 'sigmoid':\n",
    "            self.activation = self.activations.sigmoid\n",
    "            self.activation_derivative = self.activations.sigmoid_derivative\n",
    "        elif activation == 'tanh':\n",
    "            self.activation = self.activations.tanh\n",
    "            self.activation_derivative = self.activations.tanh_derivative\n",
    "        elif activation == 'relu':\n",
    "            self.activation = self.activations.relu\n",
    "            self.activation_derivative = self.activations.relu_derivative\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"Forward propagation\"\"\"\n",
    "        # Hidden layer\n",
    "        self.z1 = np.dot(X, self.W1) + self.b1\n",
    "        self.a1 = self.activation(self.z1)\n",
    "        \n",
    "        # Output layer\n",
    "        self.z2 = np.dot(self.a1, self.W2) + self.b2\n",
    "        self.a2 = self.activations.sigmoid(self.z2)  # Always sigmoid for output\n",
    "        \n",
    "        return self.a2\n",
    "    \n",
    "    def backward(self, X, y, output):\n",
    "        \"\"\"Backward propagation\"\"\"\n",
    "        m = X.shape[0]\n",
    "        \n",
    "        # Calculate gradients for output layer\n",
    "        dz2 = output - y\n",
    "        dW2 = (1/m) * np.dot(self.a1.T, dz2)\n",
    "        db2 = (1/m) * np.sum(dz2, axis=0, keepdims=True)\n",
    "        \n",
    "        # Calculate gradients for hidden layer\n",
    "        da1 = np.dot(dz2, self.W2.T)\n",
    "        dz1 = da1 * self.activation_derivative(self.z1)\n",
    "        dW1 = (1/m) * np.dot(X.T, dz1)\n",
    "        db1 = (1/m) * np.sum(dz1, axis=0, keepdims=True)\n",
    "        \n",
    "        return dW1, db1, dW2, db2\n",
    "    \n",
    "    def update_parameters(self, dW1, db1, dW2, db2, learning_rate):\n",
    "        \"\"\"Update weights and biases\"\"\"\n",
    "        self.W1 -= learning_rate * dW1\n",
    "        self.b1 -= learning_rate * db1\n",
    "        self.W2 -= learning_rate * dW2\n",
    "        self.b2 -= learning_rate * db2\n",
    "    \n",
    "    def compute_loss(self, y_true, y_pred):\n",
    "        \"\"\"Compute binary cross-entropy loss\"\"\"\n",
    "        m = y_true.shape[0]\n",
    "        y_pred = np.clip(y_pred, 1e-8, 1 - 1e-8)  # Prevent log(0)\n",
    "        loss = -(1/m) * np.sum(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "        return loss\n",
    "    \n",
    "    def train(self, X, y, epochs, learning_rate):\n",
    "        \"\"\"Train the neural network\"\"\"\n",
    "        losses = []\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            # Forward propagation\n",
    "            output = self.forward(X)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = self.compute_loss(y, output)\n",
    "            losses.append(loss)\n",
    "            \n",
    "            # Backward propagation\n",
    "            dW1, db1, dW2, db2 = self.backward(X, y, output)\n",
    "            \n",
    "            # Update parameters\n",
    "            self.update_parameters(dW1, db1, dW2, db2, learning_rate)\n",
    "            \n",
    "            if epoch % 100 == 0:\n",
    "                print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n",
    "        \n",
    "        return losses\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Make predictions\"\"\"\n",
    "        output = self.forward(X)\n",
    "        return (output > 0.5).astype(int)\n",
    "\n",
    "print(\"âœ… Neural Network class implemented!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Training on a Real Dataset\n",
    "\n",
    "Let's test our neural network on a binary classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a binary classification dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=2, n_redundant=0, \n",
    "                          n_informative=2, random_state=42, n_clusters_per_class=1)\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Reshape y for our network\n",
    "y_train = y_train.reshape(-1, 1)\n",
    "y_test = y_test.reshape(-1, 1)\n",
    "\n",
    "print(f\"Training data shape: {X_train_scaled.shape}\")\n",
    "print(f\"Training labels shape: {y_train.shape}\")\n",
    "print(f\"Test data shape: {X_test_scaled.shape}\")\n",
    "print(f\"Test labels shape: {y_test.shape}\")\n",
    "\n",
    "# Visualize the dataset\n",
    "plt.figure(figsize=(10, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(X_train[y_train.flatten() == 0, 0], X_train[y_train.flatten() == 0, 1], \n",
    "           c='red', alpha=0.6, label='Class 0')\n",
    "plt.scatter(X_train[y_train.flatten() == 1, 0], X_train[y_train.flatten() == 1, 1], \n",
    "           c='blue', alpha=0.6, label='Class 1')\n",
    "plt.title('Original Training Data')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(X_train_scaled[y_train.flatten() == 0, 0], X_train_scaled[y_train.flatten() == 0, 1], \n",
    "           c='red', alpha=0.6, label='Class 0')\n",
    "plt.scatter(X_train_scaled[y_train.flatten() == 1, 0], X_train_scaled[y_train.flatten() == 1, 1], \n",
    "           c='blue', alpha=0.6, label='Class 1')\n",
    "plt.title('Scaled Training Data')\n",
    "plt.xlabel('Feature 1 (scaled)')\n",
    "plt.ylabel('Feature 2 (scaled)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸš€ Student Activity (15 minutes)\n",
    "\n",
    "**Your Task:** Compare different activation functions and hidden layer sizes.\n",
    "\n",
    "1. Train three neural networks with different activation functions:\n",
    "   - Sigmoid\n",
    "   - Tanh\n",
    "   - ReLU\n",
    "\n",
    "2. For each network:\n",
    "   - Use 5 hidden units\n",
    "   - Train for 500 epochs\n",
    "   - Learning rate = 0.1\n",
    "   - Record the final loss and accuracy\n",
    "\n",
    "3. Answer the discussion questions below\n",
    "\n",
    "**Starter Code:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Complete this activity\n",
    "activations_to_test = ['sigmoid', 'tanh', 'relu']\n",
    "results = {}\n",
    "\n",
    "for activation in activations_to_test:\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Training with {activation.upper()} activation\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    # TODO: Create and train the network\n",
    "    # network = SimpleNeuralNetwork(input_size=?, hidden_size=?, output_size=?, activation=?)\n",
    "    # losses = network.train(?, ?, epochs=?, learning_rate=?)\n",
    "    \n",
    "    # TODO: Make predictions and calculate accuracy\n",
    "    # predictions = network.predict(?)\n",
    "    # accuracy = np.mean(predictions == y_test)\n",
    "    \n",
    "    # TODO: Store results\n",
    "    # results[activation] = {\n",
    "    #     'final_loss': losses[-1],\n",
    "    #     'accuracy': accuracy,\n",
    "    #     'losses': losses\n",
    "    # }\n",
    "    \n",
    "    pass  # Remove this when you implement the code\n",
    "\n",
    "# TODO: Print comparison\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RESULTS COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "# for activation, result in results.items():\n",
    "#     print(f\"{activation.upper():>10}: Loss = {result['final_loss']:.4f}, Accuracy = {result['accuracy']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ¤” Discussion Questions for Activity\n",
    "1. Which activation function performed best? Why do you think that is?\n",
    "2. Did any activation function fail to converge? If so, why?\n",
    "3. How do the loss curves differ between activation functions?\n",
    "4. What happens if you increase the hidden layer size to 20? Try it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Solutions to Student Activity\n",
    "\n",
    "*Scroll down to see the solutions after attempting the activity.*\n",
    "\n",
    "<br><br><br><br><br><br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION: Student Activity\n",
    "activations_to_test = ['sigmoid', 'tanh', 'relu']\n",
    "results = {}\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "for i, activation in enumerate(activations_to_test):\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Training with {activation.upper()} activation\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    # Create and train the network\n",
    "    network = SimpleNeuralNetwork(input_size=2, hidden_size=5, output_size=1, activation=activation)\n",
    "    losses = network.train(X_train_scaled, y_train, epochs=500, learning_rate=0.1)\n",
    "    \n",
    "    # Make predictions and calculate accuracy\n",
    "    predictions = network.predict(X_test_scaled)\n",
    "    accuracy = np.mean(predictions == y_test)\n",
    "    \n",
    "    # Store results\n",
    "    results[activation] = {\n",
    "        'final_loss': losses[-1],\n",
    "        'accuracy': accuracy,\n",
    "        'losses': losses\n",
    "    }\n",
    "    \n",
    "    # Plot loss curves\n",
    "    plt.subplot(1, 3, i+1)\n",
    "    plt.plot(losses, linewidth=2)\n",
    "    plt.title(f'{activation.upper()} - Acc: {accuracy:.3f}')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print comparison\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RESULTS COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "for activation, result in results.items():\n",
    "    print(f\"{activation.upper():>10}: Loss = {result['final_loss']:.4f}, Accuracy = {result['accuracy']:.4f}\")\n",
    "\n",
    "# Find best performing activation\n",
    "best_activation = max(results.keys(), key=lambda k: results[k]['accuracy'])\n",
    "print(f\"\\nðŸ† Best performing activation: {best_activation.upper()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Regularization Techniques\n",
    "\n",
    "Let's implement dropout regularization to prevent overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegularizedNeuralNetwork(SimpleNeuralNetwork):\n",
    "    \"\"\"Neural Network with regularization techniques\"\"\"\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, output_size, activation='relu', \n",
    "                 dropout_rate=0.0, l2_lambda=0.0):\n",
    "        super().__init__(input_size, hidden_size, output_size, activation)\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.l2_lambda = l2_lambda\n",
    "    \n",
    "    def forward(self, X, training=True):\n",
    "        \"\"\"Forward propagation with dropout\"\"\"\n",
    "        # Hidden layer\n",
    "        self.z1 = np.dot(X, self.W1) + self.b1\n",
    "        self.a1 = self.activation(self.z1)\n",
    "        \n",
    "        # Apply dropout during training\n",
    "        if training and self.dropout_rate > 0:\n",
    "            self.dropout_mask = np.random.binomial(1, 1-self.dropout_rate, size=self.a1.shape) / (1-self.dropout_rate)\n",
    "            self.a1 *= self.dropout_mask\n",
    "        \n",
    "        # Output layer\n",
    "        self.z2 = np.dot(self.a1, self.W2) + self.b2\n",
    "        self.a2 = self.activations.sigmoid(self.z2)\n",
    "        \n",
    "        return self.a2\n",
    "    \n",
    "    def compute_loss(self, y_true, y_pred):\n",
    "        \"\"\"Compute loss with L2 regularization\"\"\"\n",
    "        m = y_true.shape[0]\n",
    "        y_pred = np.clip(y_pred, 1e-8, 1 - 1e-8)\n",
    "        \n",
    "        # Cross-entropy loss\n",
    "        ce_loss = -(1/m) * np.sum(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "        \n",
    "        # L2 regularization\n",
    "        l2_loss = self.l2_lambda * (np.sum(self.W1**2) + np.sum(self.W2**2)) / (2*m)\n",
    "        \n",
    "        return ce_loss + l2_loss\n",
    "    \n",
    "    def backward(self, X, y, output):\n",
    "        \"\"\"Backward propagation with L2 regularization\"\"\"\n",
    "        m = X.shape[0]\n",
    "        \n",
    "        # Calculate gradients for output layer\n",
    "        dz2 = output - y\n",
    "        dW2 = (1/m) * np.dot(self.a1.T, dz2) + (self.l2_lambda/m) * self.W2\n",
    "        db2 = (1/m) * np.sum(dz2, axis=0, keepdims=True)\n",
    "        \n",
    "        # Calculate gradients for hidden layer\n",
    "        da1 = np.dot(dz2, self.W2.T)\n",
    "        \n",
    "        # Apply dropout mask to gradients\n",
    "        if hasattr(self, 'dropout_mask'):\n",
    "            da1 *= self.dropout_mask\n",
    "        \n",
    "        dz1 = da1 * self.activation_derivative(self.z1)\n",
    "        dW1 = (1/m) * np.dot(X.T, dz1) + (self.l2_lambda/m) * self.W1\n",
    "        db1 = (1/m) * np.sum(dz1, axis=0, keepdims=True)\n",
    "        \n",
    "        return dW1, db1, dW2, db2\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Make predictions (no dropout during inference)\"\"\"\n",
    "        output = self.forward(X, training=False)\n",
    "        return (output > 0.5).astype(int)\n",
    "\n",
    "print(\"âœ… Regularized Neural Network implemented!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Comparing Regularization Effects\n",
    "\n",
    "Let's create a more complex dataset and compare different regularization strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a more complex dataset that's prone to overfitting\n",
    "X_complex, y_complex = make_circles(n_samples=800, noise=0.1, factor=0.3, random_state=42)\n",
    "\n",
    "# Split the data\n",
    "X_train_c, X_test_c, y_train_c, y_test_c = train_test_split(\n",
    "    X_complex, y_complex, test_size=0.3, random_state=42)\n",
    "\n",
    "# Standardize\n",
    "scaler_c = StandardScaler()\n",
    "X_train_c_scaled = scaler_c.fit_transform(X_train_c)\n",
    "X_test_c_scaled = scaler_c.transform(X_test_c)\n",
    "\n",
    "# Reshape y\n",
    "y_train_c = y_train_c.reshape(-1, 1)\n",
    "y_test_c = y_test_c.reshape(-1, 1)\n",
    "\n",
    "# Visualize the complex dataset\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X_train_c[y_train_c.flatten() == 0, 0], X_train_c[y_train_c.flatten() == 0, 1], \n",
    "           c='red', alpha=0.6, label='Class 0')\n",
    "plt.scatter(X_train_c[y_train_c.flatten() == 1, 0], X_train_c[y_train_c.flatten() == 1, 1], \n",
    "           c='blue', alpha=0.6, label='Class 1')\n",
    "plt.title('Complex Dataset: Concentric Circles')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Complex dataset shape: {X_train_c_scaled.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different regularization strategies\n",
    "regularization_configs = [\n",
    "    {'name': 'No Regularization', 'dropout_rate': 0.0, 'l2_lambda': 0.0},\n",
    "    {'name': 'Dropout (0.2)', 'dropout_rate': 0.2, 'l2_lambda': 0.0},\n",
    "    {'name': 'L2 Regularization', 'dropout_rate': 0.0, 'l2_lambda': 0.01},\n",
    "    {'name': 'Dropout + L2', 'dropout_rate': 0.2, 'l2_lambda': 0.01}\n",
    "]\n",
    "\n",
    "regularization_results = {}\n",
    "\n",
    "plt.figure(figsize=(16, 4))\n",
    "\n",
    "for i, config in enumerate(regularization_configs):\n",
    "    print(f\"\\nTraining: {config['name']}\")\n",
    "    \n",
    "    # Create network with regularization\n",
    "    reg_network = RegularizedNeuralNetwork(\n",
    "        input_size=2, hidden_size=20, output_size=1, activation='relu',\n",
    "        dropout_rate=config['dropout_rate'], l2_lambda=config['l2_lambda']\n",
    "    )\n",
    "    \n",
    "    # Train the network\n",
    "    losses = reg_network.train(X_train_c_scaled, y_train_c, epochs=1000, learning_rate=0.01)\n",
    "    \n",
    "    # Evaluate\n",
    "    train_predictions = reg_network.predict(X_train_c_scaled)\n",
    "    test_predictions = reg_network.predict(X_test_c_scaled)\n",
    "    \n",
    "    train_accuracy = np.mean(train_predictions == y_train_c)\n",
    "    test_accuracy = np.mean(test_predictions == y_test_c)\n",
    "    \n",
    "    regularization_results[config['name']] = {\n",
    "        'train_accuracy': train_accuracy,\n",
    "        'test_accuracy': test_accuracy,\n",
    "        'losses': losses\n",
    "    }\n",
    "    \n",
    "    # Plot loss curve\n",
    "    plt.subplot(1, 4, i+1)\n",
    "    plt.plot(losses, linewidth=2)\n",
    "    plt.title(f\"{config['name']}\\nTest Acc: {test_accuracy:.3f}\")\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print results comparison\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"REGULARIZATION COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "print(f\"{'Method':<20} {'Train Acc':<12} {'Test Acc':<12} {'Overfitting':<12}\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "for method, result in regularization_results.items():\n",
    "    overfitting = result['train_accuracy'] - result['test_accuracy']\n",
    "    print(f\"{method:<20} {result['train_accuracy']:<12.3f} {result['test_accuracy']:<12.3f} {overfitting:<12.3f}\")\n",
    "\n",
    "# Find best method (highest test accuracy)\n",
    "best_method = max(regularization_results.keys(), \n",
    "                 key=lambda k: regularization_results[k]['test_accuracy'])\n",
    "print(f\"\\nðŸ† Best regularization method: {best_method}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Model Evaluation and Visualization\n",
    "\n",
    "Let's create a comprehensive evaluation of our best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the best model for final evaluation\n",
    "final_network = RegularizedNeuralNetwork(\n",
    "    input_size=2, hidden_size=20, output_size=1, activation='relu',\n",
    "    dropout_rate=0.2, l2_lambda=0.01\n",
    ")\n",
    "\n",
    "print(\"Training final model...\")\n",
    "final_losses = final_network.train(X_train_c_scaled, y_train_c, epochs=1000, learning_rate=0.01)\n",
    "\n",
    "# Make predictions\n",
    "train_pred = final_network.predict(X_train_c_scaled)\n",
    "test_pred = final_network.predict(X_test_c_scaled)\n",
    "\n",
    "# Calculate detailed metrics\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "train_metrics = {\n",
    "    'accuracy': accuracy_score(y_train_c, train_pred),\n",
    "    'precision': precision_score(y_train_c, train_pred),\n",
    "    'recall': recall_score(y_train_c, train_pred),\n",
    "    'f1': f1_score(y_train_c, train_pred)\n",
    "}\n",
    "\n",
    "test_metrics = {\n",
    "    'accuracy': accuracy_score(y_test_c, test_pred),\n",
    "    'precision': precision_score(y_test_c, test_pred),\n",
    "    'recall': recall_score(y_test_c, test_pred),\n",
    "    'f1': f1_score(y_test_c, test_pred)\n",
    "}\n",
    "\n",
    "# Create comprehensive visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# 1. Loss curve\n",
    "axes[0, 0].plot(final_losses, 'b-', linewidth=2)\n",
    "axes[0, 0].set_title('Training Loss Over Time')\n",
    "axes[0, 0].set_xlabel('Epoch')\n",
    "axes[0, 0].set_ylabel('Loss')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test_c, test_pred)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0, 1])\n",
    "axes[0, 1].set_title('Confusion Matrix (Test Set)')\n",
    "axes[0, 1].set_xlabel('Predicted')\n",
    "axes[0, 1].set_ylabel('Actual')\n",
    "\n",
    "# 3. Decision boundary visualization\n",
    "def plot_decision_boundary(X, y, model, ax, title):\n",
    "    h = 0.02\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                        np.arange(y_min, y_max, h))\n",
    "    \n",
    "    grid_points = np.c_[xx.ravel(), yy.ravel()]\n",
    "    Z = model.forward(grid_points, training=False)\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    ax.contourf(xx, yy, Z, levels=50, alpha=0.8, cmap='RdYlBu')\n",
    "    scatter = ax.scatter(X[:, 0], X[:, 1], c=y.flatten(), cmap='RdYlBu', edgecolors='black')\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel('Feature 1')\n",
    "    ax.set_ylabel('Feature 2')\n",
    "    return scatter\n",
    "\n",
    "plot_decision_boundary(X_test_c_scaled, y_test_c, final_network, axes[1, 0], 'Decision Boundary')\n",
    "\n",
    "# 4. Metrics comparison\n",
    "metrics_names = list(train_metrics.keys())\n",
    "train_values = list(train_metrics.values())\n",
    "test_values = list(test_metrics.values())\n",
    "\n",
    "x = np.arange(len(metrics_names))\n",
    "width = 0.35\n",
    "\n",
    "axes[1, 1].bar(x - width/2, train_values, width, label='Train', alpha=0.8)\n",
    "axes[1, 1].bar(x + width/2, test_values, width, label='Test', alpha=0.8)\n",
    "axes[1, 1].set_xlabel('Metrics')\n",
    "axes[1, 1].set_ylabel('Score')\n",
    "axes[1, 1].set_title('Model Performance Metrics')\n",
    "axes[1, 1].set_xticks(x)\n",
    "axes[1, 1].set_xticklabels([m.capitalize() for m in metrics_names])\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "axes[1, 1].set_ylim(0, 1.1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print final evaluation\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL MODEL EVALUATION\")\n",
    "print(\"=\"*60)\n",
    "print(f\"{'Metric':<15} {'Train':<10} {'Test':<10} {'Difference':<10}\")\n",
    "print(\"-\"*60)\n",
    "for metric in metrics_names:\n",
    "    diff = train_metrics[metric] - test_metrics[metric]\n",
    "    print(f\"{metric.capitalize():<15} {train_metrics[metric]:<10.3f} {test_metrics[metric]:<10.3f} {diff:<10.3f}\")\n",
    "\n",
    "print(f\"\\nðŸŽ¯ Final Test Accuracy: {test_metrics['accuracy']:.1%}\")\n",
    "print(f\"ðŸ“Š Model Complexity: {final_network.W1.size + final_network.W2.size} parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Key Takeaways\n",
    "\n",
    "### What We've Learned\n",
    "\n",
    "1. **Neural Network Fundamentals**\n",
    "   - Built a neural network from scratch\n",
    "   - Implemented forward and backward propagation\n",
    "   - Understood the role of activation functions\n",
    "\n",
    "2. **Activation Functions**\n",
    "   - Sigmoid: Good for binary classification output, but suffers from vanishing gradients\n",
    "   - Tanh: Zero-centered, still has vanishing gradient issues\n",
    "   - ReLU: Solves vanishing gradients, computationally efficient\n",
    "\n",
    "3. **Regularization Techniques**\n",
    "   - **Dropout**: Randomly sets neurons to zero during training\n",
    "   - **L2 Regularization**: Penalizes large weights\n",
    "   - Both help prevent overfitting\n",
    "\n",
    "4. **Training Best Practices**\n",
    "   - Always standardize your input features\n",
    "   - Monitor both training and validation performance\n",
    "   - Use appropriate learning rates\n",
    "   - Implement regularization for complex models\n",
    "\n",
    "### Next Steps\n",
    "- Experiment with different architectures (more layers, different sizes)\n",
    "- Try other optimization algorithms (Adam, RMSprop)\n",
    "- Explore batch normalization\n",
    "- Learn about convolutional neural networks for image data\n",
    "- Study recurrent neural networks for sequence data\n",
    "\n",
    "### ðŸš€ Challenge Problems\n",
    "1. Implement batch normalization in the neural network\n",
    "2. Add momentum to the gradient descent optimizer\n",
    "3. Create a multi-class classification version (3+ classes)\n",
    "4. Implement early stopping based on validation loss\n",
    "5. Add learning rate scheduling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Resources for Further Learning\n",
    "\n",
    "### Books\n",
    "- \"Deep Learning\" by Ian Goodfellow, Yoshua Bengio, and Aaron Courville\n",
    "- \"Pattern Recognition and Machine Learning\" by Christopher Bishop\n",
    "- \"The Elements of Statistical Learning\" by Hastie, Tibshirani, and Friedman\n",
    "\n",
    "### Online Courses\n",
    "- CS231n: Convolutional Neural Networks for Visual Recognition (Stanford)\n",
    "- Deep Learning Specialization (Coursera)\n",
    "- Fast.ai Practical Deep Learning for Coders\n",
    "\n",
    "### Frameworks to Explore\n",
    "- **TensorFlow/Keras**: Industry standard, great for production\n",
    "- **PyTorch**: Popular in research, dynamic computation graphs\n",
    "- **JAX**: Google's new framework, functional programming approach\n",
    "\n",
    "---\n",
    "\n",
    "**ðŸŽ‰ Congratulations! You've built and trained neural networks from scratch!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}