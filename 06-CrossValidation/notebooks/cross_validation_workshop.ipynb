{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cross-validation-title"
   },
   "source": [
    "# Cross Validation & Hyperparameter Tuning Workshop\n",
    "**CMSC 173 - Machine Learning**\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/njpinton/CMSC173/blob/main/06-CrossValidation/notebooks/cross_validation_workshop.ipynb)\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Learning Objectives\n",
    "\n",
    "By the end of this workshop, you will be able to:\n",
    "1. **Implement** different cross-validation methods (holdout, k-fold, LOOCV)\n",
    "2. **Compare** hyperparameter search strategies (grid search vs random search)\n",
    "3. **Interpret** learning curves and validation curves\n",
    "4. **Apply** best practices to avoid common pitfalls\n",
    "5. **Build** a complete model selection pipeline\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essential imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split, cross_val_score, KFold, StratifiedKFold,\n",
    "    GridSearchCV, RandomizedSearchCV, learning_curve, validation_curve\n",
    ")\n",
    "from sklearn.datasets import make_classification, load_breast_cancer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for better plots\n",
    "plt.style.use('default')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "print(\"‚úÖ Setup complete! Ready to explore cross-validation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üéØ Part 1: Understanding the Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why Do We Need Cross-Validation?\n",
    "\n",
    "Let's start with a simple example to understand the problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_informative=10, \n",
    "                          n_redundant=10, n_clusters_per_class=1, random_state=42)\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Dataset shape: {X.shape}\")\n",
    "print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples\")\n",
    "print(f\"Class distribution: {np.bincount(y)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Wrong Way: Using Training Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a model and check training vs test performance\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Training accuracy (overly optimistic!)\n",
    "train_accuracy = model.score(X_train, y_train)\n",
    "test_accuracy = model.score(X_test, y_test)\n",
    "\n",
    "print(f\"Training Accuracy: {train_accuracy:.3f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.3f}\")\n",
    "print(f\"Gap: {train_accuracy - test_accuracy:.3f}\")\n",
    "print(\"\\n‚ùå Training accuracy is overly optimistic!\")\n",
    "print(\"‚ùå We can't trust it for model selection.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üîÑ Part 2: Cross-Validation Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 1: Holdout Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def holdout_validation(X, y, model, test_size=0.2, random_state=42):\n",
    "    \"\"\"Simple holdout validation\"\"\"\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=random_state\n",
    "    )\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    val_score = model.score(X_val, y_val)\n",
    "    \n",
    "    return val_score\n",
    "\n",
    "# Test with different random states to show variance\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "holdout_scores = []\n",
    "\n",
    "for seed in range(0, 10):\n",
    "    score = holdout_validation(X_train, y_train, model, random_state=seed)\n",
    "    holdout_scores.append(score)\n",
    "\n",
    "print(f\"Holdout Validation Scores: {holdout_scores}\")\n",
    "print(f\"Mean: {np.mean(holdout_scores):.3f} ¬± {np.std(holdout_scores):.3f}\")\n",
    "print(f\"Range: [{min(holdout_scores):.3f}, {max(holdout_scores):.3f}]\")\n",
    "print(\"\\n‚ö†Ô∏è  High variance! Results depend on the specific split.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 2: K-Fold Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-fold cross-validation\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Try different values of k\n",
    "k_values = [3, 5, 10]\n",
    "cv_results = {}\n",
    "\n",
    "for k in k_values:\n",
    "    kfold = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "    scores = cross_val_score(model, X_train, y_train, cv=kfold, scoring='accuracy')\n",
    "    cv_results[k] = scores\n",
    "    \n",
    "    print(f\"{k}-Fold CV: {scores.mean():.3f} ¬± {scores.std():.3f}\")\n",
    "\n",
    "print(\"\\n‚úÖ K-fold CV provides more stable estimates!\")\n",
    "print(\"‚úÖ Uses all data for both training and validation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 3: Stratified K-Fold (Important for Imbalanced Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare regular vs stratified k-fold\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "stratified_kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Regular k-fold\n",
    "regular_scores = cross_val_score(model, X_train, y_train, cv=kfold, scoring='accuracy')\n",
    "\n",
    "# Stratified k-fold  \n",
    "stratified_scores = cross_val_score(model, X_train, y_train, cv=stratified_kfold, scoring='accuracy')\n",
    "\n",
    "print(f\"Regular K-Fold:    {regular_scores.mean():.3f} ¬± {regular_scores.std():.3f}\")\n",
    "print(f\"Stratified K-Fold: {stratified_scores.mean():.3f} ¬± {stratified_scores.std():.3f}\")\n",
    "print(\"\\nüí° Stratified preserves class distribution in each fold.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üîç Part 3: Hyperparameter Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search vs Random Search Comparison\n",
    "\n",
    "Let's compare these methods on a real dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a real dataset\n",
    "data = load_breast_cancer()\n",
    "X_real, y_real = data.data, data.target\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_real_scaled = scaler.fit_transform(X_real)\n",
    "\n",
    "# Split the data\n",
    "X_train_real, X_test_real, y_train_real, y_test_real = train_test_split(\n",
    "    X_real_scaled, y_real, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Real dataset: {data.feature_names[0]} and others\")\n",
    "print(f\"Shape: {X_real.shape}\")\n",
    "print(f\"Classes: {data.target_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameter space for SVM\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'gamma': [0.001, 0.01, 0.1, 1]\n",
    "}\n",
    "\n",
    "# Grid Search\n",
    "grid_search = GridSearchCV(\n",
    "    SVC(random_state=42), param_grid, \n",
    "    cv=5, scoring='accuracy', n_jobs=-1\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train_real, y_train_real)\n",
    "\n",
    "print(\"üîç Grid Search Results:\")\n",
    "print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best CV score: {grid_search.best_score_:.3f}\")\n",
    "print(f\"Total fits: {len(grid_search.cv_results_['params'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Search (same number of evaluations)\n",
    "from scipy.stats import uniform, loguniform\n",
    "\n",
    "param_dist = {\n",
    "    'C': loguniform(0.1, 100),\n",
    "    'gamma': loguniform(0.001, 1)\n",
    "}\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    SVC(random_state=42), param_dist,\n",
    "    n_iter=16, cv=5, scoring='accuracy', \n",
    "    random_state=42, n_jobs=-1\n",
    ")\n",
    "\n",
    "random_search.fit(X_train_real, y_train_real)\n",
    "\n",
    "print(\"üé≤ Random Search Results:\")\n",
    "print(f\"Best parameters: {random_search.best_params_}\")\n",
    "print(f\"Best CV score: {random_search.best_score_:.3f}\")\n",
    "print(f\"Total fits: {len(random_search.cv_results_['params'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare final test performance\n",
    "grid_test_score = grid_search.score(X_test_real, y_test_real)\n",
    "random_test_score = random_search.score(X_test_real, y_test_real)\n",
    "\n",
    "print(f\"\\nüèÜ Final Test Scores:\")\n",
    "print(f\"Grid Search:   {grid_test_score:.3f}\")\n",
    "print(f\"Random Search: {random_test_score:.3f}\")\n",
    "\n",
    "if random_test_score >= grid_test_score:\n",
    "    print(\"\\n‚ú® Random search found equally good (or better) results with same budget!\")\n",
    "else:\n",
    "    print(\"\\nüìä Grid search won this time, but random search is often competitive.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üìà Part 4: Learning & Validation Curves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Curves: Diagnosing Bias vs Variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create learning curves for different models\n",
    "def plot_learning_curves(models, X, y, title=\"Learning Curves\"):\n",
    "    fig, axes = plt.subplots(1, len(models), figsize=(15, 5))\n",
    "    if len(models) == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for i, (name, model) in enumerate(models.items()):\n",
    "        train_sizes, train_scores, val_scores = learning_curve(\n",
    "            model, X, y, cv=5, n_jobs=-1,\n",
    "            train_sizes=np.linspace(0.1, 1.0, 10),\n",
    "            random_state=42\n",
    "        )\n",
    "        \n",
    "        train_mean = np.mean(train_scores, axis=1)\n",
    "        train_std = np.std(train_scores, axis=1)\n",
    "        val_mean = np.mean(val_scores, axis=1)\n",
    "        val_std = np.std(val_scores, axis=1)\n",
    "        \n",
    "        axes[i].plot(train_sizes, train_mean, 'o-', color='blue', label='Training')\n",
    "        axes[i].fill_between(train_sizes, train_mean - train_std, train_mean + train_std, alpha=0.1, color='blue')\n",
    "        \n",
    "        axes[i].plot(train_sizes, val_mean, 'o-', color='red', label='Validation')\n",
    "        axes[i].fill_between(train_sizes, val_mean - val_std, val_mean + val_std, alpha=0.1, color='red')\n",
    "        \n",
    "        axes[i].set_title(f'{name}')\n",
    "        axes[i].set_xlabel('Training Set Size')\n",
    "        axes[i].set_ylabel('Accuracy')\n",
    "        axes[i].legend()\n",
    "        axes[i].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Compare different model complexities\n",
    "models = {\n",
    "    'Simple Model (Low Complexity)': LogisticRegression(C=0.01, random_state=42),\n",
    "    'Complex Model (High Complexity)': RandomForestClassifier(n_estimators=100, max_depth=None, random_state=42)\n",
    "}\n",
    "\n",
    "plot_learning_curves(models, X_train_real, y_train_real)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation Curves: Finding Optimal Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation curve for Random Forest max_depth\n",
    "param_range = [1, 2, 3, 5, 7, 10, 15, 20, None]\n",
    "# Convert None to a large number for plotting\n",
    "param_range_plot = [1, 2, 3, 5, 7, 10, 15, 20, 25]\n",
    "\n",
    "train_scores, val_scores = validation_curve(\n",
    "    RandomForestClassifier(n_estimators=50, random_state=42),\n",
    "    X_train_real, y_train_real,\n",
    "    param_name='max_depth', param_range=param_range,\n",
    "    cv=5, scoring='accuracy', n_jobs=-1\n",
    ")\n",
    "\n",
    "train_mean = np.mean(train_scores, axis=1)\n",
    "train_std = np.std(train_scores, axis=1)\n",
    "val_mean = np.mean(val_scores, axis=1)\n",
    "val_std = np.std(val_scores, axis=1)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(param_range_plot, train_mean, 'o-', color='blue', label='Training Accuracy')\n",
    "plt.fill_between(param_range_plot, train_mean - train_std, train_mean + train_std, alpha=0.1, color='blue')\n",
    "\n",
    "plt.plot(param_range_plot, val_mean, 'o-', color='red', label='Validation Accuracy')\n",
    "plt.fill_between(param_range_plot, val_mean - val_std, val_mean + val_std, alpha=0.1, color='red')\n",
    "\n",
    "plt.xlabel('Max Depth')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Validation Curve: Random Forest Max Depth')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xticks(param_range_plot, [str(p) if p != 25 else 'None' for p in param_range_plot])\n",
    "\n",
    "# Find optimal depth\n",
    "optimal_idx = np.argmax(val_mean)\n",
    "optimal_depth = param_range[optimal_idx]\n",
    "plt.axvline(x=param_range_plot[optimal_idx], color='green', linestyle='--', alpha=0.7)\n",
    "plt.text(param_range_plot[optimal_idx], 0.8, f'Optimal: {optimal_depth}', rotation=90)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(f\"üéØ Optimal max_depth: {optimal_depth}\")\n",
    "print(f\"Validation accuracy: {val_mean[optimal_idx]:.3f} ¬± {val_std[optimal_idx]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ‚ö†Ô∏è Part 5: Common Pitfalls & Best Practices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pitfall 1: Data Leakage\n",
    "\n",
    "**Wrong Way:** Scaling before splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚ùå WRONG: Scale entire dataset first\n",
    "X_wrong = StandardScaler().fit_transform(X_real)\n",
    "X_train_wrong, X_test_wrong, y_train_wrong, y_test_wrong = train_test_split(\n",
    "    X_wrong, y_real, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "model_wrong = SVC(C=1, gamma=0.1)\n",
    "model_wrong.fit(X_train_wrong, y_train_wrong)\n",
    "wrong_score = model_wrong.score(X_test_wrong, y_test_wrong)\n",
    "\n",
    "# ‚úÖ RIGHT: Scale after splitting\n",
    "X_train_right, X_test_right, y_train_right, y_test_right = train_test_split(\n",
    "    X_real, y_real, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_right = scaler.fit_transform(X_train_right)\n",
    "X_test_right = scaler.transform(X_test_right)  # Only transform, don't fit!\n",
    "\n",
    "model_right = SVC(C=1, gamma=0.1)\n",
    "model_right.fit(X_train_right, y_train_right)\n",
    "right_score = model_right.score(X_test_right, y_test_right)\n",
    "\n",
    "print(f\"‚ùå Wrong approach (data leakage):  {wrong_score:.3f}\")\n",
    "print(f\"‚úÖ Right approach (no leakage):   {right_score:.3f}\")\n",
    "print(f\"Difference: {wrong_score - right_score:.3f}\")\n",
    "print(\"\\nüí° Data leakage can lead to overly optimistic results!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best Practice: Complete Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Create a complete pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('classifier', SVC())\n",
    "])\n",
    "\n",
    "# Define parameter grid for pipeline\n",
    "param_grid = {\n",
    "    'classifier__C': [0.1, 1, 10],\n",
    "    'classifier__gamma': [0.001, 0.01, 0.1]\n",
    "}\n",
    "\n",
    "# Grid search with pipeline (no data leakage!)\n",
    "grid_search_pipeline = GridSearchCV(\n",
    "    pipeline, param_grid, cv=5, scoring='accuracy', n_jobs=-1\n",
    ")\n",
    "\n",
    "# Split raw data\n",
    "X_train_raw, X_test_raw, y_train_raw, y_test_raw = train_test_split(\n",
    "    X_real, y_real, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Fit pipeline\n",
    "grid_search_pipeline.fit(X_train_raw, y_train_raw)\n",
    "\n",
    "print(f\"‚úÖ Best pipeline CV score: {grid_search_pipeline.best_score_:.3f}\")\n",
    "print(f\"‚úÖ Best parameters: {grid_search_pipeline.best_params_}\")\n",
    "print(f\"‚úÖ Final test score: {grid_search_pipeline.score(X_test_raw, y_test_raw):.3f}\")\n",
    "print(\"\\nüéâ Pipeline ensures no data leakage during cross-validation!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üèÉ‚Äç‚ôÄÔ∏è **STUDENT ACTIVITY: Build Your Own Model Selection Pipeline**\n",
    "\n",
    "**Your Mission:** Build a complete model selection pipeline and find the best model for the breast cancer dataset.\n",
    "\n",
    "**Requirements:**\n",
    "1. Compare at least 3 different algorithms\n",
    "2. Use proper cross-validation (no data leakage!)\n",
    "3. Tune hyperparameters for each model\n",
    "4. Report final results with confidence intervals\n",
    "5. Make a recommendation\n",
    "\n",
    "**Time Limit:** 15 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Setup Your Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Load data and create train/test split\n",
    "# Hint: Use the breast cancer dataset we loaded earlier\n",
    "\n",
    "# Your code here:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Define Models and Parameter Grids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create pipelines for 3 different models\n",
    "# Suggestions: Logistic Regression, SVM, Random Forest\n",
    "# Don't forget to include StandardScaler!\n",
    "\n",
    "# Example structure:\n",
    "# models = {\n",
    "#     'Logistic Regression': Pipeline([...]),\n",
    "#     'SVM': Pipeline([...]),\n",
    "#     'Random Forest': Pipeline([...])\n",
    "# }\n",
    "# \n",
    "# param_grids = {\n",
    "#     'Logistic Regression': {...},\n",
    "#     'SVM': {...},\n",
    "#     'Random Forest': {...}\n",
    "# }\n",
    "\n",
    "# Your code here:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Perform Grid Search for Each Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Use GridSearchCV to find best parameters for each model\n",
    "# Print results for each model\n",
    "\n",
    "# Your code here:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Compare Models and Make Final Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Compare all models and select the best one\n",
    "# Evaluate on test set and report results\n",
    "\n",
    "# Your code here:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Your Recommendation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO: Write your analysis here**\n",
    "\n",
    "1. Which model performed best?\n",
    "2. What were the optimal hyperparameters?\n",
    "3. How confident are you in the results?\n",
    "4. What would you recommend for production use?\n",
    "\n",
    "*Your analysis goes here...*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üéØ **SOLUTION** (Don't peek until you're done!)\n",
    "\n",
    "<details>\n",
    "<summary>Click to reveal solution</summary>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION - Complete model selection pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Step 1: Data setup\n",
    "X_train_final, X_test_final, y_train_final, y_test_final = train_test_split(\n",
    "    X_real, y_real, test_size=0.2, random_state=42, stratify=y_real\n",
    ")\n",
    "\n",
    "# Step 2: Define models and parameters\n",
    "models = {\n",
    "    'Logistic Regression': Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('classifier', LogisticRegression(random_state=42, max_iter=1000))\n",
    "    ]),\n",
    "    'SVM': Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('classifier', SVC(random_state=42))\n",
    "    ]),\n",
    "    'Random Forest': Pipeline([\n",
    "        ('classifier', RandomForestClassifier(random_state=42))\n",
    "    ])\n",
    "}\n",
    "\n",
    "param_grids = {\n",
    "    'Logistic Regression': {\n",
    "        'classifier__C': [0.1, 1, 10, 100]\n",
    "    },\n",
    "    'SVM': {\n",
    "        'classifier__C': [0.1, 1, 10],\n",
    "        'classifier__gamma': ['scale', 0.001, 0.01]\n",
    "    },\n",
    "    'Random Forest': {\n",
    "        'classifier__n_estimators': [50, 100, 200],\n",
    "        'classifier__max_depth': [None, 10, 20]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Step 3: Perform grid search\n",
    "results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nüîç Training {name}...\")\n",
    "    \n",
    "    grid_search = GridSearchCV(\n",
    "        model, param_grids[name], \n",
    "        cv=5, scoring='accuracy', n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    grid_search.fit(X_train_final, y_train_final)\n",
    "    \n",
    "    results[name] = {\n",
    "        'best_score': grid_search.best_score_,\n",
    "        'best_params': grid_search.best_params_,\n",
    "        'best_model': grid_search.best_estimator_\n",
    "    }\n",
    "    \n",
    "    print(f\"Best CV score: {grid_search.best_score_:.3f}\")\n",
    "    print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "\n",
    "# Step 4: Compare models\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"üìä FINAL COMPARISON\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "best_model_name = max(results.keys(), key=lambda x: results[x]['best_score'])\n",
    "best_model = results[best_model_name]['best_model']\n",
    "\n",
    "for name, result in results.items():\n",
    "    test_score = result['best_model'].score(X_test_final, y_test_final)\n",
    "    marker = \"üèÜ\" if name == best_model_name else \"  \"\n",
    "    print(f\"{marker} {name:20} | CV: {result['best_score']:.3f} | Test: {test_score:.3f}\")\n",
    "\n",
    "# Step 5: Final evaluation\n",
    "final_test_score = best_model.score(X_test_final, y_test_final)\n",
    "y_pred = best_model.predict(X_test_final)\n",
    "\n",
    "print(f\"\\nüéâ WINNER: {best_model_name}\")\n",
    "print(f\"Final Test Accuracy: {final_test_score:.3f}\")\n",
    "print(f\"Best Parameters: {results[best_model_name]['best_params']}\")\n",
    "\n",
    "print(\"\\nüìã Detailed Classification Report:\")\n",
    "print(classification_report(y_test_final, y_pred, target_names=data.target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üéì Summary & Key Takeaways\n",
    "\n",
    "### What We Learned:\n",
    "\n",
    "‚úÖ **Cross-Validation Methods:**\n",
    "- Holdout: Fast but high variance\n",
    "- K-fold: Best general purpose method\n",
    "- Stratified K-fold: Essential for imbalanced data\n",
    "\n",
    "‚úÖ **Hyperparameter Search:**\n",
    "- Grid search: Exhaustive but expensive\n",
    "- Random search: Often just as good with less computation\n",
    "\n",
    "‚úÖ **Diagnostic Tools:**\n",
    "- Learning curves: Diagnose bias vs variance\n",
    "- Validation curves: Find optimal hyperparameters\n",
    "\n",
    "‚úÖ **Best Practices:**\n",
    "- Use pipelines to prevent data leakage\n",
    "- Never use test set for model selection\n",
    "- Report confidence intervals\n",
    "- Choose CV method based on data characteristics\n",
    "\n",
    "### Next Steps:\n",
    "1. Practice with your own datasets\n",
    "2. Try advanced methods like Bayesian optimization\n",
    "3. Explore nested cross-validation for unbiased model selection\n",
    "4. Learn about time series cross-validation\n",
    "\n",
    "---\n",
    "\n",
    "**Happy validating! üöÄ**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "colab": {
   "provenance": [],
   "toc_visible": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}