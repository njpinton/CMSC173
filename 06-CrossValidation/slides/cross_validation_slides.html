<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>cross_validation_slides</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            max-width: 1200px;
            margin: 0 auto;
            padding: 20px;
            background-color: #f5f5f5;
        }
        .page {
            background-color: white;
            margin: 20px 0;
            padding: 40px;
            box-shadow: 0 2px 8px rgba(0,0,0,0.1);
            border-radius: 4px;
            min-height: 400px;
        }
        .page-number {
            color: #666;
            font-size: 14px;
            margin-bottom: 20px;
            border-bottom: 1px solid #ddd;
            padding-bottom: 10px;
        }
        h1, h2, h3 {
            color: #333;
        }
        .page-content {
            line-height: 1.8;
            white-space: pre-wrap;
            font-family: 'Courier New', monospace;
            font-size: 12pt;
        }
        .note {
            color: #666;
            font-style: italic;
            margin: 10px 0;
            padding: 10px;
            background-color: #f9f9f9;
            border-left: 3px solid #ccc;
        }
    </style>
</head>
<body>
<div class="note">
    This is a text-only version of the slides. For the full visual experience with images and formatting, please refer to the PDF version.
</div>

<div class="page">
<div class="page-number">Page 1 of 48</div>
<div class="page-content">
Cross Validation &amp; Hyperparameter Tuning
CMSC 173 - Machine Learning
Course Lecture
1

</div>
</div>
<div class="page">
<div class="page-number">Page 2 of 48</div>
<div class="page-content">
Outline
How to Validate Models?
Holdout Validation
K-Fold Cross-Validation
Other Variants
Hyper-parameter Search Methods
Grid Search
Random Search
Bayesian Optimization (Optuna)
Learning Curves &amp; Validation Curves
Bias-Variance Tradeoff
Best Practices &amp; Guidelines
Summary
2

</div>
</div>
<div class="page">
<div class="page-number">Page 3 of 48</div>
<div class="page-content">
How to Validate Models?

</div>
</div>
<div class="page">
<div class="page-number">Page 4 of 48</div>
<div class="page-content">
The Model Validation Problem
Central Question: How do we assess model performance?
Training Error
• Error on data used to train the model
• Always optimistically biased
• Cannot be used for model selection
• Misleading indicator
Test Error
• Error on unseen data
• True indicator of generalization
• Should only be used once
• Gold standard
Problem
We need to estimate generalization performance without using the test set during model development and
hyperparameter tuning.
3

</div>
</div>
<div class="page">
<div class="page-number">Page 5 of 48</div>
<div class="page-content">
The Three-Way Data Split
Original Dataset
60%
20%
20%
Training Set
Used to train models
Validation Set
Model selection &amp; tuning
Test Set
Final evaluation
Model Development
Training + Validation
Final Test
Unbiased estimate
Key Insight
Validation set acts as a proxy for the test set during model development, allowing us to:
4

</div>
</div>
<div class="page">
<div class="page-number">Page 6 of 48</div>
<div class="page-content">
Holdout Validation

</div>
</div>
<div class="page">
<div class="page-number">Page 7 of 48</div>
<div class="page-content">
Holdout Validation Method
Procedure:
1. Randomly split data into training and validation sets
2. Train model on training set
3. Evaluate on validation set
4. Select best performing model/hyperparameters
5. Final evaluation on held-out test set
Common Split Ratios:
• 70% Train, 15% Validation, 15% Test
• 60% Train, 20% Validation, 20% Test
• 80% Train, 10% Validation, 10% Test
Mathematical Formulation
Given dataset D = {(xi, yi)}n
i=1:
Dtrain ∪Dval ∪Dtest = D
(1)
Dtrain ∩Dval ∩Dtest = ∅
(2)
|Dtrain| = αn
(3)
|Dval| = βn
(4)
|Dtest| = (1 −α −β)n
(5)
where typically α ∈[0.6, 0.8]
Advantage
Simple, fast, and provides unbiased estimate when validation set is large enough.
5

</div>
</div>
<div class="page">
<div class="page-number">Page 8 of 48</div>
<div class="page-content">
Holdout Validation: Advantages &amp; Disadvantages
Advantages
• Computational efficiency: Train model only
once
• Simple implementation: Straightforward to
understand and code
• Fast evaluation: Quick assessment of model
performance
• Realistic simulation: Mimics real-world
deployment scenario
When to Use
• Large datasets (n &gt; 10,000)
• Computationally expensive models
• Time-sensitive applications
Disadvantages
• High variance: Performance estimate depends
on specific split
• Data inefficiency: Reduces training data size
• Unreliable for small datasets: Estimates can be
very noisy
• Potential bias: Unlucky splits can mislead model
selection
When NOT to Use
• Small datasets (n &lt; 1,000)
• Need for robust performance estimates
• Critical applications requiring high confidence
Key Takeaway
Holdout validation trades robustness for computational efficiency.
6

</div>
</div>
<div class="page">
<div class="page-number">Page 9 of 48</div>
<div class="page-content">
K-Fold Cross-Validation

</div>
</div>
<div class="page">
<div class="page-number">Page 10 of 48</div>
<div class="page-content">
K-Fold Cross-Validation Method
5-Fold Cross-Validation Example
Original Data:
Fold 1:
Valid
Train
Fold 2:
Valid
Fold 3:
Valid
Fold 4:
Valid
Fold 5:
Valid
Average Results
CVscore = 1
k
Pk
i=1 Scorei
Core Idea
Use all data for both training and validation by systematically rotating which portion serves as the validation
set.
7

</div>
</div>
<div class="page">
<div class="page-number">Page 11 of 48</div>
<div class="page-content">
K-Fold Cross-Validation: Mathematical Formulation
Algorithm:
1. Partition dataset D into k equal-sized folds: D = D1 ∪D2 ∪· · · ∪Dk
2. For each fold i = 1, . . . , k:
• Training set: D(i)
train = D \ Di
• Validation set: D(i)
val = Di
• Train model f (i) on D(i)
train
• Compute validation error: E (i) = L(f (i), D(i)
val)
3. Estimate generalization error: ˆECV = 1
k
Pk
i=1 E (i)
Error Estimation
ˆECV = 1
k
k
X
i=1
L(f (i), Di)
Var( ˆECV ) = 1
k2
k
X
i=1
Var(E (i))
Standard Error
SE( ˆECV ) =
v
u
u
t 1
k
k
X
i=1

E (i) −ˆECV
2
Confidence interval: ˆECV ± 1.96 · SE
8

</div>
</div>
<div class="page">
<div class="page-number">Page 12 of 48</div>
<div class="page-content">
Choice of k in K-Fold Cross-Validation
Common Values:
• k = 5: Good balance, widely used
• k = 10: Most popular, good bias-variance tradeoff
• k = n (LOOCV): Lowest bias, highest variance
Bias-Variance Tradeoff:
Bias ∝k −1
k
(decreases with k)
(6)
Variance ∝correlation between folds
(7)
Computation ∝k (increases with k)
(8)
Choosing k
Small k (k = 3-5):
• Higher bias
• Lower variance
• Less computation
• Good for large datasets
Large k (k = 10-20):
• Lower bias
• Higher variance
• More computation
• Good for small datasets
Recommendation
Use k = 10 as default choice. It provides good bias-variance tradeoff for most practical scenarios.
9

</div>
</div>
<div class="page">
<div class="page-number">Page 13 of 48</div>
<div class="page-content">
K-Fold vs Holdout: Performance Comparison
Key Observation
K-fold cross-validation provides more robust and reliable performance estimates, especially important for
model comparison and selection.
10

</div>
</div>
<div class="page">
<div class="page-number">Page 14 of 48</div>
<div class="page-content">
Other Variants

</div>
</div>
<div class="page">
<div class="page-number">Page 15 of 48</div>
<div class="page-content">
Leave-One-Out Cross-Validation (LOOCV)
Mathematical Formulation
LOOCV is k-fold CV with k = n:
ˆELOOCV = 1
n
n
X
i=1
L(f (−i), (xi, yi))
(
i)
Properties
• Bias: Nearly unbiased (uses n-1 samples)
• Variance: High (high correlation between folds)
• Computation: Expensive (n model fits)
• Deterministic: No randomness in splits
11

</div>
</div>
<div class="page">
<div class="page-number">Page 16 of 48</div>
<div class="page-content">
Stratified K-Fold Cross-Validation
Key Advantage
Maintains class distribution across folds, crucial for:
I
b l
d d
E
h f ld h
i
f
ll
l
12

</div>
</div>
<div class="page">
<div class="page-number">Page 17 of 48</div>
<div class="page-content">
Specialized Cross-Validation Methods
Time Series Cross-Validation
Problem: Standard CV violates temporal order
Solution: Forward chaining
• Fold 1: Train[1:100], Test[101:150]
• Fold 2: Train[1:150], Test[151:200]
• Fold 3: Train[1:200], Test[201:250]
Preserves: Temporal dependencies
Group K-Fold
Problem: Data points are grouped (e.g., patients,
images from same source)
Solution: Ensure entire groups stay together in
splits
Prevents: Data leakage between folds
Monte Carlo Cross-Validation
Method: Random sampling approach
• Randomly split data multiple times
• Average performance across all splits
• More flexible than k-fold
Advantage: Can control train/validation ratio
Nested Cross-Validation
Purpose: Unbiased model selection + evaluation
Structure:
• Outer loop: Performance estimation
• Inner loop: Hyperparameter tuning
Result: True generalization estimate
Guideline
Choose validation method based on data characteristics and problem constraints.
13

</div>
</div>
<div class="page">
<div class="page-number">Page 18 of 48</div>
<div class="page-content">
Hyper-parameter Search Methods

</div>
</div>
<div class="page">
<div class="page-number">Page 19 of 48</div>
<div class="page-content">
The Hyperparameter Optimization Problem
Goal: Find optimal hyperparameters λ∗
λ∗= arg min
λ∈Λ
ˆECV (λ)
(9)
ˆECV (λ) = 1
k
k
X
i=1
L(f (i)
λ , Di)
(10)
where:
• λ: hyperparameter vector
• Λ: search space
• f (i)
λ : model trained with λ on fold i
Examples
SVM: λ = (C, γ)
Λ = [10−3, 103] × [10−6, 101]
Random Forest:
λ = (nest, depth, features)
Neural Network:
λ = (lr, batch, layers, dropout)
Challenge
Hyperparameter spaces are often high-dimensional, mixed-type, and expensive to evaluate.
14

</div>
</div>
<div class="page">
<div class="page-number">Page 20 of 48</div>
<div class="page-content">
Grid Search

</div>
</div>
<div class="page">
<div class="page-number">Page 21 of 48</div>
<div class="page-content">
Grid Search Method
Core Principle
Exhaustive search over a discrete grid of hyperparameter combinations.
15

</div>
</div>
<div class="page">
<div class="page-number">Page 22 of 48</div>
<div class="page-content">
Grid Search: Mathematical Formulation
Algorithm:
1. Define search grid: Λgrid = {λ1, λ2, . . . , λm}
2. For each λj ∈Λgrid:
• Compute ˆECV (λj) using k-fold cross-validation
• Store result: (λj, ˆECV (λj))
3. Return: λ∗= arg minλj ˆECV (λj)
Grid Construction
For parameter λi with range [ai, bi]:
Linear spacing:
λ(j)
i
= ai + j −1
ni −1 (bi −ai)
Log spacing (preferred for scale-invariant
parameters):
λ(j)
i
= ai ·
 bi
ai
 j−1
ni −1
Computational Complexity
Total evaluations: Qp
i=1 ni
With k-fold CV:
Total cost = k ·
p
Y
i=1
ni · Ctrain
where:
• p: number of hyperparameters
• ni: grid points for parameter i
• Ctrain: cost of training one model
16

</div>
</div>
<div class="page">
<div class="page-number">Page 23 of 48</div>
<div class="page-content">
Grid Search: Advantages &amp; Disadvantages
Advantages
• Exhaustive: Guarantees finding the best
combination on the grid
• Parallel: Evaluations are independent
• Reproducible: Deterministic results
• Simple: Easy to implement and understand
• Complete coverage: Systematic exploration
Best Practices
• Use log-uniform grids for scale parameters (C, γ,
learning rate)
• Start with coarse grid, refine around promising
regions
• Use domain knowledge for range selection
Disadvantages
• Curse of dimensionality: Exponential growth
with parameters
• Computational cost: Can be prohibitively
expensive
• Grid limitations: Optimal values might lie
between grid points
• Uniform sampling: Wastes effort in unpromising
regions
• Manual tuning: Requires careful grid design
When NOT to Use
• High-dimensional hyperparameter spaces (p &gt; 4)
• Expensive model training (Ctrain very large)
• Continuous optimization required
Rule of Thumb
Grid search is practical for ≤3-4 hyperparameters with reasonable computational budget.
17

</div>
</div>
<div class="page">
<div class="page-number">Page 24 of 48</div>
<div class="page-content">
Random Search

</div>
</div>
<div class="page">
<div class="page-number">Page 25 of 48</div>
<div class="page-content">
Random Search vs Grid Search
Key Insight
Random search often finds better solutions than grid search with the same computational budget, especially in
high dimensions.
18

</div>
</div>
<div class="page">
<div class="page-number">Page 26 of 48</div>
<div class="page-content">
Random Search: Method &amp; Theory
Algorithm:
1. Define parameter distributions: λi ∼pi(λi)
2. For j = 1 to m (budget):
• Sample: λ(j) ∼Qp
i=1 pi(λi)
• Evaluate: ˆECV (λ(j))
3. Return: λ∗= arg minj ˆECV (λ(j))
Common Distributions
Continuous parameters:
• Uniform: λ ∼U(a, b)
• Log-uniform: log λ ∼U(log a, log b)
• Normal: λ ∼N(µ, σ2)
Discrete parameters:
• Uniform: λ ∼Uniform{v1, v2, . . . , vk}
Theoretical Advantage
If only d out of p parameters matter:
Grid search: Need np points for n-point resolution
Random search: Need only n points for same
effective resolution in important dimensions
Probability of good solution:
P(success) = 1 −(1 −ϵ)m
where ϵ is fraction of good region
19

</div>
</div>
<div class="page">
<div class="page-number">Page 27 of 48</div>
<div class="page-content">
Random Search: Advantages &amp; Best Practices
Advantages
• Dimension-friendly: Scales well to high
dimensions
• Efficient: Often finds good solutions quickly
• Flexible: Works with any parameter distribution
• Parallel: Evaluations are independent
• Anytime: Can stop early if budget is limited
• Robust: Less sensitive to irrelevant parameters
Disadvantages
• No guarantee: May miss optimal regions
• Random: Results vary between runs
• Distribution-dependent: Requires good prior
knowledge
Best Practices
• Use log-uniform for scale parameters:
C ∼LogUniform(10−3, 103)
• Set reasonable bounds based on domain
knowledge
• Run multiple times and take the best result
• Start with random search, then refine with local
methods
Implementation
1: Set parameter ranges and distributions
2: for i = 1 to ntrials do
3:
Sample λ(i) from distributions
4:
Evaluate f (λ(i)) using CV
5: end for
6: return arg mini f (λ(i))
Recommendation
Use random search as default for &gt; 3 hyperparameters or when computational budget is limited.
20

</div>
</div>
<div class="page">
<div class="page-number">Page 28 of 48</div>
<div class="page-content">
Bayesian Optimization (Optuna)

</div>
</div>
<div class="page">
<div class="page-number">Page 29 of 48</div>
<div class="page-content">
Bayesian Optimization Intuition
21

</div>
</div>
<div class="page">
<div class="page-number">Page 30 of 48</div>
<div class="page-content">
Bayesian Optimization: Mathematical Framework
Core Components:
1. Surrogate Model
Gaussian Process: f (λ) ∼GP(m(λ), k(λ, λ′))
Predictive distribution:
µn(λ) = kT
n K −1
n
yn
(11)
σ2
n(λ) = k(λ, λ) −kT
n K −1
n
kn
(12)
where:
• kn: kernel vector at λ
• Kn: kernel matrix of observed points
• yn: observed function values
2. Acquisition Function
Expected Improvement (EI):
EI(λ) = E[max(f ∗−f (λ), 0)]
(13)
= (f ∗−µ(λ))Φ(z) + σ(λ)ϕ(z)
(14)
where z = f ∗−µ(λ)
σ(λ)
, f ∗is current best
Upper Confidence Bound (UCB):
UCB(λ) = µ(λ) + κσ(λ)
Algorithm:
1. Initialize with random evaluations
2. Fit GP to observed data {(λi, f (λi))}n
i=1
3. Find λn+1 = arg maxλ Acquisition(λ)
4. Evaluate f (λn+1) and update data
5. Repeat until budget exhausted
22

</div>
</div>
<div class="page">
<div class="page-number">Page 31 of 48</div>
<div class="page-content">
Optuna: Tree-Structured Parzen Estimator (TPE)
TPE Algorithm:
1. Split observations by performance quantile γ:
• Good: L = {λ : f (λ) &lt; Qγ}
• Bad: G = {λ : f (λ) ≥Qγ}
2. Model densities:
• p(λ|L): density of good configurations
• p(λ|G): density of bad configurations
3. Acquisition function:
a(λ) = p(λ|L)
p(λ|G)
4. Select: λnext = arg maxλ a(λ)
TPE Advantages
• Efficient: Lower computational cost than
GP
• Flexible: Handles mixed parameter types
• Scalable: Works well in high dimensions
• Robust: Less sensitive to
hyperparameters
Optuna Features
• Pruning: Early stopping of unpromising
trials
• Multi-objective optimization
• Distributed optimization
• Integration with ML frameworks
Key Insight
TPE focuses sampling on regions where good configurations are dense relative to bad ones.
23

</div>
</div>
<div class="page">
<div class="page-number">Page 32 of 48</div>
<div class="page-content">
Hyperparameter Search: Performance Comparison
Method Comparison
• Grid Search: Systematic but inefficient
• Random Search: Better exploration, good
baseline
• Bayesian Optimization: Fastest convergence,
most efficient
Recommendations
• Few parameters (≤3): Grid search
• Many parameters (&gt; 3): Random search
• Expensive evaluations: Bayesian optimization
(Optuna)
• Mixed types: Optuna TPE
Best Practice
S
h
d
h f
k b
l
h
B
f
fi
24

</div>
</div>
<div class="page">
<div class="page-number">Page 33 of 48</div>
<div class="page-content">
Learning Curves &amp; Validation Curves

</div>
</div>
<div class="page">
<div class="page-number">Page 34 of 48</div>
<div class="page-content">
Learning Curves
Purpose
Learning curves help diagnose bias (underfitting) vs variance (overfitting) and determine if more data would
help.
25

</div>
</div>
<div class="page">
<div class="page-number">Page 35 of 48</div>
<div class="page-content">
Learning Curve Analysis
High Bias (Underfitting)
Symptoms:
• Training and validation curves converge
• Both curves plateau at suboptimal level
• Large training error
• Small gap between curves
Solutions:
• Increase model complexity
• Add more features
• Reduce regularization
• Use more flexible model
High Variance (Overfitting)
Symptoms:
• Large gap between training and validation curves
• Training error very low
• Validation error high
• Curves don’t converge
Solutions:
• Get more training data
• Reduce model complexity
• Increase regularization
• Use ensemble methods
Mathematical Insight
For a learning algorithm with m training examples:
Generalization Error = Bias2 + Variance + Noise
Learning curves show how this decomposes as m increases.
26

</div>
</div>
<div class="page">
<div class="page-number">Page 36 of 48</div>
<div class="page-content">
Validation Curves
27

</div>
</div>
<div class="page">
<div class="page-number">Page 37 of 48</div>
<div class="page-content">
Interpreting Validation Curves
Typical Pattern Analysis:
Regularization Parameters
(e.g., SVM C parameter, Ridge α)
Low values (high regularization):
• High bias, low variance
• Training and validation errors both high
• Underfitting
High values (low regularization):
• Low bias, high variance
• Large gap between curves
• Overfitting
Optimal region: Minimum validation error
Model Complexity Parameters
(e.g., Random Forest depth, SVM γ)
Mathematical relationship:
Complexity ∝Overfitting Risk
Sweet spot identification:
1. Find minimum validation error
2. Check if training error is reasonable
3. Ensure curves are not diverging rapidly
Cross-validation confidence: Error bars show ±1
standard deviation across CV folds
Best Practice
Always plot both training and validation curves with error bars to make informed hyperparameter choices.
28

</div>
</div>
<div class="page">
<div class="page-number">Page 38 of 48</div>
<div class="page-content">
Bias-Variance Tradeoff

</div>
</div>
<div class="page">
<div class="page-number">Page 39 of 48</div>
<div class="page-content">
Bias-Variance Decomposition
29

</div>
</div>
<div class="page">
<div class="page-number">Page 40 of 48</div>
<div class="page-content">
Bias-Variance Mathematical Framework
Decomposition of Expected Test Error:
For a target function f (x) and prediction ˆf (x):
E[(ˆf (x) −f (x))2] = Bias2[ˆf (x)] + Var[ˆf (x)] + σ2
where:
Bias[ˆf (x)] = E[ˆf (x)] −f (x)
(15)
Var[ˆf (x)] = E[(ˆf (x) −E[ˆf (x)])2]
(16)
σ2 = Irreducible error (noise)
(17)
Cross-Validation &amp; Bias-Variance
CV helps estimate:
• Bias: Through training error analysis
• Variance: Through CV fold variability
• Optimal complexity: Bias-variance tradeoff
point
ˆE
Bi
2 + V
+
2
Model Selection Strategy
1. Compute ˆECV (λ) for different complexities
2. Plot learning/validation curves
3. Identify minimum of validation curve
4. Check bias-variance indicators:
• Training error (bias)
• CV std deviation (variance)
30

</div>
</div>
<div class="page">
<div class="page-number">Page 41 of 48</div>
<div class="page-content">
Best Practices &amp; Guidelines

</div>
</div>
<div class="page">
<div class="page-number">Page 42 of 48</div>
<div class="page-content">
Cross-Validation Best Practices
Data Splitting
• Stratify for classification tasks
• Preserve temporal order for time series
• Group-aware splitting for clustered data
• Test set isolation: Never use for model selection
Hyperparameter Search
• Start with random search for baseline
• Use Bayesian optimization for expensive
evaluations
• Log-uniform sampling for scale parameters
• Early stopping for unpromising configurations
Statistical Considerations
• Report mean ± std of CV scores
• Use paired t-tests for model comparison
• Account for multiple testing when comparing
many models
• Consider McNemar’s test for classification
Computational Efficiency
• Parallel evaluation of CV folds
• Caching of expensive preprocessing steps
• Progressive validation for large datasets
• Approximate methods when exact CV is too
expensive
Golden Rule
Never use the test set for any decision making during model development. Reserve it solely for final
performance evaluation.
31

</div>
</div>
<div class="page">
<div class="page-number">Page 43 of 48</div>
<div class="page-content">
Common Pitfalls &amp; How to Avoid Them
Data Leakage
Problem: Information from validation/test leaks
into training
Examples:
• Scaling on entire dataset before splitting
• Feature selection using all data
• Temporal leakage in time series
Solution: Apply preprocessing within each CV fold
Look-ahead Bias
Problem: Using future information for prediction
Example: Standard CV on time series data
Solution: Time series CV with forward chaining
Selection Bias
Problem: Multiple testing without correction
Example: Comparing 100 models, reporting best
CV score
Solution: Bonferroni correction or separate
validation set for selection
Inappropriate CV Method
Problems:
• LOOCV with large datasets (unstable)
• Standard CV with imbalanced data
• Ignoring data structure (groups, time)
Solution: Choose CV method based on data
characteristics
Validation
Always ask: ”Does my validation procedure realistically simulate deployment conditions?”
32

</div>
</div>
<div class="page">
<div class="page-number">Page 44 of 48</div>
<div class="page-content">
Hyperparameter Search Space
33

</div>
</div>
<div class="page">
<div class="page-number">Page 45 of 48</div>
<div class="page-content">
Summary

</div>
</div>
<div class="page">
<div class="page-number">Page 46 of 48</div>
<div class="page-content">
Summary: Model Validation Hierarchy
Model Validation Hierarchy:
Original Dataset
↓
Train / Validation / Test Split
↓
↓
↓
Training Set
Validation Set
Test Set
(60-80%)
(10-20%)
(10-20%)
↓
↓
↓
Cross-Validation
Hyperparameter
Final
(Model Selection)
Tuning
Evaluation
Best Practice Workflow
1. Split data
2. Use CV for model selection
3. Tune hyperparameters on validation set
4. Final evaluation on test set (once!)
34

</div>
</div>
<div class="page">
<div class="page-number">Page 47 of 48</div>
<div class="page-content">
Key Takeaways
Validation Methods
• Holdout: Fast but high variance
• K-fold CV: Best general-purpose method
• LOOCV: Unbiased but expensive
• Stratified: Essential for imbalanced data
• Time series CV: Preserves temporal order
Method Selection Guide
• Small data (n ¡ 1000): 10-fold CV or LOOCV
• Large data (n ¿ 10000): 5-fold CV or holdout
• Imbalanced: Stratified k-fold
• Time series: Forward chaining
• Grouped data: Group k-fold
Hyperparameter Search
• Grid search: ≤3 parameters, comprehensive
• Random search: ¿3 parameters, efficient baseline
• Bayesian optimization: Expensive evaluations,
smart search
• Use proper distributions: Log-uniform for scale
parameters
Critical Principles
• Never use test set for model selection
• Avoid data leakage at all costs
• Report confidence intervals (mean ± std)
• Match validation to deployment conditions
• Use learning curves to diagnose bias/variance
Remember
Cross-validation is not just about getting a number—it’s about making principled decisions about model
selection, hyperparameter tuning, and understanding model behavior.
35

</div>
</div>
<div class="page">
<div class="page-number">Page 48 of 48</div>
<div class="page-content">
Questions?
35

</div>
</div>

</body>
</html>
