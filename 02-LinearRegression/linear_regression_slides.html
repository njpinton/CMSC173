<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Linear Regression and Gradient Descent - CMSC 173</title>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <style>
        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            line-height: 1.8;
            max-width: 1200px;
            margin: 0 auto;
            padding: 20px;
            padding-bottom: 100px;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            scroll-behavior: smooth;
        }
        .slide {
            background: white;
            margin: 40px 0;
            padding: 40px;
            padding-bottom: 60px;
            border-radius: 12px;
            box-shadow: 0 10px 30px rgba(0,0,0,0.2);
            transition: transform 0.3s ease, box-shadow 0.3s ease;
            scroll-margin-top: 20px;
            min-height: 500px;
            position: relative;
            display: flex;
            flex-direction: column;
        }
        .slide:hover {
            transform: translateY(-2px);
            box-shadow: 0 15px 40px rgba(0,0,0,0.25);
        }
        .slide-header {
            background: linear-gradient(135deg, #3949AB 0%, #283593 100%);
            color: white;
            padding: 30px 40px;
            margin: -40px -40px 40px -40px;
            border-radius: 12px 12px 0 0;
        }
        h1 {
            margin: 0;
            font-size: 2.5em;
            font-weight: 700;
            letter-spacing: -0.5px;
        }
        h2 {
            color: #3949AB;
            border-bottom: 3px solid #3949AB;
            padding-bottom: 15px;
            margin-top: 0;
            margin-bottom: 25px;
            font-size: 1.8em;
            font-weight: 600;
        }
        h3 {
            color: #283593;
            margin-top: 25px;
            margin-bottom: 15px;
            font-size: 1.3em;
            font-weight: 600;
        }
        .slide-number {
            position: absolute;
            bottom: 20px;
            right: 40px;
            text-align: right;
            color: #999;
            font-size: 1em;
            font-weight: 500;
        }
        ul {
            list-style-type: disc;
            padding-left: 30px;
            margin: 20px 0;
        }
        ul li {
            margin: 12px 0;
            line-height: 1.8;
        }
        .numbered-list {
            list-style-type: decimal;
        }
        .numbered-list li {
            margin: 15px 0;
        }
        table {
            border-collapse: collapse;
            width: 100%;
            margin: 20px 0;
            table-layout: auto;
            overflow-x: auto;
            display: block;
        }
        th, td {
            border: 1px solid #ddd;
            padding: 12px;
            text-align: left;
        }
        th {
            background-color: #3949AB;
            color: white;
        }
        tr:nth-child(even) {
            background-color: #f2f2f2;
        }
        .highlight {
            background-color: #FFF9C4;
        }
        .box {
            border: 2px solid #3949AB;
            padding: 20px;
            margin: 25px 0;
            background-color: #E8EAF6;
            border-radius: 8px;
            box-shadow: 0 2px 8px rgba(57, 73, 171, 0.1);
        }
        .definition {
            background-color: #E3F2FD;
            padding: 20px;
            margin: 25px 0;
            border-left: 5px solid #2196F3;
            border-radius: 4px;
            box-shadow: 0 2px 8px rgba(33, 150, 243, 0.1);
        }
        code {
            background-color: #f4f4f4;
            padding: 2px 6px;
            border-radius: 3px;
            font-family: 'Courier New', monospace;
        }
        .algorithm {
            background-color: #f9f9f9;
            border: 1px solid #ddd;
            padding: 20px;
            margin: 25px 0;
            font-family: 'Courier New', monospace;
            border-radius: 6px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.05);
        }

        /* Navigation Styles */
        .nav-container {
            position: fixed;
            bottom: 0;
            left: 0;
            right: 0;
            background: rgba(255, 255, 255, 0.98);
            box-shadow: 0 -4px 20px rgba(0,0,0,0.15);
            padding: 15px 20px;
            z-index: 1000;
            backdrop-filter: blur(10px);
        }

        .nav-content {
            max-width: 1200px;
            margin: 0 auto;
            display: flex;
            justify-content: space-between;
            align-items: center;
            gap: 20px;
        }

        .nav-button {
            background: linear-gradient(135deg, #3949AB 0%, #283593 100%);
            color: white;
            border: none;
            padding: 12px 30px;
            border-radius: 8px;
            font-size: 16px;
            font-weight: 600;
            cursor: pointer;
            transition: all 0.3s ease;
            box-shadow: 0 4px 12px rgba(57, 73, 171, 0.3);
            display: flex;
            align-items: center;
            gap: 8px;
        }

        .nav-button:hover:not(:disabled) {
            transform: translateY(-2px);
            box-shadow: 0 6px 16px rgba(57, 73, 171, 0.4);
            background: linear-gradient(135deg, #283593 0%, #1a237e 100%);
        }

        .nav-button:active:not(:disabled) {
            transform: translateY(0);
        }

        .nav-button:disabled {
            background: #ccc;
            cursor: not-allowed;
            box-shadow: none;
        }

        .progress-container {
            flex: 1;
            text-align: center;
        }

        .progress-text {
            font-size: 16px;
            color: #333;
            font-weight: 600;
            margin-bottom: 8px;
        }

        .progress-bar {
            width: 100%;
            height: 8px;
            background-color: #e0e0e0;
            border-radius: 4px;
            overflow: hidden;
        }

        .progress-fill {
            height: 100%;
            background: linear-gradient(90deg, #3949AB 0%, #283593 100%);
            transition: width 0.3s ease;
            border-radius: 4px;
        }

        p {
            margin: 15px 0;
        }

        strong {
            color: #283593;
        }

        /* Content wrapper for flex layout */
        .slide-content {
            flex: 1;
            display: flex;
            flex-direction: column;
        }

        /* Prevent overflow of math formulas */
        .MathJax {
            overflow-x: auto;
            overflow-y: hidden;
            max-width: 100%;
        }

        /* Ensure consistent spacing in boxes */
        .box p:last-child,
        .definition p:last-child {
            margin-bottom: 0;
        }

        /* Add breathing room for content before slide number */
        .slide > *:last-child:not(.slide-number) {
            margin-bottom: 40px;
        }
    </style>
</head>
<body>

<!-- Slide 1 -->
<div class="slide" id="slide-1">
    <div class="slide-content">
        <div class="slide-header">
            <h1>Linear Regression and Gradient Descent</h1>
            <p>CMSC 173<br>September 7, 2025</p>
        </div>
    </div>
    <div class="slide-number">1 / 25</div>
</div>

<!-- Slide 2 -->
<div class="slide" id="slide-2">
    <div class="slide-content">
        <h2>Outline</h2>
        <ol class="numbered-list">
            <li>What is Linear Regression?</li>
            <li>Linear Least Squares Method</li>
            <li>Linear Regression using Gradient Descent</li>
            <li>Summary</li>
        </ol>
    </div>
    <div class="slide-number">2 / 25</div>
</div>

<!-- Slide 3 -->
<div class="slide" id="slide-3">
    <div class="slide-content">
        <h2>What is Linear Regression?</h2>

        <div class="definition">
            <strong>Definition:</strong> Linear regression is a method to model the relationship between a dependent variable \(y\) and one or more independent variables \(x_1, x_2, \ldots, x_n\).
        </div>

        <h3>Motivation:</h3>
        <ul>
            <li>Predict future outcomes based on observed data.</li>
            <li>Understand the strength and form of relationships between variables.</li>
            <li>Provide a simple, interpretable baseline model.</li>
        </ul>

        <h3>Mathematical Formulation:</h3>
        <p>$$\hat{y} = \theta_0 + \theta_1 x_1 + \theta_2 x_2 + \cdots + \theta_n x_n$$</p>

        <h3>Definitions:</h3>
        <ul>
            <li>\(y \in \mathbb{R}\): target (dependent) variable.</li>
            <li>\(\hat{y} \in \mathbb{R}\): predicted value of \(y\).</li>
            <li>\(x_j \in \mathbb{R}\): \(j\)-th independent feature, \(j = 1, \ldots, n\).</li>
            <li>\(n\): number of features (excluding bias term).</li>
            <li>\(\theta_0 \in \mathbb{R}\): intercept (bias term).</li>
            <li>\(\theta_j \in \mathbb{R}\): coefficient for feature \(x_j\).</li>
        </ul>
    </div>
    <div class="slide-number">3 / 25</div>
</div>

<!-- Slide 4 -->
<div class="slide" id="slide-4">
    <div class="slide-content">
        <h2>Linear Regression: Vectorized Form</h2>

        <h3>Vectorized Formulation:</h3>
        <p>$$\hat{y} = \mathbf{x}^\top \boldsymbol{\theta}$$</p>

        <h3>Definitions:</h3>
        <ul>
            <li>\(\mathbf{x} = [1 \quad x_1 \quad x_2 \quad \cdots \quad x_n]^\top \in \mathbb{R}^{(n+1)}\): feature vector with bias term.</li>
            <li>\(\boldsymbol{\theta} = [\theta_0 \quad \theta_1 \quad \theta_2 \quad \cdots \quad \theta_n]^\top \in \mathbb{R}^{(n+1)}\): parameter vector.</li>
            <li>\(\hat{y} \in \mathbb{R}\): predicted output (scalar).</li>
        </ul>

        <h3>Generalization to \(m\) samples:</h3>
        <p>$$\hat{\mathbf{y}} = \mathbf{X}\boldsymbol{\theta}, \quad \mathbf{X} \in \mathbb{R}^{m \times (n+1)}, \quad \hat{\mathbf{y}} \in \mathbb{R}^m$$</p>

        <h3>Expanded Form:</h3>
        <p>$$\mathbf{X} = \begin{bmatrix}
        1 & x_1^{(1)} & x_2^{(1)} & \cdots & x_n^{(1)} \\
        1 & x_1^{(2)} & x_2^{(2)} & \cdots & x_n^{(2)} \\
        \vdots & \vdots & \vdots & \ddots & \vdots \\
        1 & x_1^{(m)} & x_2^{(m)} & \cdots & x_n^{(m)}
        \end{bmatrix}, \quad
        \hat{\mathbf{y}} = \begin{bmatrix}
        \hat{y}^{(1)} \\
        \hat{y}^{(2)} \\
        \vdots \\
        \hat{y}^{(m)}
        \end{bmatrix}$$</p>

        <h3>Row Expansion:</h3>
        <p>$$\hat{y}^{(i)} = \theta_0 + \theta_1 x_1^{(i)} + \theta_2 x_2^{(i)} + \cdots + \theta_n x_n^{(i)}, \quad i = 1, \ldots, m$$</p>
    </div>
    <div class="slide-number">4 / 25</div>
</div>

<!-- Slide 5 -->
<div class="slide" id="slide-5">
    <div class="slide-content">
        <h2>Example Dataset: House Prices</h2>

        <h3>Scenario:</h3>
        <p>Predict house price (\(y\)) using features such as:</p>
        <ul>
            <li>\(x_1\) = floor area (in m²)</li>
            <li>\(x_2\) = number of bedrooms</li>
            <li>\(x_3\) = age of the house (in years)</li>
        </ul>

        <table>
            <tr>
                <th>House</th>
                <th>Floor Area (m²)</th>
                <th>Bedrooms</th>
                <th>Age (yrs)</th>
                <th>Price ($1000s)</th>
            </tr>
            <tr>
                <td>1</td>
                <td>85</td>
                <td>2</td>
                <td>10</td>
                <td>150</td>
            </tr>
            <tr class="highlight">
                <td>2</td>
                <td>120</td>
                <td>3</td>
                <td>5</td>
                <td>230</td>
            </tr>
            <tr>
                <td>3</td>
                <td>60</td>
                <td>2</td>
                <td>20</td>
                <td>100</td>
            </tr>
            <tr>
                <td>4</td>
                <td>200</td>
                <td>4</td>
                <td>2</td>
                <td>400</td>
            </tr>
            <tr>
                <td>5</td>
                <td>150</td>
                <td>3</td>
                <td>8</td>
                <td>280</td>
            </tr>
        </table>

        <h3>Model:</h3>
        <p>$$y \approx \hat{y} = \theta_0 + \theta_1 x_1 + \theta_2 x_2 + \theta_3 x_3$$</p>

        <h3>Vector Form for House 2:</h3>
        <p>$$\mathbf{x}^{(2)} = \begin{bmatrix} 1 \\ 120 \\ 3 \\ 5 \end{bmatrix}, \quad \hat{y}^{(2)} = \mathbf{x}^{(2)\top}\boldsymbol{\theta}$$</p>
    </div>
    <div class="slide-number">5 / 25</div>
</div>

<!-- Slide 6 -->
<div class="slide" id="slide-6">
    <div class="slide-content">
        <h2>Solving Linear Regression with Least Squares</h2>

        <h3>Cost Function:</h3>
        <p>$$J(\boldsymbol{\theta}) = \frac{1}{2m} \sum_{i=1}^{m} (y^{(i)} - \hat{y}^{(i)})^2$$</p>

        <h3>Closed-form solution:</h3>
        <p>$$\hat{\boldsymbol{\theta}} = (\mathbf{X}^\top \mathbf{X})^{-1} \mathbf{X}^\top \mathbf{y}$$</p>

        <p>Minimizes squared error between predictions \(\hat{y}\) and true values \(y\).</p>
    </div>
    <div class="slide-number">6 / 25</div>
</div>

<!-- Slide 7 -->
<div class="slide" id="slide-7">
    <div class="slide-content">
        <h2>Least Squares: Step-by-step Guide</h2>

        <div class="box">
            <strong>1. Form the design matrix.</strong> Construct
            <p>$$\mathbf{X} = \begin{bmatrix}
            1 & x_1^{(1)} & \cdots & x_n^{(1)} \\
            1 & x_1^{(2)} & \cdots & x_n^{(2)} \\
            \vdots & \vdots & \ddots & \vdots \\
            1 & x_1^{(m)} & \cdots & x_n^{(m)}
            \end{bmatrix} \in \mathbb{R}^{m \times (n+1)},$$</p>
            <p>where \(m\) is the number of training samples, \(n\) is the number of features (excluding bias), and row \(i\) is \(\mathbf{x}^{(i)\top}\).</p>
        </div>

        <div class="box">
            <strong>2. Compute normal-system components.</strong>
            <p>$$\mathbf{A} = \mathbf{X}^\top \mathbf{X} \in \mathbb{R}^{(n+1) \times (n+1)}, \quad \mathbf{b} = \mathbf{X}^\top \mathbf{y} \in \mathbb{R}^{(n+1)}.$$</p>
            <p>Here \(\mathbf{y} \in \mathbb{R}^m\) is the target vector with entries \(y^{(i)}\).</p>
        </div>

        <div class="box">
            <strong>3. Solve the linear system (normal equations).</strong>
            <p>$$\mathbf{A}\boldsymbol{\theta} = \mathbf{b}.$$</p>
            <p>If \(\mathbf{A}\) is invertible (i.e. \(\mathbf{X}\) has full column rank \(n + 1\)), compute</p>
            <p>$$\hat{\boldsymbol{\theta}} = \mathbf{A}^{-1}\mathbf{b} = (\mathbf{X}^\top \mathbf{X})^{-1} \mathbf{X}^\top \mathbf{y},$$</p>
            <p>where \(\hat{\boldsymbol{\theta}} \in \mathbb{R}^{(n+1)}\) is the estimated parameter vector (including \(\theta_0\)).</p>
        </div>

        <div class="box">
            <strong>4. Computational costs (rough).</strong> forming \(\mathbf{X}^\top \mathbf{X}\): \(O(m(n + 1)^2)\); solving \(\mathbf{A}\boldsymbol{\theta} = \mathbf{b}\) by direct methods: \(O((n + 1)^3)\).
        </div>
    </div>
    <div class="slide-number">7 / 25</div>
</div>

<!-- Slide 8 -->
<div class="slide" id="slide-8">
    <div class="slide-content">
        <h2>Least Squares: Derivation of Normal Equations</h2>

        <h3>Cost function (least squares):</h3>
        <p>$$J(\boldsymbol{\theta}) = \frac{1}{2m} \|\mathbf{X}\boldsymbol{\theta} - \mathbf{y}\|_2^2$$</p>
        <p>where \(\mathbf{X} \in \mathbb{R}^{m \times (n+1)}\) is the design matrix, \(\boldsymbol{\theta} \in \mathbb{R}^{(n+1)}\) the parameter vector, \(\mathbf{y} \in \mathbb{R}^m\) the target vector, and \(m\) the number of samples.</p>

        <h3>Expand the quadratic:</h3>
        <p>$$J(\boldsymbol{\theta}) = \frac{1}{2m} (\boldsymbol{\theta}^\top \mathbf{X}^\top \mathbf{X} \boldsymbol{\theta} - 2\boldsymbol{\theta}^\top \mathbf{X}^\top \mathbf{y} + \mathbf{y}^\top \mathbf{y}).$$</p>
        <p>Each symbol as above; note \(\mathbf{X}^\top \mathbf{X} \in \mathbb{R}^{(n+1) \times (n+1)}\).</p>

        <h3>Gradient w.r.t. \(\boldsymbol{\theta}\):</h3>
        <p>$$\nabla_{\boldsymbol{\theta}} J(\boldsymbol{\theta}) = \frac{1}{m} (\mathbf{X}^\top \mathbf{X} \boldsymbol{\theta} - \mathbf{X}^\top \mathbf{y}).$$</p>
        <p>(Standard matrix calculus: derivative of \(\frac{1}{2}\boldsymbol{\theta}^\top \mathbf{M}\boldsymbol{\theta}\) is \(\mathbf{M}\boldsymbol{\theta}\) for symmetric \(\mathbf{M}\).)</p>

        <h3>Set gradient to zero (first-order optimality):</h3>
        <p>$$\mathbf{X}^\top \mathbf{X} \hat{\boldsymbol{\theta}} = \mathbf{X}^\top \mathbf{y}.$$</p>
        <p>This is the <em>normal equations</em>. Symbols: \(\mathbf{X}^\top \mathbf{X} \in \mathbb{R}^{(n+1) \times (n+1)}\), \(\mathbf{X}^\top \mathbf{y} \in \mathbb{R}^{(n+1)}\).</p>

        <h3>Hessian and convexity:</h3>
        <p>$$\nabla_{\boldsymbol{\theta}}^2 J(\boldsymbol{\theta}) = \frac{1}{m} \mathbf{X}^\top \mathbf{X},$$</p>
        <p>which is positive semidefinite. If \(\mathbf{X}\) has full column rank (\(\text{rank}(\mathbf{X}) = n + 1\)) then \(\mathbf{X}^\top \mathbf{X}\) is positive definite and the solution is unique.</p>

        <h3>Closed-form solution (when invertible):</h3>
        <div class="box">
            <p>$$\hat{\boldsymbol{\theta}} = (\mathbf{X}^\top \mathbf{X})^{-1} \mathbf{X}^\top \mathbf{y}$$</p>
        </div>
    </div>
    <div class="slide-number">8 / 25</div>
</div>

<!-- Slide 9 -->
<div class="slide" id="slide-9">
    <div class="slide-content">
        <h2>Least Squares: Model</h2>

        <h3>Linear regression model (scalar form):</h3>
        <p>$$\hat{y}^{(i)} = \theta_0 + \sum_{j=1}^{n} \theta_j x_j^{(i)}, \quad i = 1, \ldots, m,$$</p>

        <p>where</p>
        <ul>
            <li>\(m\) = number of training samples,</li>
            <li>\(n\) = number of features (excluding bias),</li>
            <li>\(x_j^{(i)}\) = \(j\)-th feature of sample \(i\),</li>
            <li>\(\theta_0\) = intercept, \(\theta_j\) = parameters,</li>
            <li>\(\hat{y}^{(i)}\) = predicted output.</li>
        </ul>
    </div>
    <div class="slide-number">9 / 25</div>
</div>

<!-- Slide 10 -->
<div class="slide" id="slide-10">
    <div class="slide-content">
        <h2>Least Squares: Cost Function</h2>

        <h3>Least squares objective:</h3>
        <p>$$J(\boldsymbol{\theta}) = \frac{1}{2m} \sum_{i=1}^{m} (y^{(i)} - \hat{y}^{(i)})^2$$</p>

        <h3>Substitute model:</h3>
        <p>$$J(\boldsymbol{\theta}) = \frac{1}{2m} \sum_{i=1}^{m} \left(y^{(i)} - \theta_0 - \sum_{j=1}^{n} \theta_j x_j^{(i)}\right)^2,$$</p>

        <p>where \(y^{(i)}\) is the observed target.</p>
    </div>
    <div class="slide-number">10 / 25</div>
</div>

<!-- Slide 11 -->
<div class="slide" id="slide-11">
    <div class="slide-content">
        <h2>Derivative with respect to \(\theta_0\)</h2>

        <p>$$\frac{\partial J}{\partial \theta_0} = -\frac{1}{m} \sum_{i=1}^{m} \left(y^{(i)} - \theta_0 - \sum_{j=1}^{n} \theta_j x_j^{(i)}\right).$$</p>

        <h3>Set to zero (optimality condition):</h3>
        <p>$$\sum_{i=1}^{m} y^{(i)} = m\theta_0 + \sum_{j=1}^{n} \theta_j \sum_{i=1}^{m} x_j^{(i)}.$$</p>

        <h3>Define means:</h3>
        <p>$$\bar{y} = \frac{1}{m} \sum_{i=1}^{m} y^{(i)}, \quad \bar{x}_j = \frac{1}{m} \sum_{i=1}^{m} x_j^{(i)}.$$</p>

        <h3>So:</h3>
        <p>$$\theta_0 = \bar{y} - \sum_{j=1}^{n} \theta_j \bar{x}_j.$$</p>
    </div>
    <div class="slide-number">11 / 25</div>
</div>

<!-- Slide 12 -->
<div class="slide" id="slide-12">
    <div class="slide-content">
        <h2>Derivative with respect to \(\theta_k\)</h2>

        <p>For \(k = 1, \ldots, n\):</p>

        <p>$$\frac{\partial J}{\partial \theta_k} = -\frac{1}{m} \sum_{i=1}^{m} x_k^{(i)} \left(y^{(i)} - \theta_0 - \sum_{j=1}^{n} \theta_j x_j^{(i)}\right).$$</p>

        <h3>Set to zero:</h3>
        <p>$$\sum_{i=1}^{m} x_k^{(i)} y^{(i)} = \theta_0 \sum_{i=1}^{m} x_k^{(i)} + \sum_{j=1}^{n} \theta_j \sum_{i=1}^{m} x_k^{(i)} x_j^{(i)}.$$</p>

        <p>These \(n\) equations plus the \(\theta_0\) equation form a linear system for the unknowns \(\theta_0, \ldots, \theta_n\).</p>
    </div>
    <div class="slide-number">12 / 25</div>
</div>

<!-- Slide 13 -->
<div class="slide" id="slide-13">
    <div class="slide-content">
        <h2>Special Case: Simple Linear Regression (\(n = 1\))</h2>

        <h3>Model:</h3>
        <p>$$\hat{y}^{(i)} = \theta_0 + \theta_1 x^{(i)}, \quad i = 1, \ldots, m.$$</p>

        <h3>Solution:</h3>
        <p>$$\theta_1 = \frac{\sum_{i=1}^{m} (x^{(i)} - \bar{x})(y^{(i)} - \bar{y})}{\sum_{i=1}^{m} (x^{(i)} - \bar{x})^2},$$</p>

        <p>$$\theta_0 = \bar{y} - \theta_1 \bar{x},$$</p>

        <p>where \(\bar{x} = \frac{1}{m} \sum_i x^{(i)}\), \(\bar{y} = \frac{1}{m} \sum_i y^{(i)}\).</p>
    </div>
    <div class="slide-number">13 / 25</div>
</div>

<!-- Slide 14 -->
<div class="slide" id="slide-14">
    <div class="slide-content">
        <h2>Worked Example: House Prices vs Floor Area</h2>

        <h3>Dataset (\(m = 5\), \(n = 1\)):</h3>
        <table>
            <tr>
                <th>House No.</th>
                <th>Floor Area (\(x_1\))</th>
                <th>Bedrooms (\(x_2\))</th>
                <th>Age (\(x_3\))</th>
                <th>Price (\(y\))</th>
            </tr>
            <tr>
                <td>1</td>
                <td>85</td>
                <td>2</td>
                <td>10</td>
                <td>200</td>
            </tr>
            <tr>
                <td>2</td>
                <td>120</td>
                <td>3</td>
                <td>5</td>
                <td>250</td>
            </tr>
            <tr>
                <td>3</td>
                <td>60</td>
                <td>2</td>
                <td>20</td>
                <td>180</td>
            </tr>
            <tr>
                <td>4</td>
                <td>200</td>
                <td>4</td>
                <td>8</td>
                <td>300</td>
            </tr>
            <tr>
                <td>5</td>
                <td>150</td>
                <td>3</td>
                <td>15</td>
                <td>220</td>
            </tr>
        </table>
    </div>
    <div class="slide-number">14 / 25</div>
</div>

<!-- Slide 15 -->
<div class="slide" id="slide-15">
    <div class="slide-content">
        <h2>Worked Example: Step-by-Step Solution</h2>

        <h3>Step 1: Means</h3>
        <p>$$\bar{x} = \frac{1}{m} \sum_{i=1}^{m} x^{(i)}, \quad \bar{y} = \frac{1}{m} \sum_{i=1}^{m} y^{(i)}$$</p>

        <p>For \(m = 5\):</p>
        <p>$$\bar{x} = \frac{85+120+60+200+150}{5} = 123, \quad \bar{y} = \frac{200+250+180+300+220}{5} = 230$$</p>

        <h3>Step 2: Slope</h3>
        <p>$$\theta_1 = \frac{\sum_{i=1}^{m} (x^{(i)} - \bar{x})(y^{(i)} - \bar{y})}{\sum_{i=1}^{m} (x^{(i)} - \bar{x})^2}$$</p>

        <p>$$\theta_1 = \frac{(85 - 123)(200 - 230) + \cdots + (150 - 123)(220 - 230)}{(85 - 123)^2 + \cdots + (150 - 123)^2} \approx 0.74$$</p>

        <h3>Step 3: Intercept</h3>
        <p>$$\theta_0 = \bar{y} - \theta_1 \bar{x} = 230 - (0.74)(123) \approx 139.0$$</p>

        <p>$$\therefore \hat{y} = 139.0 + 0.74x$$</p>
    </div>
    <div class="slide-number">15 / 25</div>
</div>

<!-- Slide 16 -->
<div class="slide" id="slide-16">
    <div class="slide-content">
        <h2>Worked Example: Scatterplot with Residuals</h2>

        <h3>Fitted Line:</h3>
        <p>$$\hat{y} = 139.0 + 0.74 x$$</p>

        <h3>Residuals:</h3>
        <p>$$r^{(i)} = y^{(i)} - \hat{y}^{(i)}$$</p>

        <p>Residuals are larger here because the correlation is weaker.</p>
    </div>
    <div class="slide-number">16 / 25</div>
</div>

<!-- Slide 17 -->
<div class="slide" id="slide-17">
    <div class="slide-content">
        <h2>Gradient Descent (Scalar Form): Model</h2>

        <h3>Linear model (scalar form):</h3>
        <p>$$\hat{y}^{(i)} = \theta_0 + \sum_{j=1}^{n} \theta_j x_j^{(i)}, \quad i = 1, \ldots, m.$$</p>

        <p>where</p>
        <ul>
            <li>\(m\) is the number of training samples,</li>
            <li>\(n\) is the number of features (excluding bias),</li>
            <li>\(x_j^{(i)}\) is the \(j\)-th feature of sample \(i\),</li>
            <li>\(\theta_0\) is the intercept (bias), \(\theta_j\) are parameters,</li>
            <li>\(\hat{y}^{(i)}\) is the predicted output for sample \(i\).</li>
        </ul>
    </div>
    <div class="slide-number">17 / 25</div>
</div>

<!-- Slide 18 -->
<div class="slide" id="slide-18">
    <div class="slide-content">
        <h2>Cost Function (Least Squares) – Scalar</h2>

        <h3>Least-squares objective (scalar summation form):</h3>
        <p>$$J(\boldsymbol{\theta}) = \frac{1}{2m} \sum_{i=1}^{m} (y^{(i)} - \hat{y}^{(i)})^2 = \frac{1}{2m} \sum_{i=1}^{m} \left(y^{(i)} - \theta_0 - \sum_{j=1}^{n} \theta_j x_j^{(i)}\right)^2.$$</p>

        <p>where</p>
        <ul>
            <li>\(y^{(i)}\) is the observed (true) target for sample \(i\),</li>
            <li>\(\hat{y}^{(i)}\) defined in previous frame,</li>
            <li>\(J(\boldsymbol{\theta})\) is the scalar cost to minimize over \(\boldsymbol{\theta} \in \mathbb{R}^{n+1}\).</li>
        </ul>
    </div>
    <div class="slide-number">18 / 25</div>
</div>

<!-- Slide 19 -->
<div class="slide" id="slide-19">
    <div class="slide-content">
        <h2>Gradient: derivative w.r.t. \(\theta_0\) (detailed)</h2>

        <p>Compute partial derivative of \(J\) with respect to \(\theta_0\) using the chain rule:</p>

        <p>$$\frac{\partial J}{\partial \theta_0} = \frac{\partial}{\partial \theta_0} \left(\frac{1}{2m} \sum_{i=1}^{m} \left(y^{(i)} - \theta_0 - \sum_{j=1}^{n} \theta_j x_j^{(i)}\right)^2\right)$$</p>

        <p>$$= -\frac{1}{m} \sum_{i=1}^{m} \left(y^{(i)} - \theta_0 - \sum_{j=1}^{n} \theta_j x_j^{(i)}\right).$$</p>

        <p>where</p>
        <ul>
            <li>the derivative used: \(\frac{d}{du} \frac{1}{2} u^2 = u\),</li>
            <li>the inner residual for sample \(i\) is \(r^{(i)} = y^{(i)} - \hat{y}^{(i)}\).</li>
        </ul>

        <h3>Set to zero (normal-equation form for bias):</h3>
        <p>$$\sum_{i=1}^{m} y^{(i)} = m\theta_0 + \sum_{j=1}^{n} \theta_j \sum_{i=1}^{m} x_j^{(i)}.$$</p>

        <p>where sums are as defined above.</p>
    </div>
    <div class="slide-number">19 / 25</div>
</div>

<!-- Slide 20 -->
<div class="slide" id="slide-20">
    <div class="slide-content">
        <h2>Gradient: derivative w.r.t. \(\theta_k\) (general \(k\))</h2>

        <p>For \(k \in \{1, \ldots, n\}\) apply the chain rule:</p>

        <p>$$\frac{\partial J}{\partial \theta_k} = -\frac{1}{m} \sum_{i=1}^{m} x_k^{(i)} \left(y^{(i)} - \theta_0 - \sum_{j=1}^{n} \theta_j x_j^{(i)}\right).$$</p>

        <p>where</p>
        <ul>
            <li>\(x_k^{(i)}\) multiplies the residual because \(\partial(\theta_j x_j^{(i)})/\partial \theta_k = x_k^{(i)}\),</li>
            <li>the term is the scalar inner product between feature \(k\) and residuals.</li>
        </ul>

        <p>Equivalently, define per-sample residual \(r^{(i)} = y^{(i)} - \hat{y}^{(i)}\), then</p>

        <p>$$\frac{\partial J}{\partial \theta_k} = -\frac{1}{m} \sum_{i=1}^{m} x_k^{(i)} r^{(i)}.$$</p>
    </div>
    <div class="slide-number">20 / 25</div>
</div>

<!-- Slide 21 -->
<div class="slide" id="slide-21">
    <div class="slide-content">
        <h2>Gradient Descent: Coordinate-wise Update</h2>

        <h3>Batch gradient descent update (scalar coordinate form):</h3>
        <p>$$\theta_k \leftarrow \theta_k - \alpha \frac{\partial J}{\partial \theta_k}, \quad k = 0, 1, \ldots, n.$$</p>

        <p>where</p>
        <ul>
            <li>\(\alpha > 0\) is the learning rate (step size),</li>
            <li>\(\frac{\partial J}{\partial \theta_k} = -\frac{1}{m} \sum_{i=1}^{m} x_k^{(i)} r^{(i)}\)</li>
        </ul>

        <h3>Substitute the derivative to obtain the explicit update:</h3>
        <div class="box">
            <p>$$\theta_k \leftarrow \theta_k + \frac{\alpha}{m} \sum_{i=1}^{m} x_k^{(i)} (y^{(i)} - \hat{y}^{(i)})$$</p>
        </div>

        <p>where \(x_0^{(i)} \equiv 1\) for the bias (so the update for \(\theta_0\) uses \(x_0^{(i)} = 1\)).</p>
    </div>
    <div class="slide-number">21 / 25</div>
</div>

<!-- Slide 22 -->
<div class="slide" id="slide-22">
    <div class="slide-content">
        <h2>Algorithm: Batch Gradient Descent (Scalar Pseudocode)</h2>

        <div class="algorithm">
            <p><strong>1.</strong> <strong>Inputs:</strong> dataset \(\{(x_{1:n}^{(i)}, y^{(i)})\}_{i=1}^m\), learning rate \(\alpha\), max iterations \(T\).</p>

            <p><strong>2.</strong> <strong>Initialize:</strong> \(\theta_k^{(0)} = 0\) (or small random) for \(k = 0, \ldots, n\).</p>

            <p><strong>3.</strong> <strong>For</strong> \(t = 0, 1, \ldots, T - 1\) <strong>do:</strong></p>
            <ul style="list-style-type: none;">
                <li><strong>1.</strong> Compute predictions \(\hat{y}^{(i)} = \theta_0^{(t)} + \sum_{j=1}^{n} \theta_j^{(t)} x_j^{(i)}\) for all \(i\).</li>
                <li><strong>2.</strong> For each \(k = 0, \ldots, n\) compute gradient:
                    <p>$$g_k^{(t)} = -\frac{1}{m} \sum_{i=1}^{m} x_k^{(i)} (y^{(i)} - \hat{y}^{(i)}).$$</p>
                </li>
                <li><strong>3.</strong> Update:
                    <p>$$\theta_k^{(t+1)} = \theta_k^{(t)} - \alpha g_k^{(t)}.$$</p>
                </li>
            </ul>

            <p><strong>4.</strong> <strong>Return</strong> \(\boldsymbol{\theta}^{(T)}\).</p>
        </div>

        <p><strong>Definitions:</strong> \(g_k^{(t)}\) is the partial derivative at iteration \(t\); \(x_0^{(i)} = 1\).</p>
    </div>
    <div class="slide-number">22 / 25</div>
</div>

<!-- Slide 23 -->
<div class="slide" id="slide-23">
    <div class="slide-content">
        <h2>Variants: SGD and Mini-batch (Scalar)</h2>

        <h3>Stochastic Gradient Descent (SGD) — per-sample update:</h3>
        <p>$$\theta_k \leftarrow \theta_k + \alpha x_k^{(i)} (y^{(i)} - \hat{y}^{(i)}),$$</p>

        <p>where</p>
        <ul>
            <li>the update uses one sample \(i\) (or randomly sampled \(i\)),</li>
            <li>note: this expression omits the \(1/m\) factor (conventional SGD uses per-sample learning rate).</li>
        </ul>

        <h3>Mini-batch of indices \(\mathcal{B}\):</h3>
        <p>$$\theta_k \leftarrow \theta_k + \frac{\alpha}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} x_k^{(i)} (y^{(i)} - \hat{y}^{(i)}),$$</p>

        <p>where \(|\mathcal{B}|\) is batch size. <strong>Definitions:</strong> \(x_0^{(i)} = 1\).</p>
    </div>
    <div class="slide-number">23 / 25</div>
</div>

<!-- Slide 24 -->
<div class="slide" id="slide-24">
    <div class="slide-content">
        <h2>Practical Considerations (scalar viewpoint)</h2>

        <ul>
            <li><strong>Feature scaling:</strong> Standardize each feature \(x_j\) (zero mean, unit std) so typical \(\sum_i (x_j^{(i)})^2\) are comparable; this reduces \(\lambda_{\max}\) and improves conditioning of \(H\). <strong>Definitions:</strong> standardize via \(\tilde{x}_j^{(i)} = (x_j^{(i)} - \bar{x}_j)/s_j\).</li>

            <li><strong>Bias update:</strong> If features are centered then \(\theta_0\) update simplifies to \(\theta_0 \leftarrow \theta_0 + \frac{\alpha}{m} \sum_i r^{(i)}\) and decouples from other \(\theta_j\) updates.</li>

            <li><strong>Initialization:</strong> Use zeros or small random values for \(\theta_k\).</li>

            <li><strong>Stopping criteria:</strong> small \(\|\Delta\boldsymbol{\theta}\|\), small relative change in \(J\), or fixed iterations.</li>

            <li><strong>Regularization (ridge) in scalar form:</strong> add \(\frac{\lambda}{2m} \sum_{j=1}^{n} \theta_j^2\) (do not regularize \(\theta_0\) typically). Then gradient becomes:
                <p>$$\frac{\partial J_{\text{ridge}}}{\partial \theta_k} = -\frac{1}{m} \sum_{i=1}^{m} x_k^{(i)} r^{(i)} + \frac{\lambda}{m} \theta_k \quad (k \geq 1).$$</p>
            </li>
        </ul>
    </div>
    <div class="slide-number">24 / 25</div>
</div>

<!-- Slide 25 -->
<div class="slide" id="slide-25">
    <div class="slide-content">
        <h2>Computational Cost (scalar) & Summary</h2>

        <h3>Per-iteration cost (batch GD):</h3>
        <ul>
            <li>computing all predictions \(\hat{y}^{(i)}\): \(O(m(n + 1))\) operations,</li>
            <li>computing gradients \(g_k\): additional \(O(m(n + 1))\),</li>
            <li>total per iteration: \(O(mn)\) (dominant).</li>
        </ul>

        <h3>Summary (scalar form):</h3>
        <ul>
            <li><strong>Model:</strong> \(\hat{y}^{(i)} = \theta_0 + \sum_{j=1}^{n} \theta_j x_j^{(i)}\).</li>
            <li><strong>Gradient (component):</strong> \(\frac{\partial J}{\partial \theta_k} = -\frac{1}{m} \sum_{i=1}^{m} x_k^{(i)} r^{(i)}\).</li>
            <li><strong>Update:</strong> \(\theta_k \leftarrow \theta_k + \frac{\alpha}{m} \sum_{i=1}^{m} x_k^{(i)} r^{(i)}\).</li>
            <li><strong>Convergence:</strong> choose \(\alpha\) with \(0 < \alpha < 2/\lambda_{\max}(H)\), where \(H_{k\ell} = \frac{1}{m} \sum_i x_k^{(i)} x_\ell^{(i)}\).</li>
        </ul>
    </div>
    <div class="slide-number">25 / 25</div>
</div>

<!-- Navigation Bar -->
<div class="nav-container">
    <div class="nav-content">
        <button class="nav-button" id="prevBtn" onclick="navigateSlide(-1)">
            <span>◄</span> Previous
        </button>

        <div class="progress-container">
            <div class="progress-text" id="progressText">Slide 1 of 25</div>
            <div class="progress-bar">
                <div class="progress-fill" id="progressFill"></div>
            </div>
        </div>

        <button class="nav-button" id="nextBtn" onclick="navigateSlide(1)">
            Next <span>►</span>
        </button>
    </div>
</div>

<script>
    let currentSlide = 1;
    const totalSlides = 25;

    function updateNavigation() {
        // Update button states
        document.getElementById('prevBtn').disabled = currentSlide === 1;
        document.getElementById('nextBtn').disabled = currentSlide === totalSlides;

        // Update progress text
        document.getElementById('progressText').textContent = `Slide ${currentSlide} of ${totalSlides}`;

        // Update progress bar
        const progress = ((currentSlide - 1) / (totalSlides - 1)) * 100;
        document.getElementById('progressFill').style.width = progress + '%';
    }

    function navigateSlide(direction) {
        const newSlide = currentSlide + direction;

        if (newSlide >= 1 && newSlide <= totalSlides) {
            currentSlide = newSlide;

            // Scroll to the slide
            const slideElement = document.getElementById(`slide-${currentSlide}`);
            if (slideElement) {
                slideElement.scrollIntoView({ behavior: 'smooth', block: 'start' });
            }

            updateNavigation();
        }
    }

    function goToSlide(slideNumber) {
        if (slideNumber >= 1 && slideNumber <= totalSlides) {
            currentSlide = slideNumber;

            const slideElement = document.getElementById(`slide-${currentSlide}`);
            if (slideElement) {
                slideElement.scrollIntoView({ behavior: 'smooth', block: 'start' });
            }

            updateNavigation();
        }
    }

    // Keyboard navigation
    document.addEventListener('keydown', function(e) {
        if (e.key === 'ArrowLeft' || e.key === 'ArrowUp') {
            navigateSlide(-1);
        } else if (e.key === 'ArrowRight' || e.key === 'ArrowDown' || e.key === ' ') {
            e.preventDefault(); // Prevent space from scrolling
            navigateSlide(1);
        } else if (e.key === 'Home') {
            goToSlide(1);
        } else if (e.key === 'End') {
            goToSlide(totalSlides);
        }
    });

    // Detect which slide is in view on scroll
    let isScrolling;
    window.addEventListener('scroll', function() {
        clearTimeout(isScrolling);

        isScrolling = setTimeout(function() {
            const slides = document.querySelectorAll('.slide');
            const windowHeight = window.innerHeight;

            slides.forEach((slide, index) => {
                const rect = slide.getBoundingClientRect();
                const slideMiddle = rect.top + rect.height / 2;

                if (slideMiddle >= 0 && slideMiddle <= windowHeight) {
                    currentSlide = index + 1;
                    updateNavigation();
                }
            });
        }, 100);
    });

    // Initialize
    updateNavigation();
</script>

</body>
</html>
