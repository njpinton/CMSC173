{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Neural Networks - Beginner Workshop\n",
    "\n",
    "**Course:** CMSC 173 - Machine Learning  \n",
    "**Institution:** University of the Philippines - Cebu  \n",
    "**Instructor:** Noel Jeffrey Pinton\n",
    "\n",
    "---\n",
    "\n",
    "## Workshop Overview\n",
    "\n",
    "This hands-on workshop introduces you to modern neural network architectures through **practical examples**. You'll learn by USING pre-trained models, not building them from scratch. Perfect for beginners!\n",
    "\n",
    "**What you'll do:**\n",
    "- Use a CNN for image classification\n",
    "- Generate images with a diffusion model\n",
    "- Work with text using transformers\n",
    "- See real-world applications\n",
    "\n",
    "**Time:** 60-75 minutes  \n",
    "**Prerequisites:** Basic Python, NumPy, basic ML concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Setup and Imports\n",
    "\n",
    "Let's install and import everything we need. We'll use simple, beginner-friendly libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (run this once)\n",
    "# Uncomment if needed:\n",
    "# !pip install torch torchvision transformers pillow matplotlib numpy\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from transformers import pipeline\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Check if GPU is available (optional - CPU works fine for this workshop)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(\"\\nâœ“ Setup complete! Ready to go.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Motivation - Why These Architectures?\n",
    "\n",
    "Before diving in, let's understand what makes each architecture special:\n",
    "\n",
    "### CNNs (Convolutional Neural Networks)\n",
    "- **Best for:** Images, videos, spatial data\n",
    "- **Real examples:** Face recognition in your phone, medical image analysis, self-driving cars\n",
    "- **Why they work:** They detect patterns like edges, textures, and shapes\n",
    "\n",
    "### Transformers\n",
    "- **Best for:** Text, sequences, language\n",
    "- **Real examples:** ChatGPT, Google Translate, autocomplete\n",
    "- **Why they work:** They understand context and relationships between words\n",
    "\n",
    "### Diffusion Models\n",
    "- **Best for:** Generating images from text\n",
    "- **Real examples:** DALL-E 2, Midjourney, Stable Diffusion\n",
    "- **Why they work:** They learn to remove noise and create realistic images\n",
    "\n",
    "**Today's goal:** Get hands-on experience using these models!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Core Concepts - CNNs in Action\n",
    "\n",
    "Let's start with CNNs. We'll use ResNet50, a powerful pre-trained model that can recognize 1000 different objects.\n",
    "\n",
    "### What is ResNet50?\n",
    "- A CNN trained on 1.2 million images\n",
    "- Can recognize dogs, cats, vehicles, furniture, etc.\n",
    "- Used in real apps like Google Photos\n",
    "\n",
    "### How CNNs \"See\" Images\n",
    "Think of a CNN as having multiple detection layers:\n",
    "1. **Layer 1:** Detects edges (horizontal, vertical, diagonal)\n",
    "2. **Layer 2:** Combines edges into shapes (circles, squares)\n",
    "3. **Layer 3:** Combines shapes into parts (eyes, wheels, windows)\n",
    "4. **Layer 4:** Recognizes complete objects (cat, car, house)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained ResNet50 model\n",
    "print(\"Loading ResNet50 model...\")\n",
    "model = models.resnet50(pretrained=True)\n",
    "model.eval()  # Set to evaluation mode\n",
    "print(\"âœ“ Model loaded!\")\n",
    "\n",
    "# Load ImageNet class labels\n",
    "import urllib.request\n",
    "import json\n",
    "\n",
    "# Download class labels\n",
    "url = \"https://raw.githubusercontent.com/anishathalye/imagenet-simple-labels/master/imagenet-simple-labels.json\"\n",
    "try:\n",
    "    with urllib.request.urlopen(url) as response:\n",
    "        labels = json.loads(response.read())\n",
    "    print(f\"âœ“ Loaded {len(labels)} class labels\")\n",
    "except:\n",
    "    print(\"Note: Could not load labels (offline?). Using indices instead.\")\n",
    "    labels = [f\"Class {i}\" for i in range(1000)]\n",
    "\n",
    "# Define preprocessing transformation\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Test Image\n",
    "\n",
    "Let's create a simple test image to see what the model can recognize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple test image (a cat-like shape)\n",
    "def create_test_image():\n",
    "    \"\"\"Create a simple colored test pattern\"\"\"\n",
    "    img_array = np.ones((224, 224, 3), dtype=np.uint8) * 200  # Light gray background\n",
    "    \n",
    "    # Draw a circle (representing a face)\n",
    "    center_x, center_y = 112, 112\n",
    "    radius = 60\n",
    "    for i in range(224):\n",
    "        for j in range(224):\n",
    "            if (i - center_x)**2 + (j - center_y)**2 < radius**2:\n",
    "                img_array[i, j] = [255, 200, 150]  # Orange/tan color\n",
    "    \n",
    "    # Draw eyes\n",
    "    for eye_y in [90, 90]:\n",
    "        for eye_x in [90, 134]:\n",
    "            for i in range(-8, 8):\n",
    "                for j in range(-8, 8):\n",
    "                    if i**2 + j**2 < 40:\n",
    "                        img_array[eye_y + i, eye_x + j] = [50, 50, 50]  # Dark eyes\n",
    "    \n",
    "    return Image.fromarray(img_array)\n",
    "\n",
    "# Create and display test image\n",
    "test_img = create_test_image()\n",
    "plt.figure(figsize=(4, 4))\n",
    "plt.imshow(test_img)\n",
    "plt.title(\"Our Test Image\")\n",
    "plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Test image created! Now let's see what ResNet50 thinks it is...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a Prediction\n",
    "\n",
    "Now let's use ResNet50 to classify our test image!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the image\n",
    "input_tensor = preprocess(test_img)\n",
    "input_batch = input_tensor.unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "# Make prediction\n",
    "with torch.no_grad():\n",
    "    output = model(input_batch)\n",
    "\n",
    "# Get top 5 predictions\n",
    "probabilities = torch.nn.functional.softmax(output[0], dim=0)\n",
    "top5_prob, top5_idx = torch.topk(probabilities, 5)\n",
    "\n",
    "print(\"\\nðŸ” ResNet50's Top 5 Predictions:\\n\")\n",
    "for i, (prob, idx) in enumerate(zip(top5_prob, top5_idx)):\n",
    "    print(f\"{i+1}. {labels[idx]:20s} - {prob.item()*100:5.2f}%\")\n",
    "\n",
    "print(\"\\nâœ“ CNN classification complete!\")\n",
    "print(\"\\nNote: Results may vary - our simple test image is quite abstract!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the Results\n",
    "\n",
    "**What just happened?**\n",
    "1. We created a simple image\n",
    "2. ResNet50 processed it through 50 layers\n",
    "3. Each layer detected different features\n",
    "4. The final layer combined everything to make a prediction\n",
    "\n",
    "**Real-world applications:**\n",
    "- Medical imaging (detecting tumors)\n",
    "- Facial recognition (unlocking phones)\n",
    "- Quality control (detecting defects in manufacturing)\n",
    "- Wildlife monitoring (counting animals in photos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: Implementation - Transformers for Text\n",
    "\n",
    "Now let's explore Transformers - the technology behind ChatGPT and Google Translate!\n",
    "\n",
    "### What are Transformers?\n",
    "- Neural networks designed for understanding sequences (especially text)\n",
    "- Use \"attention\" to understand context\n",
    "- Power most modern AI chatbots\n",
    "\n",
    "### Simple Example: Sentiment Analysis\n",
    "Let's use a pre-trained transformer to analyze the emotion in text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a sentiment analysis pipeline\n",
    "print(\"Loading sentiment analysis model...\")\n",
    "try:\n",
    "    sentiment_analyzer = pipeline(\"sentiment-analysis\", model=\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "    print(\"âœ“ Model loaded!\\n\")\n",
    "    \n",
    "    # Test sentences\n",
    "    test_sentences = [\n",
    "        \"I love learning about neural networks!\",\n",
    "        \"This workshop is really helpful and interesting.\",\n",
    "        \"Machine learning is challenging but rewarding.\",\n",
    "        \"I'm frustrated with this bug in my code.\",\n",
    "        \"The weather is nice today.\",\n",
    "        \"Neural networks are amazing tools for solving complex problems!\"\n",
    "    ]\n",
    "    \n",
    "    print(\"ðŸ¤– Analyzing sentiment of different sentences:\\n\")\n",
    "    for sentence in test_sentences:\n",
    "        result = sentiment_analyzer(sentence)[0]\n",
    "        sentiment = result['label']\n",
    "        confidence = result['score']\n",
    "        \n",
    "        emoji = \"ðŸ˜Š\" if sentiment == \"POSITIVE\" else \"ðŸ˜”\"\n",
    "        print(f\"{emoji} {sentiment:8s} ({confidence*100:5.1f}%) - \\\"{sentence}\\\"\")\n",
    "    \n",
    "    print(\"\\nâœ“ Sentiment analysis complete!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Note: Could not load model. Error: {e}\")\n",
    "    print(\"This might happen if you're offline or have limited internet connectivity.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How Did It Work?\n",
    "\n",
    "**The transformer analyzed each sentence by:**\n",
    "1. Breaking it into word pieces (tokens)\n",
    "2. Understanding relationships between words using \"attention\"\n",
    "3. Determining overall emotional tone\n",
    "4. Providing a confidence score\n",
    "\n",
    "**Real-world applications:**\n",
    "- Customer feedback analysis (understanding reviews)\n",
    "- Social media monitoring (tracking brand sentiment)\n",
    "- Content moderation (detecting toxic comments)\n",
    "- Mental health apps (detecting distress signals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Another Transformer Task: Text Generation\n",
    "\n",
    "Let's try another cool application - text completion!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load text generation pipeline\n",
    "print(\"Loading text generation model (this may take a moment)...\")\n",
    "try:\n",
    "    text_generator = pipeline(\"text-generation\", model=\"distilgpt2\")\n",
    "    print(\"âœ“ Model loaded!\\n\")\n",
    "    \n",
    "    # Test prompts\n",
    "    prompts = [\n",
    "        \"Machine learning is\",\n",
    "        \"The future of artificial intelligence\",\n",
    "        \"Neural networks can be used for\"\n",
    "    ]\n",
    "    \n",
    "    print(\"âœï¸  Generating text completions:\\n\")\n",
    "    for prompt in prompts:\n",
    "        result = text_generator(prompt, max_length=30, num_return_sequences=1, temperature=0.7)\n",
    "        generated_text = result[0]['generated_text']\n",
    "        print(f\"Prompt: \\\"{prompt}\\\"\")\n",
    "        print(f\"Generated: \\\"{generated_text}\\\"\\n\")\n",
    "    \n",
    "    print(\"âœ“ Text generation complete!\")\n",
    "    print(\"\\nNote: This is a small model. GPT-3 and GPT-4 are much larger and more capable!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Note: Could not load model. Error: {e}\")\n",
    "    print(\"This might happen if you're offline or have limited resources.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5: Evaluation and Comparison\n",
    "\n",
    "Let's compare what we've learned about different architectures:\n",
    "\n",
    "### When to Use Each Architecture\n",
    "\n",
    "| Task | Best Architecture | Example |\n",
    "|------|------------------|----------|\n",
    "| Image Classification | **CNN** | Identifying objects in photos |\n",
    "| Object Detection | **CNN** | Self-driving cars detecting pedestrians |\n",
    "| Text Analysis | **Transformer** | Sentiment analysis, spam detection |\n",
    "| Language Translation | **Transformer** | Google Translate |\n",
    "| Text-to-Image | **Diffusion Model** | DALL-E, Midjourney |\n",
    "| Image Editing | **Diffusion Model** | Photoshop AI features |\n",
    "| Chatbots | **Transformer** | ChatGPT, customer service bots |\n",
    "| Video Recognition | **CNN + Transformer** | YouTube content analysis |\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "**CNNs:**\n",
    "- âœ… Excellent for images and spatial data\n",
    "- âœ… Fast and efficient\n",
    "- âŒ Not good for text or sequences\n",
    "\n",
    "**Transformers:**\n",
    "- âœ… Best for text and language tasks\n",
    "- âœ… Can handle long-range dependencies\n",
    "- âŒ Computationally expensive\n",
    "- âŒ Require lots of training data\n",
    "\n",
    "**Diffusion Models:**\n",
    "- âœ… Generate high-quality, realistic images\n",
    "- âœ… Can edit existing images\n",
    "- âŒ Slow to generate images\n",
    "- âŒ Require powerful GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a visualization comparing architecture characteristics\n",
    "fig, axes = plt.subplots(1, 3, figsize=(14, 4))\n",
    "\n",
    "architectures = ['CNN', 'Transformer', 'Diffusion']\n",
    "characteristics = {\n",
    "    'Speed': [9, 6, 3],\n",
    "    'Image Quality': [7, 4, 10],\n",
    "    'Text Capability': [2, 10, 1],\n",
    "    'Ease of Use': [9, 7, 6],\n",
    "    'Training Cost': [5, 8, 9]\n",
    "}\n",
    "\n",
    "colors = ['#2E86AB', '#A23B72', '#F18F01']\n",
    "\n",
    "for idx, arch in enumerate(architectures):\n",
    "    ax = axes[idx]\n",
    "    values = [characteristics[char][idx] for char in characteristics.keys()]\n",
    "    bars = ax.barh(list(characteristics.keys()), values, color=colors[idx], alpha=0.7)\n",
    "    ax.set_xlim(0, 10)\n",
    "    ax.set_xlabel('Rating (0-10)', fontsize=10)\n",
    "    ax.set_title(f'{arch}', fontsize=12, fontweight='bold')\n",
    "    ax.grid(axis='x', alpha=0.3, linestyle='--')\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar in bars:\n",
    "        width = bar.get_width()\n",
    "        ax.text(width + 0.2, bar.get_y() + bar.get_height()/2, \n",
    "                f'{width:.0f}', ha='left', va='center', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../figures/architecture_comparison_workshop.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ“ Comparison chart created!\")\n",
    "print(\"\\nNote: Ratings are approximate and depend on specific use cases.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 6: Advanced Topics - Combining Architectures\n",
    "\n",
    "Real-world applications often combine multiple architectures! Let's see some examples:\n",
    "\n",
    "### Example 1: Image Captioning\n",
    "**Architecture:** CNN + Transformer\n",
    "- CNN extracts features from the image\n",
    "- Transformer generates a text description\n",
    "- Used in: Accessibility tools, content moderation\n",
    "\n",
    "### Example 2: Visual Question Answering\n",
    "**Architecture:** CNN + Transformer\n",
    "- CNN processes the image\n",
    "- Transformer processes the question\n",
    "- Combined model generates an answer\n",
    "- Used in: Educational apps, virtual assistants\n",
    "\n",
    "### Example 3: Text-to-Image (DALL-E 2)\n",
    "**Architecture:** Transformer + Diffusion Model\n",
    "- Transformer understands the text prompt\n",
    "- Diffusion model generates the image\n",
    "- Used in: Creative tools, marketing, game development\n",
    "\n",
    "### Future Directions\n",
    "\n",
    "**Multimodal Models** (like GPT-4 Vision):\n",
    "- Process text, images, and even audio together\n",
    "- Understand context across different data types\n",
    "- Enable more natural human-AI interaction\n",
    "\n",
    "**Efficient Architectures:**\n",
    "- Smaller models that run on phones\n",
    "- Faster inference for real-time applications\n",
    "- Lower energy consumption"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 7: Challenge Problems\n",
    "\n",
    "Now it's your turn! Try these challenges to deepen your understanding.\n",
    "\n",
    "### Challenge 1: CNN Image Classification\n",
    "**Task:** Modify the CNN example to classify a different type of test image.\n",
    "\n",
    "**Hints:**\n",
    "- Try creating a car-like shape (rectangle with circles)\n",
    "- Or a house-like shape (square with triangle on top)\n",
    "- See what ResNet50 predicts!\n",
    "\n",
    "### Challenge 2: Transformer Sentiment\n",
    "**Task:** Analyze sentiment of your own sentences.\n",
    "\n",
    "**Try:**\n",
    "- Write 3 positive sentences\n",
    "- Write 3 negative sentences\n",
    "- Write 3 neutral sentences\n",
    "- See if the model agrees with your assessment!\n",
    "\n",
    "### Challenge 3: Architecture Selection\n",
    "**Task:** For each application below, choose the best architecture and explain why:\n",
    "1. Building a spam email detector\n",
    "2. Creating an app that identifies plant species from photos\n",
    "3. Developing a logo generator from text descriptions\n",
    "4. Making a system that transcribes handwritten notes\n",
    "\n",
    "### Challenge 4: Research Extension\n",
    "**Task:** Pick one architecture and research a recent advancement:\n",
    "- CNNs: Look up EfficientNet or Vision Transformers\n",
    "- Transformers: Research GPT-4 or Claude\n",
    "- Diffusion: Investigate SDXL or DALL-E 3\n",
    "\n",
    "Write a short paragraph (3-5 sentences) about what you learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Challenge 1 Solution Template\n",
    "# Uncomment and modify this code for Challenge 1\n",
    "\n",
    "# def create_custom_image():\n",
    "#     \"\"\"Create your own test image here\"\"\"\n",
    "#     img_array = np.ones((224, 224, 3), dtype=np.uint8) * 255\n",
    "#     \n",
    "#     # Your code here: Draw shapes to create an object\n",
    "#     # Example: Draw a car, house, animal, etc.\n",
    "#     \n",
    "#     return Image.fromarray(img_array)\n",
    "# \n",
    "# # Test your image\n",
    "# my_image = create_custom_image()\n",
    "# plt.imshow(my_image)\n",
    "# plt.show()\n",
    "# \n",
    "# # Classify it\n",
    "# input_tensor = preprocess(my_image)\n",
    "# input_batch = input_tensor.unsqueeze(0)\n",
    "# with torch.no_grad():\n",
    "#     output = model(input_batch)\n",
    "# probabilities = torch.nn.functional.softmax(output[0], dim=0)\n",
    "# top5_prob, top5_idx = torch.topk(probabilities, 5)\n",
    "# for i, (prob, idx) in enumerate(zip(top5_prob, top5_idx)):\n",
    "#     print(f\"{i+1}. {labels[idx]:20s} - {prob.item()*100:5.2f}%\")\n",
    "\n",
    "print(\"Challenge 1: Uncomment the code above and try your own image!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Challenge 2 Solution Template\n",
    "# Uncomment and modify for Challenge 2\n",
    "\n",
    "# my_sentences = [\n",
    "#     \"Your positive sentence 1\",\n",
    "#     \"Your positive sentence 2\",\n",
    "#     \"Your positive sentence 3\",\n",
    "#     \"Your negative sentence 1\",\n",
    "#     \"Your negative sentence 2\",\n",
    "#     \"Your negative sentence 3\",\n",
    "#     \"Your neutral sentence 1\",\n",
    "#     \"Your neutral sentence 2\",\n",
    "#     \"Your neutral sentence 3\",\n",
    "# ]\n",
    "# \n",
    "# for sentence in my_sentences:\n",
    "#     result = sentiment_analyzer(sentence)[0]\n",
    "#     print(f\"{result['label']:8s} ({result['score']*100:5.1f}%) - {sentence}\")\n",
    "\n",
    "print(\"Challenge 2: Uncomment the code above and add your sentences!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 8: Solutions and Discussion\n",
    "\n",
    "### Challenge 3 Solutions:\n",
    "\n",
    "1. **Spam email detector:** **Transformer**\n",
    "   - Spam detection requires understanding text context\n",
    "   - Transformers excel at language understanding\n",
    "   - Can detect subtle patterns in writing style\n",
    "\n",
    "2. **Plant species identifier:** **CNN**\n",
    "   - This is image classification\n",
    "   - CNNs are designed for visual pattern recognition\n",
    "   - Can detect leaves, flowers, stem patterns\n",
    "\n",
    "3. **Logo generator from text:** **Diffusion Model**\n",
    "   - Generating images from text descriptions\n",
    "   - Diffusion models create high-quality images\n",
    "   - Can be combined with transformers for better text understanding\n",
    "\n",
    "4. **Handwriting transcription:** **CNN + Transformer**\n",
    "   - CNN extracts features from handwritten text images\n",
    "   - Transformer decodes features into text\n",
    "   - Common architecture for OCR (Optical Character Recognition)\n",
    "\n",
    "### Discussion Points:\n",
    "\n",
    "**Why Pre-trained Models?**\n",
    "- Training from scratch requires massive datasets and computing power\n",
    "- Pre-trained models have learned general features\n",
    "- You can fine-tune them for your specific task\n",
    "- Democratizes AI - anyone can use powerful models!\n",
    "\n",
    "**Ethical Considerations:**\n",
    "- **Bias:** Models learn from data, which may contain biases\n",
    "- **Deepfakes:** Generative models can be misused\n",
    "- **Privacy:** Facial recognition raises privacy concerns\n",
    "- **Environmental impact:** Large models consume significant energy\n",
    "\n",
    "**Best Practices:**\n",
    "1. Understand your model's limitations\n",
    "2. Test on diverse data\n",
    "3. Be transparent about AI use\n",
    "4. Consider ethical implications\n",
    "5. Use appropriate model size for your task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 9: Summary and Next Steps\n",
    "\n",
    "### What You Learned Today\n",
    "\n",
    "**Technical Skills:**\n",
    "- âœ… Used a pre-trained CNN (ResNet50) for image classification\n",
    "- âœ… Applied transformers for sentiment analysis and text generation\n",
    "- âœ… Understood when to use each architecture\n",
    "- âœ… Explored real-world applications\n",
    "\n",
    "**Key Concepts:**\n",
    "- **CNNs** detect visual patterns hierarchically (edges â†’ shapes â†’ objects)\n",
    "- **Transformers** understand context in text using attention mechanisms\n",
    "- **Diffusion Models** generate images by learning to remove noise\n",
    "- **Multimodal models** combine architectures for complex tasks\n",
    "\n",
    "### Quick Reference Guide\n",
    "\n",
    "| Your Task | Architecture | Python Library | Pre-trained Model |\n",
    "|-----------|--------------|----------------|-------------------|\n",
    "| Classify images | CNN | torchvision | ResNet, EfficientNet |\n",
    "| Detect objects | CNN | detectron2 | YOLO, Faster R-CNN |\n",
    "| Analyze sentiment | Transformer | transformers | DistilBERT, RoBERTa |\n",
    "| Generate text | Transformer | transformers | GPT-2, GPT-3 |\n",
    "| Translate languages | Transformer | transformers | MarianMT, mBART |\n",
    "| Generate images | Diffusion | diffusers | Stable Diffusion |\n",
    "\n",
    "### Next Steps for Learning\n",
    "\n",
    "**Beginner Level:**\n",
    "1. Practice with more pre-trained models on Hugging Face\n",
    "2. Try fine-tuning a model on your own small dataset\n",
    "3. Build a simple image classifier for a personal project\n",
    "4. Experiment with different transformer models\n",
    "\n",
    "**Intermediate Level:**\n",
    "1. Learn about transfer learning and fine-tuning techniques\n",
    "2. Understand model architectures in detail\n",
    "3. Try training a small model from scratch\n",
    "4. Explore model optimization (quantization, pruning)\n",
    "\n",
    "**Advanced Level:**\n",
    "1. Implement custom architectures\n",
    "2. Research recent papers on arXiv\n",
    "3. Contribute to open-source ML projects\n",
    "4. Experiment with multimodal models\n",
    "\n",
    "### Recommended Resources\n",
    "\n",
    "**Online Courses:**\n",
    "- Fast.ai Practical Deep Learning for Coders (free)\n",
    "- Hugging Face NLP Course (free)\n",
    "- Stanford CS231n (CNNs) and CS224n (NLP)\n",
    "\n",
    "**Documentation:**\n",
    "- PyTorch tutorials: pytorch.org/tutorials\n",
    "- Hugging Face docs: huggingface.co/docs\n",
    "- Papers with Code: paperswithcode.com\n",
    "\n",
    "**Communities:**\n",
    "- r/MachineLearning (Reddit)\n",
    "- Hugging Face Forums\n",
    "- Fast.ai Forums\n",
    "- Local ML meetups\n",
    "\n",
    "### Final Thoughts\n",
    "\n",
    "You've taken your first steps with modern neural networks! Remember:\n",
    "\n",
    "- **Start simple:** Use pre-trained models before building your own\n",
    "- **Practice regularly:** ML is a skill that improves with practice\n",
    "- **Stay curious:** The field evolves rapidly - keep learning\n",
    "- **Build projects:** Apply what you learn to real problems\n",
    "- **Be ethical:** Consider the impact of your AI applications\n",
    "\n",
    "**Thank you for participating in this workshop!**\n",
    "\n",
    "---\n",
    "\n",
    "*Questions? Contact: Noel Jeffrey Pinton*  \n",
    "*Course: CMSC 173 - Machine Learning*  \n",
    "*Institution: University of the Philippines - Cebu*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final code cell - Summary visualization\n",
    "print(\"=\"*60)\n",
    "print(\" \" * 10 + \"WORKSHOP COMPLETE!\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nðŸ“š What you explored today:\")\n",
    "print(\"   âœ“ CNNs (Convolutional Neural Networks)\")\n",
    "print(\"   âœ“ Transformers\")\n",
    "print(\"   âœ“ Diffusion Models (conceptually)\")\n",
    "print(\"\\nðŸŽ¯ Key takeaways:\")\n",
    "print(\"   â€¢ CNNs excel at image tasks\")\n",
    "print(\"   â€¢ Transformers are best for text\")\n",
    "print(\"   â€¢ Diffusion models generate high-quality images\")\n",
    "print(\"   â€¢ Real-world apps often combine architectures\")\n",
    "print(\"\\nðŸš€ Keep practicing and building!\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
