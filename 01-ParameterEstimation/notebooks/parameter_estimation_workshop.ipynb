{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": [
    "# Parameter Estimation Workshop\n",
    "## Method of Moments & Maximum Likelihood Estimation\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/your-repo/CMSC173/blob/main/08-ParameterEstimation/notebooks/parameter_estimation_workshop.ipynb)\n",
    "\n",
    "**Course:** CMSC 173 - Machine Learning  \n",
    "**Topic:** Parameter Estimation  \n",
    "**Duration:** 45-60 minutes  \n",
    "\n",
    "### Learning Objectives\n",
    "By the end of this workshop, you will be able to:\n",
    "1. Understand the fundamental concepts of parameter estimation\n",
    "2. Implement Method of Moments (MoM) estimators\n",
    "3. Implement Maximum Likelihood Estimation (MLE)\n",
    "4. Compare different estimation methods\n",
    "5. Apply parameter estimation to real-world problems\n",
    "6. Evaluate estimator quality and diagnostics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats, optimize\n",
    "from sklearn.datasets import make_regression\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "print(\"‚úÖ Setup complete! All libraries imported successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Problem Understanding\n",
    "\n",
    "Parameter estimation is the process of inferring unknown parameters of a probability distribution from observed data.\n",
    "\n",
    "**The Setup:**\n",
    "- We have data samples: $\\{x_1, x_2, \\ldots, x_n\\}$\n",
    "- We assume they come from a distribution: $f(x|\\theta)$\n",
    "- We want to estimate: $\\hat{\\theta}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate example data from a known distribution\n",
    "true_mu = 5.0\n",
    "true_sigma = 2.0\n",
    "n_samples = 100\n",
    "\n",
    "# Generate normal data\n",
    "data = np.random.normal(true_mu, true_sigma, n_samples)\n",
    "\n",
    "print(f\"Generated {n_samples} samples from Normal({true_mu}, {true_sigma}¬≤)\")\n",
    "print(f\"Sample mean: {np.mean(data):.3f}\")\n",
    "print(f\"Sample std: {np.std(data, ddof=1):.3f}\")\n",
    "\n",
    "# Visualize the data\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Histogram\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(data, bins=20, density=True, alpha=0.7, color='lightblue', edgecolor='black')\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Density')\n",
    "plt.title('Sample Data')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Box plot\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.boxplot(data, vert=True)\n",
    "plt.ylabel('Value')\n",
    "plt.title('Data Distribution')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Notice: We generated this data, so we know the true parameters.\")\n",
    "print(\"   In practice, we only have the data and need to estimate the parameters!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Method of Moments (MoM)\n",
    "\n",
    "**Core Idea:** Match sample moments to theoretical moments.\n",
    "\n",
    "**Algorithm:**\n",
    "1. Express theoretical moments in terms of parameters\n",
    "2. Calculate sample moments\n",
    "3. Set theoretical = sample moments\n",
    "4. Solve for parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def method_of_moments_normal(data):\n",
    "    \"\"\"\n",
    "    Method of Moments estimation for normal distribution\n",
    "    \n",
    "    For Normal(Œº, œÉ¬≤):\n",
    "    - E[X] = Œº\n",
    "    - E[X¬≤] = Œº¬≤ + œÉ¬≤\n",
    "    \n",
    "    Sample moments:\n",
    "    - m‚ÇÅ = (1/n) Œ£ x·µ¢\n",
    "    - m‚ÇÇ = (1/n) Œ£ x·µ¢¬≤\n",
    "    \"\"\"\n",
    "    # First moment (mean)\n",
    "    m1 = np.mean(data)\n",
    "    \n",
    "    # Second moment\n",
    "    m2 = np.mean(data**2)\n",
    "    \n",
    "    # MoM estimates\n",
    "    mu_hat = m1\n",
    "    sigma_squared_hat = m2 - m1**2\n",
    "    sigma_hat = np.sqrt(sigma_squared_hat)\n",
    "    \n",
    "    return mu_hat, sigma_hat\n",
    "\n",
    "# Apply MoM to our data\n",
    "mu_mom, sigma_mom = method_of_moments_normal(data)\n",
    "\n",
    "print(\"Method of Moments Results:\")\n",
    "print(f\"ŒºÃÇ = {mu_mom:.3f} (true Œº = {true_mu})\")\n",
    "print(f\"œÉÃÇ = {sigma_mom:.3f} (true œÉ = {true_sigma})\")\n",
    "print(f\"Error in Œº: {abs(mu_mom - true_mu):.3f}\")\n",
    "print(f\"Error in œÉ: {abs(sigma_mom - true_sigma):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize MoM fit\n",
    "x_range = np.linspace(data.min() - 1, data.max() + 1, 200)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(data, bins=20, density=True, alpha=0.7, color='lightblue', \n",
    "         edgecolor='black', label='Data')\n",
    "\n",
    "# True distribution\n",
    "true_pdf = stats.norm.pdf(x_range, true_mu, true_sigma)\n",
    "plt.plot(x_range, true_pdf, 'r-', linewidth=2, \n",
    "         label=f'True: N({true_mu}, {true_sigma}¬≤)')\n",
    "\n",
    "# MoM fitted distribution\n",
    "mom_pdf = stats.norm.pdf(x_range, mu_mom, sigma_mom)\n",
    "plt.plot(x_range, mom_pdf, 'g--', linewidth=2, \n",
    "         label=f'MoM: N({mu_mom:.2f}, {sigma_mom:.2f}¬≤)')\n",
    "\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Density')\n",
    "plt.title('Method of Moments: Normal Distribution')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"üí° The MoM estimate closely matches the true distribution!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MoM Example: Poisson Distribution\n",
    "\n",
    "For Poisson(Œª): E[X] = Œª, so the MoM estimator is simply ŒªÃÇ = xÃÑ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Poisson data\n",
    "true_lambda = 3.5\n",
    "poisson_data = np.random.poisson(true_lambda, 200)\n",
    "\n",
    "# MoM estimate\n",
    "lambda_mom = np.mean(poisson_data)\n",
    "\n",
    "print(f\"Poisson MoM Estimation:\")\n",
    "print(f\"True Œª = {true_lambda}\")\n",
    "print(f\"MoM ŒªÃÇ = {lambda_mom:.3f}\")\n",
    "print(f\"Error: {abs(lambda_mom - true_lambda):.3f}\")\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Data histogram\n",
    "counts = np.bincount(poisson_data)\n",
    "x_vals = np.arange(len(counts))\n",
    "plt.bar(x_vals, counts/len(poisson_data), alpha=0.7, color='lightcoral', \n",
    "        edgecolor='black', label='Data')\n",
    "\n",
    "# Theoretical PMFs\n",
    "x_theory = np.arange(0, max(poisson_data) + 1)\n",
    "true_pmf = stats.poisson.pmf(x_theory, true_lambda)\n",
    "mom_pmf = stats.poisson.pmf(x_theory, lambda_mom)\n",
    "\n",
    "plt.plot(x_theory, true_pmf, 'ro-', markersize=4, \n",
    "         label=f'True: Poisson({true_lambda})')\n",
    "plt.plot(x_theory, mom_pmf, 'g^-', markersize=4, \n",
    "         label=f'MoM: Poisson({lambda_mom:.2f})')\n",
    "\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Probability')\n",
    "plt.title('Method of Moments: Poisson Distribution')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Maximum Likelihood Estimation (MLE)\n",
    "\n",
    "**Core Idea:** Find parameter values that make the observed data most likely.\n",
    "\n",
    "**Likelihood Function:**\n",
    "$$L(\\theta) = \\prod_{i=1}^n f(x_i | \\theta)$$\n",
    "\n",
    "**Log-Likelihood:**\n",
    "$$\\ell(\\theta) = \\sum_{i=1}^n \\log f(x_i | \\theta)$$\n",
    "\n",
    "**MLE:** $\\hat{\\theta}_{MLE} = \\arg\\max_\\theta \\ell(\\theta)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maximum_likelihood_normal(data):\n",
    "    \"\"\"\n",
    "    Maximum Likelihood estimation for normal distribution\n",
    "    \n",
    "    For Normal(Œº, œÉ¬≤), the MLE estimates are:\n",
    "    - ŒºÃÇ = (1/n) Œ£ x·µ¢\n",
    "    - œÉÃÇ¬≤ = (1/n) Œ£ (x·µ¢ - ŒºÃÇ)¬≤\n",
    "    \"\"\"\n",
    "    n = len(data)\n",
    "    \n",
    "    # MLE for mean\n",
    "    mu_mle = np.mean(data)\n",
    "    \n",
    "    # MLE for variance (biased estimator)\n",
    "    sigma_squared_mle = np.sum((data - mu_mle)**2) / n\n",
    "    sigma_mle = np.sqrt(sigma_squared_mle)\n",
    "    \n",
    "    return mu_mle, sigma_mle\n",
    "\n",
    "# Apply MLE to our data\n",
    "mu_mle, sigma_mle = maximum_likelihood_normal(data)\n",
    "\n",
    "print(\"Maximum Likelihood Results:\")\n",
    "print(f\"ŒºÃÇ = {mu_mle:.3f} (true Œº = {true_mu})\")\n",
    "print(f\"œÉÃÇ = {sigma_mle:.3f} (true œÉ = {true_sigma})\")\n",
    "print(f\"Error in Œº: {abs(mu_mle - true_mu):.3f}\")\n",
    "print(f\"Error in œÉ: {abs(sigma_mle - true_sigma):.3f}\")\n",
    "\n",
    "print(\"\\nüìä Comparison:\")\n",
    "print(f\"MoM ŒºÃÇ: {mu_mom:.3f}, MLE ŒºÃÇ: {mu_mle:.3f}\")\n",
    "print(f\"MoM œÉÃÇ: {sigma_mom:.3f}, MLE œÉÃÇ: {sigma_mle:.3f}\")\n",
    "print(\"\\nüí° For normal distribution, MoM and MLE give the same estimate for Œº!\")\n",
    "print(\"   But œÉ estimates differ slightly (MLE uses n, MoM effectively uses n-1)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the likelihood function\n",
    "def log_likelihood_normal(mu, sigma, data):\n",
    "    \"\"\"Compute log-likelihood for normal distribution\"\"\"\n",
    "    n = len(data)\n",
    "    if sigma <= 0:\n",
    "        return -np.inf\n",
    "    \n",
    "    ll = -0.5 * n * np.log(2 * np.pi) - n * np.log(sigma) - \\\n",
    "         0.5 * np.sum((data - mu)**2) / (sigma**2)\n",
    "    return ll\n",
    "\n",
    "# Create grid for likelihood surface\n",
    "mu_range = np.linspace(3, 7, 50)\n",
    "sigma_range = np.linspace(1, 3, 50)\n",
    "MU, SIGMA = np.meshgrid(mu_range, sigma_range)\n",
    "\n",
    "# Compute log-likelihood surface\n",
    "log_lik_surface = np.zeros_like(MU)\n",
    "for i in range(len(mu_range)):\n",
    "    for j in range(len(sigma_range)):\n",
    "        log_lik_surface[j, i] = log_likelihood_normal(MU[j, i], SIGMA[j, i], data)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# 2D contour plot\n",
    "plt.subplot(1, 2, 1)\n",
    "contour = plt.contour(MU, SIGMA, log_lik_surface, levels=20, colors='blue', alpha=0.6)\n",
    "plt.clabel(contour, inline=True, fontsize=8)\n",
    "plt.plot(true_mu, true_sigma, 'ro', markersize=8, label='True Parameters')\n",
    "plt.plot(mu_mle, sigma_mle, 'go', markersize=8, label='MLE Estimates')\n",
    "plt.xlabel('Œº')\n",
    "plt.ylabel('œÉ')\n",
    "plt.title('2D Log-Likelihood Surface')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# 1D slice (fix sigma at true value)\n",
    "plt.subplot(1, 2, 2)\n",
    "log_lik_1d = [log_likelihood_normal(mu, true_sigma, data) for mu in mu_range]\n",
    "plt.plot(mu_range, log_lik_1d, 'b-', linewidth=2, label='Log-Likelihood')\n",
    "plt.axvline(true_mu, color='red', linestyle='--', linewidth=2, label='True Œº')\n",
    "plt.axvline(mu_mle, color='green', linestyle=':', linewidth=2, label='MLE ŒºÃÇ')\n",
    "plt.xlabel('Œº')\n",
    "plt.ylabel('Log-Likelihood')\n",
    "plt.title('1D Log-Likelihood (œÉ fixed)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLE Example: Exponential Distribution\n",
    "\n",
    "For Exponential(Œª): f(x|Œª) = Œªe^{-Œªx}, the MLE is ŒªÃÇ = 1/xÃÑ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate exponential data\n",
    "true_rate = 2.0\n",
    "exp_data = np.random.exponential(1/true_rate, 100)\n",
    "\n",
    "# MLE estimate\n",
    "rate_mle = 1 / np.mean(exp_data)\n",
    "\n",
    "print(f\"Exponential MLE Estimation:\")\n",
    "print(f\"True Œª = {true_rate}\")\n",
    "print(f\"MLE ŒªÃÇ = {rate_mle:.3f}\")\n",
    "print(f\"Error: {abs(rate_mle - true_rate):.3f}\")\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Data and fitted distributions\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(exp_data, bins=20, density=True, alpha=0.7, color='lightcoral', \n",
    "         edgecolor='black', label='Data')\n",
    "\n",
    "x_exp = np.linspace(0, max(exp_data), 200)\n",
    "true_pdf = stats.expon.pdf(x_exp, scale=1/true_rate)\n",
    "mle_pdf = stats.expon.pdf(x_exp, scale=1/rate_mle)\n",
    "\n",
    "plt.plot(x_exp, true_pdf, 'r-', linewidth=2, label=f'True: Exp({true_rate})')\n",
    "plt.plot(x_exp, mle_pdf, 'g--', linewidth=2, label=f'MLE: Exp({rate_mle:.2f})')\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Density')\n",
    "plt.title('Exponential Distribution MLE')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Log-likelihood function\n",
    "plt.subplot(1, 2, 2)\n",
    "rate_range = np.linspace(0.5, 4, 100)\n",
    "log_lik_exp = []\n",
    "\n",
    "for rate in rate_range:\n",
    "    ll = np.sum(stats.expon.logpdf(exp_data, scale=1/rate))\n",
    "    log_lik_exp.append(ll)\n",
    "\n",
    "plt.plot(rate_range, log_lik_exp, 'b-', linewidth=2, label='Log-Likelihood')\n",
    "plt.axvline(true_rate, color='red', linestyle='--', linewidth=2, label='True Œª')\n",
    "plt.axvline(rate_mle, color='green', linestyle=':', linewidth=2, label='MLE ŒªÃÇ')\n",
    "plt.xlabel('Œª (Rate Parameter)')\n",
    "plt.ylabel('Log-Likelihood')\n",
    "plt.title('Log-Likelihood Function')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Advanced Techniques\n",
    "\n",
    "### Numerical MLE for Complex Distributions\n",
    "\n",
    "When there's no closed-form solution, we use numerical optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Gamma distribution (requires numerical optimization)\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "# Generate gamma data\n",
    "true_shape = 2.5\n",
    "true_scale = 1.5\n",
    "gamma_data = np.random.gamma(true_shape, true_scale, 200)\n",
    "\n",
    "def negative_log_likelihood_gamma(params, data):\n",
    "    \"\"\"Negative log-likelihood for gamma distribution\"\"\"\n",
    "    shape, scale = params\n",
    "    if shape <= 0 or scale <= 0:\n",
    "        return np.inf\n",
    "    \n",
    "    try:\n",
    "        ll = np.sum(stats.gamma.logpdf(data, a=shape, scale=scale))\n",
    "        return -ll  # Return negative for minimization\n",
    "    except:\n",
    "        return np.inf\n",
    "\n",
    "# Initial guess\n",
    "initial_guess = [2.0, 1.0]\n",
    "\n",
    "# Optimize\n",
    "result = minimize(negative_log_likelihood_gamma, initial_guess, \n",
    "                 args=(gamma_data,), method='Nelder-Mead')\n",
    "\n",
    "shape_mle, scale_mle = result.x\n",
    "\n",
    "print(\"Gamma Distribution Numerical MLE:\")\n",
    "print(f\"True: shape={true_shape}, scale={true_scale}\")\n",
    "print(f\"MLE:  shape={shape_mle:.3f}, scale={scale_mle:.3f}\")\n",
    "print(f\"Optimization success: {result.success}\")\n",
    "print(f\"Function evaluations: {result.nfev}\")\n",
    "\n",
    "# Compare with Method of Moments\n",
    "sample_mean = np.mean(gamma_data)\n",
    "sample_var = np.var(gamma_data)\n",
    "\n",
    "# For Gamma: E[X] = Œ±Œ≤, Var(X) = Œ±Œ≤¬≤\n",
    "scale_mom = sample_var / sample_mean\n",
    "shape_mom = sample_mean / scale_mom\n",
    "\n",
    "print(\"\\nMethod of Moments:\")\n",
    "print(f\"MoM:  shape={shape_mom:.3f}, scale={scale_mom:.3f}\")\n",
    "\n",
    "# Visualize results\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(gamma_data, bins=25, density=True, alpha=0.7, color='lightblue', \n",
    "         edgecolor='black', label='Data')\n",
    "\n",
    "x_gamma = np.linspace(0, max(gamma_data), 200)\n",
    "\n",
    "# True distribution\n",
    "true_pdf = stats.gamma.pdf(x_gamma, a=true_shape, scale=true_scale)\n",
    "plt.plot(x_gamma, true_pdf, 'r-', linewidth=2, \n",
    "         label=f'True: Œì({true_shape}, {true_scale})')\n",
    "\n",
    "# MLE fitted\n",
    "mle_pdf = stats.gamma.pdf(x_gamma, a=shape_mle, scale=scale_mle)\n",
    "plt.plot(x_gamma, mle_pdf, 'g--', linewidth=2, \n",
    "         label=f'MLE: Œì({shape_mle:.2f}, {scale_mle:.2f})')\n",
    "\n",
    "# MoM fitted\n",
    "mom_pdf = stats.gamma.pdf(x_gamma, a=shape_mom, scale=scale_mom)\n",
    "plt.plot(x_gamma, mom_pdf, 'b:', linewidth=2, \n",
    "         label=f'MoM: Œì({shape_mom:.2f}, {scale_mom:.2f})')\n",
    "\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Density')\n",
    "plt.title('Numerical MLE: Gamma Distribution')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Diagnostic Tools\n",
    "\n",
    "How do we know if our parameter estimates are good?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bootstrap confidence intervals\n",
    "def bootstrap_confidence_interval(data, estimator_func, n_bootstrap=1000, confidence=0.95):\n",
    "    \"\"\"Compute bootstrap confidence interval for an estimator\"\"\"\n",
    "    estimates = []\n",
    "    \n",
    "    for _ in range(n_bootstrap):\n",
    "        # Bootstrap sample\n",
    "        bootstrap_sample = np.random.choice(data, size=len(data), replace=True)\n",
    "        estimate = estimator_func(bootstrap_sample)\n",
    "        estimates.append(estimate)\n",
    "    \n",
    "    estimates = np.array(estimates)\n",
    "    alpha = 1 - confidence\n",
    "    lower = np.percentile(estimates, 100 * alpha/2)\n",
    "    upper = np.percentile(estimates, 100 * (1 - alpha/2))\n",
    "    \n",
    "    return estimates, (lower, upper)\n",
    "\n",
    "# Apply to our normal data\n",
    "def mean_estimator(data):\n",
    "    return np.mean(data)\n",
    "\n",
    "mean_bootstrap, mean_ci = bootstrap_confidence_interval(data, mean_estimator)\n",
    "\n",
    "print(f\"Bootstrap Results for Mean Estimation:\")\n",
    "print(f\"Original estimate: {np.mean(data):.3f}\")\n",
    "print(f\"Bootstrap mean: {np.mean(mean_bootstrap):.3f}\")\n",
    "print(f\"Bootstrap std: {np.std(mean_bootstrap):.3f}\")\n",
    "print(f\"95% CI: [{mean_ci[0]:.3f}, {mean_ci[1]:.3f}]\")\n",
    "print(f\"True value in CI: {mean_ci[0] <= true_mu <= mean_ci[1]}\")\n",
    "\n",
    "# Visualize bootstrap distribution\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Bootstrap distribution\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(mean_bootstrap, bins=30, density=True, alpha=0.7, color='lightgreen', \n",
    "         edgecolor='black', label='Bootstrap Distribution')\n",
    "plt.axvline(true_mu, color='red', linestyle='--', linewidth=2, label='True Œº')\n",
    "plt.axvline(np.mean(data), color='blue', linestyle='-', linewidth=2, label='Original Estimate')\n",
    "plt.axvline(mean_ci[0], color='orange', linestyle=':', linewidth=2, label='95% CI')\n",
    "plt.axvline(mean_ci[1], color='orange', linestyle=':', linewidth=2)\n",
    "plt.fill_between([mean_ci[0], mean_ci[1]], 0, plt.ylim()[1], alpha=0.2, color='orange')\n",
    "plt.xlabel('Mean Estimate')\n",
    "plt.ylabel('Density')\n",
    "plt.title('Bootstrap Distribution of Mean')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Q-Q plot for normality check\n",
    "plt.subplot(1, 2, 2)\n",
    "from scipy import stats\n",
    "(osm, osr), (slope, intercept, r) = stats.probplot(data, dist=\"norm\", plot=None)\n",
    "plt.scatter(osm, osr, alpha=0.6, color='blue', s=20)\n",
    "plt.plot(osm, slope * osm + intercept, 'r-', linewidth=2, \n",
    "         label=f'Normal Q-Q Line (R¬≤ = {r**2:.3f})')\n",
    "plt.xlabel('Theoretical Quantiles')\n",
    "plt.ylabel('Sample Quantiles')\n",
    "plt.title('Normality Check (Q-Q Plot)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Goodness-of-Fit Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kolmogorov-Smirnov test\n",
    "from scipy.stats import kstest\n",
    "\n",
    "# Test if our data comes from the estimated normal distribution\n",
    "ks_stat, ks_pvalue = kstest(data, lambda x: stats.norm.cdf(x, mu_mle, sigma_mle))\n",
    "\n",
    "print(\"Goodness-of-Fit Tests:\")\n",
    "print(f\"Kolmogorov-Smirnov Test:\")\n",
    "print(f\"  Statistic: {ks_stat:.4f}\")\n",
    "print(f\"  P-value: {ks_pvalue:.4f}\")\n",
    "print(f\"  Conclusion: {'Reject H0' if ks_pvalue < 0.05 else 'Fail to reject H0'} (Œ±=0.05)\")\n",
    "\n",
    "# Anderson-Darling test for normality\n",
    "from scipy.stats import anderson\n",
    "ad_stat, ad_critical, ad_significance = anderson(data, dist='norm')\n",
    "\n",
    "print(f\"\\nAnderson-Darling Test for Normality:\")\n",
    "print(f\"  Statistic: {ad_stat:.4f}\")\n",
    "for i, (crit, sig) in enumerate(zip(ad_critical, ad_significance)):\n",
    "    print(f\"  Critical value at {sig}%: {crit:.4f} {'(Reject)' if ad_stat > crit else '(Accept)'}\")\n",
    "\n",
    "# Visualize fit\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Empirical vs theoretical CDF\n",
    "plt.subplot(1, 2, 1)\n",
    "sorted_data = np.sort(data)\n",
    "empirical_cdf = np.arange(1, len(sorted_data) + 1) / len(sorted_data)\n",
    "theoretical_cdf = stats.norm.cdf(sorted_data, mu_mle, sigma_mle)\n",
    "\n",
    "plt.plot(sorted_data, empirical_cdf, 'b-', linewidth=2, label='Empirical CDF')\n",
    "plt.plot(sorted_data, theoretical_cdf, 'r--', linewidth=2, label='Theoretical CDF')\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Cumulative Probability')\n",
    "plt.title('CDF Comparison')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# P-P plot\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(theoretical_cdf, empirical_cdf, alpha=0.6, s=20)\n",
    "plt.plot([0, 1], [0, 1], 'r--', linewidth=2, label='Perfect Fit')\n",
    "plt.xlabel('Theoretical Cumulative Probability')\n",
    "plt.ylabel('Empirical Cumulative Probability')\n",
    "plt.title('P-P Plot')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° If the points lie close to the diagonal line, the fit is good!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Best Practices\n",
    "\n",
    "### Comparing Estimator Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulation study to compare MoM vs MLE\n",
    "def compare_estimators(true_params, distribution, sample_sizes, n_simulations=500):\n",
    "    \"\"\"Compare MoM and MLE estimators across different sample sizes\"\"\"\n",
    "    results = {\n",
    "        'sample_sizes': sample_sizes,\n",
    "        'mom_bias': [],\n",
    "        'mle_bias': [],\n",
    "        'mom_variance': [],\n",
    "        'mle_variance': [],\n",
    "        'mom_mse': [],\n",
    "        'mle_mse': []\n",
    "    }\n",
    "    \n",
    "    for n in sample_sizes:\n",
    "        mom_estimates = []\n",
    "        mle_estimates = []\n",
    "        \n",
    "        for _ in range(n_simulations):\n",
    "            # Generate data\n",
    "            if distribution == 'normal':\n",
    "                data_sim = np.random.normal(true_params[0], true_params[1], n)\n",
    "                mom_est = np.std(data_sim, ddof=1)  # Unbiased variance estimator\n",
    "                mle_est = np.std(data_sim, ddof=0)  # MLE variance estimator\n",
    "            elif distribution == 'exponential':\n",
    "                data_sim = np.random.exponential(1/true_params[0], n)\n",
    "                mom_est = 1/np.mean(data_sim)  # Same as MLE for exponential\n",
    "                mle_est = 1/np.mean(data_sim)\n",
    "            \n",
    "            mom_estimates.append(mom_est)\n",
    "            mle_estimates.append(mle_est)\n",
    "        \n",
    "        # Calculate statistics\n",
    "        true_param = true_params[1] if distribution == 'normal' else true_params[0]\n",
    "        \n",
    "        mom_estimates = np.array(mom_estimates)\n",
    "        mle_estimates = np.array(mle_estimates)\n",
    "        \n",
    "        mom_bias = np.mean(mom_estimates) - true_param\n",
    "        mle_bias = np.mean(mle_estimates) - true_param\n",
    "        \n",
    "        mom_var = np.var(mom_estimates)\n",
    "        mle_var = np.var(mle_estimates)\n",
    "        \n",
    "        mom_mse = mom_bias**2 + mom_var\n",
    "        mle_mse = mle_bias**2 + mle_var\n",
    "        \n",
    "        results['mom_bias'].append(mom_bias)\n",
    "        results['mle_bias'].append(mle_bias)\n",
    "        results['mom_variance'].append(mom_var)\n",
    "        results['mle_variance'].append(mle_var)\n",
    "        results['mom_mse'].append(mom_mse)\n",
    "        results['mle_mse'].append(mle_mse)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run comparison for normal distribution (comparing variance estimators)\n",
    "sample_sizes = [10, 20, 50, 100, 200, 500]\n",
    "results = compare_estimators([5.0, 2.0], 'normal', sample_sizes)\n",
    "\n",
    "# Plot results\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Bias\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(sample_sizes, results['mom_bias'], 'bo-', label='MoM (Unbiased)')\n",
    "plt.plot(sample_sizes, results['mle_bias'], 'ro-', label='MLE (Biased)')\n",
    "plt.axhline(0, color='black', linestyle='--', alpha=0.5)\n",
    "plt.xlabel('Sample Size')\n",
    "plt.ylabel('Bias')\n",
    "plt.title('Bias Comparison')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Variance\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.loglog(sample_sizes, results['mom_variance'], 'bo-', label='MoM')\n",
    "plt.loglog(sample_sizes, results['mle_variance'], 'ro-', label='MLE')\n",
    "plt.xlabel('Sample Size')\n",
    "plt.ylabel('Variance (log scale)')\n",
    "plt.title('Variance Comparison')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# MSE\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.loglog(sample_sizes, results['mom_mse'], 'bo-', label='MoM')\n",
    "plt.loglog(sample_sizes, results['mle_mse'], 'ro-', label='MLE')\n",
    "plt.xlabel('Sample Size')\n",
    "plt.ylabel('MSE (log scale)')\n",
    "plt.title('Mean Squared Error')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üìä Key Observations:\")\n",
    "print(\"‚Ä¢ MoM (unbiased) estimator has zero bias\")\n",
    "print(\"‚Ä¢ MLE estimator is biased but has lower variance\")\n",
    "print(\"‚Ä¢ For large samples, MLE bias becomes negligible\")\n",
    "print(\"‚Ä¢ MSE reflects the bias-variance tradeoff\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Student Activity (15 minutes)\n",
    "\n",
    "### üéØ Your Turn: Parameter Estimation Challenge\n",
    "\n",
    "You're given a dataset and need to:\n",
    "1. Identify the most likely distribution\n",
    "2. Estimate parameters using both MoM and MLE\n",
    "3. Compare the methods\n",
    "4. Validate your results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mystery dataset - what distribution does this come from?\n",
    "np.random.seed(123)  # Different seed for the activity\n",
    "mystery_data = np.array([\n",
    "    2.3, 1.8, 3.1, 2.7, 1.9, 4.2, 2.1, 3.8, 2.9, 1.6,\n",
    "    3.5, 2.4, 1.7, 3.9, 2.8, 2.2, 4.1, 1.5, 3.3, 2.6,\n",
    "    1.9, 3.7, 2.5, 2.0, 3.4, 2.7, 1.8, 4.0, 2.3, 3.2,\n",
    "    2.1, 2.9, 1.7, 3.6, 2.4, 2.8, 3.1, 1.6, 3.8, 2.2,\n",
    "    2.6, 1.9, 3.3, 2.7, 2.0, 3.5, 2.5, 1.8, 4.2, 2.9\n",
    "])\n",
    "\n",
    "print(\"üîç Mystery Dataset Analysis\")\n",
    "print(f\"Sample size: {len(mystery_data)}\")\n",
    "print(f\"Sample mean: {np.mean(mystery_data):.3f}\")\n",
    "print(f\"Sample std: {np.std(mystery_data, ddof=1):.3f}\")\n",
    "print(f\"Sample min: {np.min(mystery_data):.3f}\")\n",
    "print(f\"Sample max: {np.max(mystery_data):.3f}\")\n",
    "\n",
    "# Visualize the data\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.hist(mystery_data, bins=10, density=True, alpha=0.7, color='lightpink', edgecolor='black')\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Density')\n",
    "plt.title('Histogram')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.boxplot(mystery_data)\n",
    "plt.ylabel('Value')\n",
    "plt.title('Box Plot')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "stats.probplot(mystery_data, dist=\"norm\", plot=plt)\n",
    "plt.title('Q-Q Plot (Normal)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìù Your Tasks:\")\n",
    "print(\"1. Based on the visualizations, what distribution do you think this is?\")\n",
    "print(\"2. Estimate parameters using Method of Moments\")\n",
    "print(\"3. Estimate parameters using Maximum Likelihood\")\n",
    "print(\"4. Compare your estimates\")\n",
    "print(\"5. Test goodness of fit\")\n",
    "print(\"\\n‚è∞ You have 15 minutes. Work in the cells below!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üöÄ YOUR CODE HERE - Task 1: Hypothesis about distribution\n",
    "# Look at the histogram, box plot, and Q-Q plot\n",
    "# What distribution family do you think this data comes from?\n",
    "\n",
    "print(\"My hypothesis: The data appears to come from a _______ distribution\")\n",
    "print(\"Reasoning: \")\n",
    "# Your reasoning here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üöÄ YOUR CODE HERE - Task 2: Method of Moments estimation\n",
    "# Assuming normal distribution, estimate Œº and œÉ using MoM\n",
    "\n",
    "# Calculate sample moments\n",
    "# Your code here\n",
    "\n",
    "# MoM estimates\n",
    "# Your code here\n",
    "\n",
    "print(f\"Method of Moments estimates:\")\n",
    "print(f\"ŒºÃÇ_MoM = \")\n",
    "print(f\"œÉÃÇ_MoM = \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üöÄ YOUR CODE HERE - Task 3: Maximum Likelihood estimation\n",
    "# Estimate Œº and œÉ using MLE\n",
    "\n",
    "# MLE estimates for normal distribution\n",
    "# Your code here\n",
    "\n",
    "print(f\"Maximum Likelihood estimates:\")\n",
    "print(f\"ŒºÃÇ_MLE = \")\n",
    "print(f\"œÉÃÇ_MLE = \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üöÄ YOUR CODE HERE - Task 4: Compare estimates and visualize fit\n",
    "# Plot the data with both fitted distributions\n",
    "\n",
    "# Your visualization code here\n",
    "\n",
    "print(\"Comparison of estimates:\")\n",
    "print(f\"Difference in Œº estimates: \")\n",
    "print(f\"Difference in œÉ estimates: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üöÄ YOUR CODE HERE - Task 5: Goodness of fit test\n",
    "# Use Kolmogorov-Smirnov test or similar\n",
    "\n",
    "# Your testing code here\n",
    "\n",
    "print(\"Goodness of fit results:\")\n",
    "print(f\"Test statistic: \")\n",
    "print(f\"P-value: \")\n",
    "print(f\"Conclusion: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solutions (Hidden - Reveal After Activity)\n",
    "\n",
    "<details>\n",
    "<summary>Click to reveal solutions</summary>\n",
    "\n",
    "### Solution Code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION - Task 1: Distribution identification\n",
    "print(\"‚úÖ SOLUTION - Task 1:\")\n",
    "print(\"The data appears to come from a NORMAL distribution\")\n",
    "print(\"Reasoning:\")\n",
    "print(\"‚Ä¢ Histogram shows roughly bell-shaped distribution\")\n",
    "print(\"‚Ä¢ Box plot shows symmetric distribution with no extreme outliers\")\n",
    "print(\"‚Ä¢ Q-Q plot points lie approximately on the diagonal line\")\n",
    "print(\"‚Ä¢ Data range is continuous and symmetric around the mean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION - Task 2: Method of Moments\n",
    "print(\"\\n‚úÖ SOLUTION - Task 2:\")\n",
    "\n",
    "# For normal distribution: E[X] = Œº, Var(X) = œÉ¬≤\n",
    "mu_mom_solution = np.mean(mystery_data)\n",
    "sigma_mom_solution = np.std(mystery_data, ddof=1)  # Using unbiased estimator\n",
    "\n",
    "print(f\"Method of Moments estimates:\")\n",
    "print(f\"ŒºÃÇ_MoM = {mu_mom_solution:.3f}\")\n",
    "print(f\"œÉÃÇ_MoM = {sigma_mom_solution:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION - Task 3: Maximum Likelihood\n",
    "print(\"\\n‚úÖ SOLUTION - Task 3:\")\n",
    "\n",
    "# For normal distribution MLE\n",
    "mu_mle_solution = np.mean(mystery_data)\n",
    "sigma_mle_solution = np.std(mystery_data, ddof=0)  # MLE uses n in denominator\n",
    "\n",
    "print(f\"Maximum Likelihood estimates:\")\n",
    "print(f\"ŒºÃÇ_MLE = {mu_mle_solution:.3f}\")\n",
    "print(f\"œÉÃÇ_MLE = {sigma_mle_solution:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION - Task 4: Comparison and visualization\n",
    "print(\"\\n‚úÖ SOLUTION - Task 4:\")\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(mystery_data, bins=12, density=True, alpha=0.7, color='lightpink', \n",
    "         edgecolor='black', label='Data')\n",
    "\n",
    "x_range = np.linspace(mystery_data.min() - 0.5, mystery_data.max() + 0.5, 200)\n",
    "\n",
    "# MoM fitted distribution\n",
    "mom_pdf = stats.norm.pdf(x_range, mu_mom_solution, sigma_mom_solution)\n",
    "plt.plot(x_range, mom_pdf, 'b-', linewidth=2, \n",
    "         label=f'MoM: N({mu_mom_solution:.2f}, {sigma_mom_solution:.2f}¬≤)')\n",
    "\n",
    "# MLE fitted distribution\n",
    "mle_pdf = stats.norm.pdf(x_range, mu_mle_solution, sigma_mle_solution)\n",
    "plt.plot(x_range, mle_pdf, 'r--', linewidth=2, \n",
    "         label=f'MLE: N({mu_mle_solution:.2f}, {sigma_mle_solution:.2f}¬≤)')\n",
    "\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Density')\n",
    "plt.title('Parameter Estimation Results')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Comparison of estimates:\")\n",
    "print(f\"Œº estimates are identical: {mu_mom_solution:.3f}\")\n",
    "print(f\"œÉ difference: {abs(sigma_mom_solution - sigma_mle_solution):.4f}\")\n",
    "print(f\"MoM uses n-1 in denominator (unbiased), MLE uses n (biased but consistent)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION - Task 5: Goodness of fit\n",
    "print(\"\\n‚úÖ SOLUTION - Task 5:\")\n",
    "\n",
    "# Kolmogorov-Smirnov test\n",
    "ks_stat_sol, ks_p_sol = kstest(mystery_data, \n",
    "                               lambda x: stats.norm.cdf(x, mu_mle_solution, sigma_mle_solution))\n",
    "\n",
    "# Anderson-Darling test\n",
    "ad_stat_sol, ad_crit_sol, ad_sig_sol = anderson(mystery_data, dist='norm')\n",
    "\n",
    "print(f\"Kolmogorov-Smirnov Test:\")\n",
    "print(f\"Test statistic: {ks_stat_sol:.4f}\")\n",
    "print(f\"P-value: {ks_p_sol:.4f}\")\n",
    "print(f\"Conclusion: {'Reject H0' if ks_p_sol < 0.05 else 'Fail to reject H0'} (normal distribution)\")\n",
    "\n",
    "print(f\"\\nAnderson-Darling Test:\")\n",
    "print(f\"Test statistic: {ad_stat_sol:.4f}\")\n",
    "print(f\"Critical value (5%): {ad_crit_sol[2]:.4f}\")\n",
    "print(f\"Conclusion: {'Reject' if ad_stat_sol > ad_crit_sol[2] else 'Accept'} normality hypothesis\")\n",
    "\n",
    "print(f\"\\nüéØ OVERALL CONCLUSION:\")\n",
    "print(f\"The mystery data appears to be normally distributed with:\")\n",
    "print(f\"‚Ä¢ Mean ‚âà {mu_mle_solution:.2f}\")\n",
    "print(f\"‚Ä¢ Standard deviation ‚âà {sigma_mle_solution:.2f}\")\n",
    "print(f\"‚Ä¢ Both MoM and MLE give very similar results\")\n",
    "print(f\"‚Ä¢ Goodness-of-fit tests support the normal distribution hypothesis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary & Key Takeaways\n",
    "\n",
    "### üéØ What We Learned Today\n",
    "\n",
    "1. **Parameter Estimation Fundamentals**\n",
    "   - The goal: estimate unknown parameters from observed data\n",
    "   - Two main approaches: Method of Moments (MoM) and Maximum Likelihood Estimation (MLE)\n",
    "\n",
    "2. **Method of Moments (MoM)**\n",
    "   - ‚úÖ Simple and intuitive\n",
    "   - ‚úÖ Always exists (if moments exist)\n",
    "   - ‚ùå Not always optimal (higher variance)\n",
    "   - Best for: quick estimates, starting values, simple distributions\n",
    "\n",
    "3. **Maximum Likelihood Estimation (MLE)**\n",
    "   - ‚úÖ Optimal properties (minimum variance, consistency, efficiency)\n",
    "   - ‚úÖ Strong theoretical foundation\n",
    "   - ‚ùå May require numerical optimization\n",
    "   - Best for: precise estimates, inference, model comparison\n",
    "\n",
    "4. **Practical Considerations**\n",
    "   - Always validate your assumptions\n",
    "   - Use diagnostic tools (Q-Q plots, goodness-of-fit tests)\n",
    "   - Consider bootstrap for uncertainty quantification\n",
    "   - Start simple (MoM) then refine (MLE) if needed\n",
    "\n",
    "### üöÄ Next Steps\n",
    "\n",
    "- **Advanced Topics**: Bayesian estimation, robust methods, regularization\n",
    "- **Applications**: Dive deeper into ML models (neural networks, time series)\n",
    "- **Practice**: Apply these methods to your own datasets\n",
    "\n",
    "### üìö Additional Resources\n",
    "\n",
    "- **Books**: Casella & Berger \"Statistical Inference\", Lehmann & Casella \"Theory of Point Estimation\"\n",
    "- **Software**: Practice with `scipy.optimize`, `statsmodels`, `PyMC` for Bayesian methods\n",
    "- **Datasets**: UCI ML Repository, Kaggle for real-world practice\n",
    "\n",
    "---\n",
    "\n",
    "**üéâ Congratulations!** You've completed the Parameter Estimation Workshop. You now have practical skills in:\n",
    "- Implementing MoM and MLE estimators\n",
    "- Comparing different estimation methods\n",
    "- Validating estimation results\n",
    "- Applying parameter estimation to real problems\n",
    "\n",
    "**Questions?** Don't hesitate to ask or explore the additional resources!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "colab": {
   "provenance": [],
   "collapsed_sections": []
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}