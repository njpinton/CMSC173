<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>slides</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            max-width: 1200px;
            margin: 0 auto;
            padding: 20px;
            background-color: #f5f5f5;
        }
        .page {
            background-color: white;
            margin: 20px 0;
            padding: 40px;
            box-shadow: 0 2px 8px rgba(0,0,0,0.1);
            border-radius: 4px;
            min-height: 400px;
        }
        .page-number {
            color: #666;
            font-size: 14px;
            margin-bottom: 20px;
            border-bottom: 1px solid #ddd;
            padding-bottom: 10px;
        }
        h1, h2, h3 {
            color: #333;
        }
        .page-content {
            line-height: 1.8;
            white-space: pre-wrap;
            font-family: 'Courier New', monospace;
            font-size: 12pt;
        }
        .note {
            color: #666;
            font-style: italic;
            margin: 10px 0;
            padding: 10px;
            background-color: #f9f9f9;
            border-left: 3px solid #ccc;
        }
    </style>
</head>
<body>
<div class="note">
    This is a text-only version of the slides. For the full visual experience with images and formatting, please refer to the PDF version.
</div>

<div class="page">
<div class="page-number">Page 1 of 40</div>
<div class="page-content">
Introduction to Machine Learning
CMSC 173 - Machine Learning
Noel Jeffrey Pinton
October 17, 2025
Department of Computer Science
University of the Philippines - Cebu
1

</div>
</div>
<div class="page">
<div class="page-number">Page 2 of 40</div>
<div class="page-content">
Course Overview
What is Machine Learning?
Types of Machine Learning
Supervised Learning
Unsupervised Learning
Semi-Supervised Learning
Reinforcement Learning
The Machine Learning Pipeline
Key Challenges in ML
Course Structure
Summary
2

</div>
</div>
<div class="page">
<div class="page-number">Page 3 of 40</div>
<div class="page-content">
What is Machine Learning?

</div>
</div>
<div class="page">
<div class="page-number">Page 4 of 40</div>
<div class="page-content">
What is Machine Learning?
Formal Definition
Machine Learning (ML) is the science of getting computers to
learn and act like humans do, and improve their learning over
time in autonomous fashion, by feeding them data and
information.
Key Characteristics
• Learning from data without explicit programming
• Improving performance with experience
• Discovering patterns in complex datasets
• Making predictions or decisions
Traditional Programming vs ML
Traditional:
Rules + Data →Answers
Machine Learning:
Data + Answers →Rules
Core Insight
ML finds the rules automatically from
examples!
3

</div>
</div>
<div class="page">
<div class="page-number">Page 5 of 40</div>
<div class="page-content">
Historical Context
“A computer would deserve to be called intelligent if it could deceive a human into believing that it was human.” — Alan Turing
Major Milestones
• 1950s: Alan Turing - ”Can machines think?”
• 1957: Perceptron (Frank Rosenblatt)
• 1986: Backpropagation popularized
• 1990s: Support Vector Machines
• 1997: Deep Blue defeats Kasparov
• 2006: Deep Learning renaissance
• 2012: AlexNet wins ImageNet
• 2016: AlphaGo defeats Lee Sedol
• 2020s: Large Language Models
The Three AI Winters
Periods of reduced funding and interest:
• 1970s: Perceptron limitations
• 1987-1993: Expert systems fail
• Post-2000: AI hype deflation
Current Era
We’re in the Deep Learning Revolution:
• Big data availability
• GPU acceleration
• Novel architectures (Transformers)
4

</div>
</div>
<div class="page">
<div class="page-number">Page 6 of 40</div>
<div class="page-content">
Real-World Applications
“Machine learning is the last invention that humanity will ever need to make.” — Nick Bostrom
Computer Vision
• Medical image diagnosis
• Autonomous vehicles
• Facial recognition
• Object detection &amp; tracking
• Image generation (DALL-E, Midjourney)
Natural Language Processing
• Machine translation
Other Domains
• Finance: Fraud detection, trading
• Healthcare: Drug discovery, medicine
• E-commerce: Recommendations
• Gaming: AI opponents
• Manufacturing: Quality control
• Agriculture: Crop monitoring
Impact
ML is transforming every industry!
5

</div>
</div>
<div class="page">
<div class="page-number">Page 7 of 40</div>
<div class="page-content">
Learning Objectives
By the end of this course, you will be able to:
1. Understand the fundamental concepts and mathematical foundations of machine learning
2. Distinguish between different types of learning paradigms (supervised, unsupervised, etc.)
3. Implement core ML algorithms from scratch using Python
4. Apply appropriate ML techniques to real-world problems
5. Evaluate model performance using rigorous metrics
6. Analyze the theoretical properties of learning algorithms
7. Compare different approaches and select optimal methods
8. Understand state-of-the-art techniques in deep learning
Prerequisites
CMSC 170: Linear algebra, probability theory, calculus, Python programming
6

</div>
</div>
<div class="page">
<div class="page-number">Page 8 of 40</div>
<div class="page-content">
Types of Machine Learning

</div>
</div>
<div class="page">
<div class="page-number">Page 9 of 40</div>
<div class="page-content">
Machine Learning Taxonomy
Machine Learning
Supervised Learning
Regression
Classification
Unsupervised Learning
Clustering
Dimensionality
Reduction
Others
Semi-supervised
Reinforcement
7

</div>
</div>
<div class="page">
<div class="page-number">Page 10 of 40</div>
<div class="page-content">
Supervised Learning

</div>
</div>
<div class="page">
<div class="page-number">Page 11 of 40</div>
<div class="page-content">
Supervised Learning
“Learning is finding out what you already know. Doing is demonstrating that you know it.” — Richard Bach
Definition
Learning from labeled data where each training example
consists of:
• Input: Feature vector x ∈Rd
• Output: Label/target y
Goal: Learn a function f : X →Y such that f (x) ≈y
Training Process
Given dataset D = {(x1, y1), . . . , (xn, yn)}:
1. Choose a hypothesis class H
2. Define a loss function L(y, ˆy)
Key Properties
• Labeled data required
• Teacher signal guides learning
• Generalization to new examples
• Performance measurable
Two Main Tasks
1. Regression (y ∈R)
Predict continuous values
2. Classification (y ∈{1, . . . , K})
Predict discrete categories
8

</div>
</div>
<div class="page">
<div class="page-number">Page 12 of 40</div>
<div class="page-content">
Regression: Predicting Continuous Values
“All models are wrong, but some are useful.” — George Box
Problem Formulation
Input: x ∈Rd (features)
Output: y ∈R (continuous target)
Model: ˆy = f (x; θ)
Common Loss Functions
Mean Squared Error (MSE):
L(θ) = 1
n
n
X
i=1
(yi −ˆyi)2
Mean Absolute Error (MAE):
L(θ) = 1
n
X
|yi −ˆyi|
Regression Algorithms
• Linear Regression
• Ridge Regression (L2 regularization)
• Lasso Regression (L1 regularization)
• Polynomial Regression
• Cubic Splines
• Support Vector Regression (SVR)
• Decision Tree Regression
• Neural Networks
Real-World Examples
• House price prediction
9

</div>
</div>
<div class="page">
<div class="page-number">Page 13 of 40</div>
<div class="page-content">
Classification: Predicting Categories
“The goal is to turn data into information, and information into insight.” — Carly Fiorina
Problem Formulation
Input: x ∈Rd (features)
Output: y ∈{1, 2, . . . , K} (class label)
Model: ˆy = arg maxk P(y = k|x)
Types of Classification
Binary Classification: K = 2
• Spam vs. not spam
• Disease vs. healthy
Multi-class: K &gt; 2
• Digit recognition (0-9)
• Animal species classification
Classification Algorithms
• Logistic Regression
• Na¨ıve Bayes
• K-Nearest Neighbors (KNN)
• Decision Trees
• Support Vector Machines (SVM)
• Random Forests
• Gradient Boosting
• Neural Networks
Common Loss Functions
Cross-Entropy (log loss):
10

</div>
</div>
<div class="page">
<div class="page-number">Page 14 of 40</div>
<div class="page-content">
Unsupervised Learning

</div>
</div>
<div class="page">
<div class="page-number">Page 15 of 40</div>
<div class="page-content">
Unsupervised Learning
Definition
Learning from unlabeled data without explicit target
outputs:
• Input: Feature vectors {x1, . . . , xn}
• Output: None (discover structure)
Goal: Discover hidden patterns, structures, or relationships
in data
Main Tasks
1. Clustering
• Group similar data points
• Algorithms: K-Means, DBSCAN, Hierarchical
2. Dimensionality Reduction
• Compress high-dimensional data
• Algorithms: PCA, t-SNE, UMAP
3. Density Estimation
• Model the data distribution
Key Characteristics
• No labels required
• Exploratory in nature
• Structure discovery
• Performance harder to measure
Applications
• Customer segmentation
• Anomaly detection
• Data visualization
• Feature extraction
• Compression
• Recommender systems
Challenge
How do we evaluate without labels?
11

</div>
</div>
<div class="page">
<div class="page-number">Page 16 of 40</div>
<div class="page-content">
Clustering: Grouping Similar Data
“Without data, you’re just another person with an opinion.” — W. Edwards Deming
Problem Formulation
Input: Dataset {x1, . . . , xn}
Output: Cluster assignments {c1, . . . , cn}
Goal: Maximize intra-cluster similarity, minimize
inter-cluster similarity
K-Means Algorithm
Objective: Minimize within-cluster variance
min
{µk },{ci }
n
X
i=1
∥xi −µci ∥2
Algorithm:
Other Clustering Methods
Hierarchical Clustering:
• Agglomerative (bottom-up)
• Divisive (top-down)
• Creates dendrogram
DBSCAN:
• Density-based
• Finds arbitrary shapes
• Handles noise/outliers
Gaussian Mixture Models:
12

</div>
</div>
<div class="page">
<div class="page-number">Page 17 of 40</div>
<div class="page-content">
Dimensionality Reduction: Compression &amp; Visualization
“In God we trust, all others must bring data.” — W. Edwards Deming
Motivation
High-dimensional data (d ≫1) causes:
• Curse of dimensionality
• Computational complexity
• Overfitting
• Difficulty in visualization
Solution: Project to lower dimensions while
preserving structure
Principal Component Analysis (PCA)
Goal: Find directions of maximum variance
M th d
Other Techniques
Linear Methods:
• PCA (maximum variance)
• LDA (maximum discrimination)
• ICA (independent components)
Non-linear Methods:
• Kernel PCA
• t-SNE (visualization)
• UMAP (topology preservation)
• Autoencoders (neural networks)
13

</div>
</div>
<div class="page">
<div class="page-number">Page 18 of 40</div>
<div class="page-content">
Semi-Supervised Learning

</div>
</div>
<div class="page">
<div class="page-number">Page 19 of 40</div>
<div class="page-content">
Semi-Supervised Learning
Definition
Learning from both labeled and unlabeled data:
• Labeled: DL = {(x1, y1), . . . , (xl, yl)}
• Unlabeled: DU = {xl+1, . . . , xl+u}
• Typically l ≪u (few labels, many unlabeled)
Goal: Leverage unlabeled data to improve performance
Fundamental Assumptions
1. Smoothness Assumption
• Nearby points share same label
2. Cluster Assumption
• Data forms discrete clusters
• Points in same cluster have same label
3. Manifold Assumption
• High-dim data lies on low-dim manifold
Common Approaches
Self-Training:
• Train on labeled data
• Predict unlabeled data
• Add confident predictions to training set
• Iterate
Co-Training:
• Multiple views of data
• Train separate classifiers
• Exchange confident predictions
Graph-Based Methods:
• Construct similarity graph
• Propagate labels
Why Semi-Supervised?
Labels are expensive! (Human annotation,
expert knowledge, time)
14

</div>
</div>
<div class="page">
<div class="page-number">Page 20 of 40</div>
<div class="page-content">
Reinforcement Learning

</div>
</div>
<div class="page">
<div class="page-number">Page 21 of 40</div>
<div class="page-content">
Reinforcement Learning
“You can use a spoon to eat soup, but it’s better to use a ladle. Learning is choosing the right tool.” — Yann LeCun
Definition
Learning through interaction with an environment:
• Agent takes actions
• Environment provides states &amp; rewards
• Goal: Maximize cumulative reward
Markov Decision Process (MDP)
Formal framework: (S, A, P, R, γ)
• S: State space
• A: Action space
• P(s′|s, a): Transition probabilities
• R(s a s′): Reward function
RL vs Other Paradigms
Key Differences:
• No direct supervision
• Delayed rewards
• Exploration vs exploitation
• Sequential decision making
• Trial and error learning
Classic Algorithms
• Q-Learning
• SARSA
• Policy Gradient
15

</div>
</div>
<div class="page">
<div class="page-number">Page 22 of 40</div>
<div class="page-content">
RL Example: Q-Learning
Q-Learning Algorithm
Goal: Learn optimal action-value function
Q∗(s, a) = max
π
E
" ∞
X
t=0
γtRt | s0 = s, a0 = a, π
#
Update Rule:
Q(s, a) ←Q(s, a) + α[r + γ max
a′ Q(s′, a′) −Q(s, a)]
where:
• α: Learning rate
• r: Immediate reward
• s′: Next state
• γ: Discount factor
Policy: π(s) = arg maxa Q(s, a)
Algorithm Pseudocode
1: Initialize Q(s, a) arbitrarily
2: for each episode do
3:
Initialize state s
4:
repeat
5:
Choose action a using ϵ-greedy policy
6:
Take action a, observe r, s′
7:
Q(s, a) ←Q(s, a)+α[r +γ maxa′ Q(s′, a′)−
Q(s, a)]
8:
s ←s′
9:
until s is terminal
10: end for
Key Concepts
Exploration vs Exploitation:
• ϵ-greedy: explore with probability ϵ
• Balances trying new actions vs using known
good ones
16

</div>
</div>
<div class="page">
<div class="page-number">Page 23 of 40</div>
<div class="page-content">
The Machine Learning Pipeline

</div>
</div>
<div class="page">
<div class="page-number">Page 24 of 40</div>
<div class="page-content">
The ML Pipeline: From Data to Deployment
1. Data Collection
2. Data Cleaning
3. EDA
4. Feature Engineering
5. Train/Test Split
6. Model Selection
7. Training
8. Hyperpa-
rameter Tuning
9. Evaluation
10. Validation
11. Deployment
12. Monitoring
Retrain
Key Insight
ML is iterative! Model performance informs feature engineering, data collection, etc.
17

</div>
</div>
<div class="page">
<div class="page-number">Page 25 of 40</div>
<div class="page-content">
Data Preprocessing
“Garbage in, garbage out.” — George Fuechsel
Data Cleaning
Common Issues:
• Missing values: Imputation, deletion
• Outliers: Detect and handle
• Duplicates: Remove
• Inconsistencies: Standardize formats
• Noise: Filter or smooth
Feature Scaling
Why? Many algorithms sensitive to feature scales
Standardization (Z-score):
Feature Engineering
Creating new features from existing ones:
• Polynomial features: x1x2, x2
1
• Domain-specific transformations
• Binning continuous variables
• One-hot encoding categorical
• Date/time feature extraction
• Text vectorization (TF-IDF)
Train/Test Split
Why? Evaluate generalization
18

</div>
</div>
<div class="page">
<div class="page-number">Page 26 of 40</div>
<div class="page-content">
Model Selection &amp; Training
Choosing a Model
Consider:
• Problem type: Regression, classification, etc.
• Data size: Deep learning needs more data
• Interpretability: Linear models vs black boxes
• Training time: Real-time vs offline
• Prediction speed: Production requirements
No Free Lunch Theorem
Theorem: No single algorithm works best for all
problems
Implication: Must try multiple approaches and
validate empirically
Start Simple!
1. Simple baseline (mean, majority class)
2. Linear model
3. More complex models
4. Ensemble methods
Training Process
Optimization: Minimize loss function
θ∗= arg min
θ L(θ; D)
Common Optimizers:
• Gradient Descent
• Stochastic Gradient Descent (SGD)
• Adam (adaptive learning rate)
• RMSprop
Hyperparameter Tuning
Hyperparameters: Set before training
• Learning rate, regularization strength
• Number of layers, hidden units
• Tree depth, number of trees
Search Methods:
• Grid search
• Random search
19

</div>
</div>
<div class="page">
<div class="page-number">Page 27 of 40</div>
<div class="page-content">
Model Evaluation Metrics
Regression Metrics
Mean Squared Error (MSE):
MSE = 1
n
n
X
i=1
(yi −ˆyi)2
Root MSE (RMSE):
RMSE =
√
MSE
Mean Absolute Error (MAE):
MAE = 1
n
n
X
i=1
|yi −ˆyi|
R-squared (coefficient of determination):
R2 = 1 −
P
i(yi −ˆyi)2
P
i(yi −¯y)2
Range: (−∞, 1], closer to 1 is better
Classification Metrics
Accuracy:
Acc = correct predictions
total predictions
Precision (positive predictive value):
Prec =
TP
TP + FP
Recall (sensitivity, true positive rate):
Rec =
TP
TP + FN
F1-Score (harmonic mean):
F1 = 2 · Prec · Rec
Prec + Rec
ROC-AUC: Area under ROC curve
Important
20

</div>
</div>
<div class="page">
<div class="page-number">Page 28 of 40</div>
<div class="page-content">
Key Challenges in ML

</div>
</div>
<div class="page">
<div class="page-number">Page 29 of 40</div>
<div class="page-content">
Bias-Variance Tradeoff
Decomposition of Expected Error
For regression, expected test error:
E[(y −ˆf (x))2] = Bias2 + Variance + Noise
Bias: Error from wrong assumptions
Bias[ˆf (x)] = E[ˆf (x)] −f (x)
Variance: Error from sensitivity to training set
Var[ˆf (x)] = E[(ˆf (x) −E[ˆf (x)])2]
Noise: Irreducible error σ2
The Tradeoff
• Simple models: High bias, low variance
• Complex models: Low bias, high variance
• Goal: Find sweet spot!
Underfitting
Symptoms:
• High training error
• High test error
• Model too simple
Solutions:
• More features
• More complex model
• Less regularization
Overfitting
Symptoms:
• Low training error
• High test error
• Model too complex
Solutions:
• More training data
21

</div>
</div>
<div class="page">
<div class="page-number">Page 30 of 40</div>
<div class="page-content">
Regularization Techniques
L2 Regularization (Ridge)
Modified objective:
min
θ L(θ) + λ∥θ∥2
2
where λ &gt; 0 is regularization strength
Effect:
• Penalizes large weights
• Shrinks coefficients toward zero
• Improves generalization
• Handles multicollinearity
Closed-form solution (linear regression):
ˆθ = (XT X + λI)−1XT y
L1 Regularization (Lasso)
Modified objective:
min
θ L(θ) + λ∥θ∥1
Effect:
• Sparse solutions (some θi = 0)
• Automatic feature selection
• More aggressive than L2
No closed-form: Use iterative methods
Elastic Net
Combines L1 and L2:
min
θ L(θ) + λ1∥θ∥1 + λ2∥θ∥2
2
Benefits:
• Sparsity from L1
• Stability from L2
• Best of both worlds
22

</div>
</div>
<div class="page">
<div class="page-number">Page 31 of 40</div>
<div class="page-content">
The Curse of Dimensionality
Problem Statement
As dimensionality d increases:
• Volume grows exponentially: V ∝rd
• Data becomes sparse: Points far apart
• Distance metrics break down: All points equidistant
• Overfitting risk increases: More parameters to fit
Mathematical Insight
In high dimensions, volume concentrated in corners:
Vcorners
Vtotal
= 1 −

1 −1
2d
2d
≈1 −e−1
For unit hypercube, most volume is near edges!
Data Requirements
To maintain density, need n ∝cd samples where c &gt; 1
Solutions
1. Dimensionality Reduction
• PCA, t-SNE, UMAP
• Feature selection
2. Feature Selection
• Filter methods (correlation)
• Wrapper methods (RFE)
• Embedded (Lasso, trees)
3. Regularization
• L1/L2 penalties
• Early stopping
4. Collect More Data
• Exponentially more needed
• Often impractical
Rule of Thumb
n ≥10 · d for reliable models
23

</div>
</div>
<div class="page">
<div class="page-number">Page 32 of 40</div>
<div class="page-content">
Course Structure

</div>
</div>
<div class="page">
<div class="page-number">Page 33 of 40</div>
<div class="page-content">
CMSC 173 Course Topics
Core Foundations
I. Overview (Today!)
• Learning paradigms
• Applications
II. Parameter Estimation
• Method of Moments
• Maximum Likelihood Estimation
III. Regression
• Linear Regression
• Lasso &amp; Ridge
• Cubic Splines
IV. Model Selection
• Bias-Variance Decomposition
• Cross-Validation
• Regularization
Advanced Methods
V. Classification
• Logistic Regression, Na¨ıve Bayes
• KNN, Decision Trees
VI. Kernel Methods
• Support Vector Machines
• Kernel trick
VII. Dimensionality Reduction
• Principal Component Analysis
VIII. Neural Networks
• Feedforward Networks
• CNNs, Transformers
• Generative Models
IX. Clustering
• K-Means, Hierarchical
G
i
Mi t
M d l
24

</div>
</div>
<div class="page">
<div class="page-number">Page 34 of 40</div>
<div class="page-content">
Learning Resources
Recommended Textbooks
Primary:
• Murphy, K. P. (2022). Probabilistic Machine
Learning: An Introduction. MIT Press.
• Bishop, C. M. (2006). Pattern Recognition and
Machine Learning. Springer.
Supplementary:
• Hastie et al. (2009). The Elements of Statistical
Learning. Springer.
• Goodfellow et al. (2016). Deep Learning. MIT
Press.
Online Resources
• Scikit-learn documentation
• PyTorch/TensorFlow tutorials
• Coursera ML courses (Andrew Ng)
• Stanford CS229 lecture notes
• ArXiv.org for research papers
Tools We’ll Use
• Python 3.8+
• NumPy, Pandas, Matplotlib
• Scikit-learn
• Jupyter Notebooks
• PyTorch (for deep learning)
Installation
Ensure you have Python and required packages installed before next session!
25

</div>
</div>
<div class="page">
<div class="page-number">Page 35 of 40</div>
<div class="page-content">
Best Practices in Machine Learning
“The best way to predict the future is to invent it.” — Alan Kay
Development Workflow
1. Start with baseline
• Simple model first
• Establish minimum performance
2. Iterate systematically
• Change one thing at a time
• Track experiments
• Version control (Git)
3. Validate rigorously
• Cross-validation
Common Pitfalls to Avoid
• Data leakage: Test data in training
• Ignoring class imbalance
• Not checking for overfitting
• Using wrong metrics
• Not scaling features
• Forgetting randomness: Set seeds!
• Over-engineering: Keep it simple
Reproducibility
Essential for science:
26

</div>
</div>
<div class="page">
<div class="page-number">Page 36 of 40</div>
<div class="page-content">
Ethics &amp; Responsible AI
“With great power comes great responsibility.” — Stan Lee (adapted from Voltaire)
Ethical Considerations
Bias &amp; Fairness:
• Training data may contain biases
• Models can amplify discrimination
• Ensure fairness across groups
Privacy:
• Protect sensitive information
• Anonymization techniques
• Comply with regulations (GDPR)
Transparency:
Societal Impact
Positive:
• Healthcare improvements
• Scientific discoveries
• Accessibility tools
• Environmental monitoring
Concerns:
• Job displacement
• Deepfakes &amp; misinformation
• Surveillance
• Autonomous weapons
27

</div>
</div>
<div class="page">
<div class="page-number">Page 37 of 40</div>
<div class="page-content">
Summary

</div>
</div>
<div class="page">
<div class="page-number">Page 38 of 40</div>
<div class="page-content">
Key Takeaways
What We Covered Today
1. Definition of Machine Learning: Learning from data to improve performance
2. Supervised Learning: Regression &amp; classification with labeled data
3. Unsupervised Learning: Clustering &amp; dimensionality reduction
4. Semi-Supervised Learning: Leveraging both labeled &amp; unlabeled data
5. Reinforcement Learning: Learning through interaction &amp; rewards
6. ML Pipeline: From data collection to deployment
7. Key Challenges: Bias-variance tradeoff, overfitting, curse of dimensionality
8. Best Practices: Systematic development, validation, ethics
Next Lecture
Parameter Estimation: Method of Moments &amp; Maximum Likelihood Estimation
28

</div>
</div>
<div class="page">
<div class="page-number">Page 39 of 40</div>
<div class="page-content">
Prepare for Next Session
Required Reading
Murphy (2022):
• Chapter 4: Statistics (4.1-4.3)
• Chapter 5: Decision Theory (5.1-5.2)
Bishop (2006):
• Chapter 1: Introduction (1.1-1.5)
• Chapter 2: Probability (2.1-2.3)
Practice Problems
1. Review probability theory
2. Linear algebra refresher
3. Set up Python environment
4. Install required packages
Questions to Ponder
1. When would you choose supervised vs
unsupervised learning?
2. How do you decide on train/test split ratio?
3. What metrics are appropriate for imbalanced
datasets?
4. How can we detect overfitting early?
5. What are ethical concerns in your domain of
interest?
Office Hours
Available for questions and discussion after class or
by appointment
29

</div>
</div>
<div class="page">
<div class="page-number">Page 40 of 40</div>
<div class="page-content">
Questions?
Thank you for your attention!
Next Lecture: Parameter Estimation
See you next time!
29

</div>
</div>

</body>
</html>
