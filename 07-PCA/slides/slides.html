<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>slides</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            max-width: 1200px;
            margin: 0 auto;
            padding: 20px;
            background-color: #f5f5f5;
        }
        .page {
            background-color: white;
            margin: 20px 0;
            padding: 40px;
            box-shadow: 0 2px 8px rgba(0,0,0,0.1);
            border-radius: 4px;
            min-height: 400px;
        }
        .page-number {
            color: #666;
            font-size: 14px;
            margin-bottom: 20px;
            border-bottom: 1px solid #ddd;
            padding-bottom: 10px;
        }
        h1, h2, h3 {
            color: #333;
        }
        .page-content {
            line-height: 1.8;
            white-space: pre-wrap;
            font-family: 'Courier New', monospace;
            font-size: 12pt;
        }
        .note {
            color: #666;
            font-style: italic;
            margin: 10px 0;
            padding: 10px;
            background-color: #f9f9f9;
            border-left: 3px solid #ccc;
        }
    </style>
</head>
<body>
<div class="note">
    This is a text-only version of the slides. For the full visual experience with images and formatting, please refer to the PDF version.
</div>

<div class="page">
<div class="page-number">Page 1 of 50</div>
<div class="page-content">
Principal Component Analysis
CMSC 173 - Machine Learning
Noel Jeffrey Pinton
October 12, 2025
Department of Computer Science
University of the Philippines - Cebu
1

</div>
</div>
<div class="page">
<div class="page-number">Page 2 of 50</div>
<div class="page-content">
Outline
Introduction &amp; Motivation
Mathematical Foundations
The PCA Algorithm
PCA Variants
Evaluation &amp; Component Selection
Applications
Best Practices &amp; Pitfalls
Summary
2

</div>
</div>
<div class="page">
<div class="page-number">Page 3 of 50</div>
<div class="page-content">
Introduction &amp; Motivation

</div>
</div>
<div class="page">
<div class="page-number">Page 4 of 50</div>
<div class="page-content">
What is Principal Component Analysis?
Definition
Principal Component Analysis (PCA) is a
statistical technique that transforms
high-dimensional data into a lower-dimensional
representation while preserving as much variance as
possible.
Key Characteristics
• Unsupervised learning method
• Linear dimensionality reduction
• Orthogonal transformation
• Variance maximization
Applications:
• Data visualization
• Feature extraction
• Noise reduction
• Image compression
Goal
Find new axes (principal components) that maximize variance and are uncorrelated with each other.
3

</div>
</div>
<div class="page">
<div class="page-number">Page 5 of 50</div>
<div class="page-content">
The Curse of Dimensionality
Problem
As the number of features increases, data becomes
increasingly sparse, making analysis difficult and
computationally expensive.
Challenges in High Dimensions:
• Data sparsity: Points become isolated
• Distance measures: Lose meaning
• Overfitting: Models fit noise
• Computation: Time/memory grows
exponentially
Hughes Phenomenon
Model performance initially improves with more
features, then degrades beyond an optimal point.
Example Impact:
• 10 features: 102 = 100 parameter pairs
• 100 features: 1002 = 10, 000 pairs
• 1000 features: 1, 000, 000 pairs!
Solution: PCA
Reduce dimensions while retaining information.
4

</div>
</div>
<div class="page">
<div class="page-number">Page 6 of 50</div>
<div class="page-content">
Why Dimensionality Reduction?
Motivation for PCA:
• Visualization: Reduce to 2D/3D for plotting
• Storage: Compress data efficiently
• Speed: Faster training and inference
• Noise removal: Filter out low-variance
components
• Feature extraction: Create meaningful features
• Collinearity: Remove redundant features
Core Principle
Most real-world data has intrinsic dimensionality
much lower than its ambient dimensionality.
Example: Images of faces
• Ambient: 10,000 pixels
• Intrinsic: 100-200 dimensions
Benefits vs Trade-offs:
Advantages
• Reduced computational cost
• Easier visualization
• Noise reduction
• Better generalization
Trade-offs
• Information loss
• Interpretability reduced
• Linear assumptions
• Sensitive to scaling
When to Use PCA
• Many correlated features
• Need for visualization
• Computational constraints
• Preprocessing for ML
5

</div>
</div>
<div class="page">
<div class="page-number">Page 7 of 50</div>
<div class="page-content">
Real-World Applications
Computer Vision:
• Face recognition: Eigenfaces
• Image compression: Reduce storage
• Object detection: Feature extraction
• Image denoising: Remove noise
Finance:
• Portfolio optimization
• Risk assessment
• Market trend analysis
• Fraud detection
Biology &amp; Medicine:
• Gene expression analysis
• Medical imaging
• Drug discovery
• Disease classification
Natural Language Processing:
• Document clustering
• Topic modeling
• Sentiment analysis
• Information retrieval
Signal Processing:
• Audio compression
• Speech recognition
• Sensor data analysis
• Anomaly detection
Recommendation Systems:
• User preference modeling
• Content filtering
• Collaborative filtering
• Feature engineering
Success Story
Netflix Prize: Teams used PCA for feature reduction
and achieved significant performance gains.
6

</div>
</div>
<div class="page">
<div class="page-number">Page 8 of 50</div>
<div class="page-content">
Mathematical Foundations

</div>
</div>
<div class="page">
<div class="page-number">Page 9 of 50</div>
<div class="page-content">
Variance: Measuring Spread
Definition
Variance measures how far data points spread from
their mean value.
For a single variable:
Var(X) = 1
n
n
X
i=1
(xi −¯x)2
(1)
= E[(X −E[X])2]
(2)
Properties:
• Always non-negative: Var(X) ≥0
• Zero variance: constant values
• Units: squared original units
• Standard deviation: σ =
p
Var(X)
Example Calculation:
Data: [2, 4, 6, 8, 10]
¯x = 2 + 4 + 6 + 8 + 10
5
= 6
(3)
Var(X) = (2 −6)2 + (4 −6)2 + · · ·
5
(4)
= 16 + 4 + 0 + 4 + 16
5
= 8
(5)
Intuition
High variance direction contains more information
than low variance direction.
7

</div>
</div>
<div class="page">
<div class="page-number">Page 10 of 50</div>
<div class="page-content">
Covariance: Measuring Linear Relationship
Definition
Covariance measures the joint variability of two
variables.
Formula:
Cov(X, Y ) = 1
n
n
X
i=1
(xi −¯x)(yi −¯y)
(6)
= E[(X −E[X])(Y −E[Y ])]
(7)
Interpretation:
• Cov(X, Y ) &gt; 0: Positive relationship
• Cov(X, Y ) &lt; 0: Negative relationship
• Cov(X, Y ) = 0: No linear relationship
• Cov(X, X) = Var(X)
Correlation
Normalized covariance:
ρXY = Cov(X, Y )
σX σY
∈[−1, 1]
Covariance Matrix:
For data X ∈Rn×d:
Σ = 1
n XT X ∈Rd×d
Structure:
Σ =


Var(X1)
Cov(X1, X2)
· · ·
Cov(X2, X1)
Var(X2)
· · ·
...
...
...


Properties:
• Symmetric: Σij = Σji
• Positive semi-definite
• Diagonal: variances
• Off-diagonal: covariances
PCA Key
Diagonalize covariance matrix to find uncorrelated
components.
8

</div>
</div>
<div class="page">
<div class="page-number">Page 11 of 50</div>
<div class="page-content">
Eigendecomposition: The Core of PCA
Definition
For a square matrix A, eigendecomposition finds
vectors and scalars such that:
Av = λv
Components:
• v: Eigenvector (direction)
• λ: Eigenvalue (scaling factor)
• Matrix only changes magnitude, not direction
For Covariance Matrix:
Σ = VΛVT
where:
• V: Eigenvector matrix
• Λ: Diagonal eigenvalue matrix
Geometric Interpretation:
• Eigenvectors: Principal component directions
• Eigenvalues: Variance along each PC
• Largest eigenvalue: Most variance
• Smallest eigenvalue: Least variance
PCA Connection
• PC directions = Eigenvectors
• PC importance = Eigenvalues
• Sorted by decreasing eigenvalue
9

</div>
</div>
<div class="page">
<div class="page-number">Page 12 of 50</div>
<div class="page-content">
Eigendecomposition Example
Problem: Find eigenvalues and eigenvectors
Σ =
"
4
2
2
3
#
Step 1: Characteristic equation
det(Σ −λI) = 0
det
"
4 −λ
2
2
3 −λ
#
= 0
(4 −λ)(3 −λ) −4 = 0
λ2 −7λ + 8 = 0
Step 2: Solve for eigenvalues
λ1 = 5.56,
λ2 = 1.44
Step 3: Find eigenvectors
For λ1 = 5.56:
(Σ −5.56I)v1 = 0
v1 =
"
0.79
0.61
#
For λ2 = 1.44:
v2 =
"
−0.61
0.79
#
Interpretation:
• PC1 direction: (0.79, 0.61)
• PC1 variance: 5.56
• PC2 direction: (−0.61, 0.79)
• PC2 variance: 1.44
• Total variance: 5.56 + 1.44 = 7
Note
Eigenvectors are orthogonal: v1 ⊥v2
10

</div>
</div>
<div class="page">
<div class="page-number">Page 13 of 50</div>
<div class="page-content">
Singular Value Decomposition (SVD)
Definition
Any matrix X ∈Rn×d can be decomposed as:
X = UΣVT
Components:
• U ∈Rn×n: Left singular vectors
Advantages of SVD for PCA:
• More numerically stable
• Works for n &lt; d or n &gt; d
• No need to form XT X
• Efficient algorithms available
Truncated SVD
11

</div>
</div>
<div class="page">
<div class="page-number">Page 14 of 50</div>
<div class="page-content">
SVD vs Eigendecomposition
Eigendecomposition Approach
Steps:
1. Center data: ˜X = X −¯X
2. Compute covariance: Σ = 1
n ˜XT ˜X
3. Eigendecomposition: Σ = VΛVT
4. Sort by eigenvalues
Complexity: O(d2n + d3)
Issues:
• Numerical instability for XT X
• Squaring condition number
• Memory: storing d × d matrix
SVD Approach
Steps:
1. Center data: ˜X = X −¯X
2. SVD: ˜X = UΣVT
3. PCs: columns of V
4. Variance: σ2
i /n
Complexity: O(min(nd2, n2d))
Advantages:
• Better numerical stability
• Works directly on X
• Better for n ≪d or n ≫d
• Modern implementations optimized
Recommendation
Use SVD for PCA in practice.
12

</div>
</div>
<div class="page">
<div class="page-number">Page 15 of 50</div>
<div class="page-content">
Projection and Reconstruction
Projection onto PCs:
Transform to k principal components:
Z = XVk
where Vk ∈Rd×k are first k PCs.
Properties:
• Z ∈Rn×k: Reduced representation
• Columns of Z are uncorrelated
• Maximum variance preserved
• Linear transformation
Reconstruction:
ˆX = ZVT
k = XVkVT
k
Reconstruction Error
d
Example:
Original: x = [5, 3]
PCs: v1 = [0.8, 0.6]
Projection:
z1 = xT v1 = 5(0.8) + 3(0.6) = 5.8
Reconstruction:
ˆx = z1v1 = 5.8[0.8, 0.6] = [4.64, 3.48]
Error:
∥x −ˆx∥= ∥[0.36, −0.48]∥= 0.6
13

</div>
</div>
<div class="page">
<div class="page-number">Page 16 of 50</div>
<div class="page-content">
Data Centering and Standardization
Centering (Required):
Remove mean from each feature:
˜xij = xij −¯xj
Why?
• PCA finds directions of variance
• Variance computed about mean
• Ensures PC1 passes through origin
Example Impact
Without standardization:
• Income: $20,000-$200,000
• Age: 20-80 years
• PC1 dominated by income scale
With standardization:
14

</div>
</div>
<div class="page">
<div class="page-number">Page 17 of 50</div>
<div class="page-content">
The PCA Algorithm

</div>
</div>
<div class="page">
<div class="page-number">Page 18 of 50</div>
<div class="page-content">
PCA Algorithm: Step-by-Step
Algorithm 1 Principal Component Analysis
Require: Data matrix X ∈Rn×d, number of compo-
nents k
Ensure: Principal components Vk, transformed data Z
1: Center the data:
2: ¯x = 1
n
Pn
i=1 xi
3: ˜X = X −1n¯xT
4: Optional: Standardize features
5: Compute covariance matrix:
6: Σ = 1
n ˜XT ˜X
7: OR compute SVD:
8: ˜X = UDVT
9: Extract top k eigenvectors/singular vectors:
10: Vk = [v1, . . . , vk]
11: Project data onto PCs:
12: Z = ˜XVk
13: return Vk, Z
Key Steps Explained:
Step 1-2: Centering
Essential for computing variance correctly. Shift
data so mean is at origin.
Step 3-4: Covariance/SVD
Two equivalent approaches. SVD more stable
numerically.
Step 5: Top-k Selection
Sort by eigenvalues (descending) and keep largest k
components.
Step 6: Projection
Transform original data to new coordinate system
defined by PCs.
Complexity:
• Eigendecomposition: O(d3)
• SVD: O(min(nd2, n2d))
15

</div>
</div>
<div class="page">
<div class="page-number">Page 19 of 50</div>
<div class="page-content">
Worked Example: PCA on Toy Dataset (Part 1)
Problem Setup:
Dataset with 5 points in 2D:
X =


1
2
2
3
3
4
4
5
5
6


Goal: Find principal components
Step 1: Center the data
Compute mean:
¯x = 1
5 [15, 20]T = [3, 4]T
Centered data:
˜X =


−2
−2
−1
−1
0
0
1
1
2
2


Step 2: Compute covariance matrix
Σ = 1
5
˜XT ˜X
= 1
5
"
−2
−1
0
1
2
−2
−1
0
1
2
#


−2
−2
−1
−1
0
0
1
1
2
2


= 1
5
"
10
10
10
10
#
=
"
2
2
2
2
#
Observation
Perfect correlation:
Cov(X1, X2) = Var(X1) = Var(X2) = 2
16

</div>
</div>
<div class="page">
<div class="page-number">Page 20 of 50</div>
<div class="page-content">
Worked Example: PCA on Toy Dataset (Part 2)
Step 3: Find eigenvalues
Solve: det(Σ −λI) = 0
det
"
2 −λ
2
2
2 −λ
#
= 0
(2 −λ)2 −4 = 0
λ2 −4λ = 0
λ(λ −4) = 0
Eigenvalues:
λ1 = 4,
λ2 = 0
Interpretation:
• All variance in first PC: λ1 = 4
• Zero variance in second PC: λ2 = 0
• Data lies on a line!
Step 4: Find eigenvectors
For λ1 = 4:
(Σ −4I)v1 = 0
"
−2
2
2
−2
#
v1 = 0
Solution: v1 =
1
√
2 [1, 1]T
For λ2 = 0:
v2 =
1
√
2
[1, −1]T
Verification:
vT
1 v2 = 1
2 (1 −1) = 0✓
Result
PC1: [0.707, 0.707] (diagonal direction)
PC2: [0.707, −0.707] (perpendicular)
17

</div>
</div>
<div class="page">
<div class="page-number">Page 21 of 50</div>
<div class="page-content">
Worked Example: PCA on Toy Dataset (Part 3)
Step 5: Project data onto PCs
Z = ˜XV
=


−2
−2
−1
−1
0
0
1
1
2
2


"
0.707
0.707
0.707
−0.707
#
=


−2.83
0
−1.41
0
0
0
1.41
0
2.83
0


Observation: All points lie on PC1 axis (PC2
coordinates are zero).
Step 6: Explained variance
Total variance:
Total = λ1 + λ2 = 4 + 0 = 4
Explained by PC1:
λ1
Total = 4
4 = 100%
Explained by PC2:
λ2
Total = 0
4 = 0%
Reconstruction (k=1):
ˆX = Z:,1vT
1 + ¯X
Perfect reconstruction since all variance in PC1!
Conclusion
This toy example shows perfectly correlated data
requiring only 1 dimension.
18

</div>
</div>
<div class="page">
<div class="page-number">Page 22 of 50</div>
<div class="page-content">
Worked Example: Non-Trivial 2D Case (Part 1)
New Dataset (4 points):
X =


1
2
2
1
3
4
4
3


Step 1: Center data
Mean: ¯x = [2.5, 2.5]T
˜X =


−1.5
−0.5
−0.5
−1.5
0.5
1.5
1.5
0.5


Step 2: Covariance
Σ = 1
4
˜XT ˜X
= 1
4
"
5
3
3
5
#
=
"
1.25
0.75
0.75
1.25
#
Step 3: Eigenvalues
det(Σ −λI) = 0
(1.25 −λ)2 −0.5625 = 0
λ2 −2.5λ + 1 = 0
Using quadratic formula:
λ = 2.5 ± √6.25 −4
2
= 2.5 ± 1.5
2
Eigenvalues:
λ1 = 2.0,
λ2 = 0.5
Variance explained:
• PC1:
2.0
2.5 = 80%
• PC2:
0.5
2.5 = 20%
• Both components carry information!
19

</div>
</div>
<div class="page">
<div class="page-number">Page 23 of 50</div>
<div class="page-content">
Worked Example: Non-Trivial 2D Case (Part 2)
Step 4: Eigenvectors
For λ1 = 2.0:
"
−0.75
0.75
0.75
−0.75
#
v1 = 0
Normalize: v1 =
1
√
2 [1, 1]T = [0.707, 0.707]T
For λ2 = 0.5:
v2 =
1
√
2
[1, −1]T = [0.707, −0.707]T
Step 5: Transform data
Z = ˜XV
=


−1.414
−0.707
−1.414
0.707
1.414
−0.707
1.414
0.707


Step 6: Reconstruction (k=1)
Keep only PC1:
ˆ˜X = Z:,1vT
1
=


−1.414
−1.414
1.414
1.414


h
0.707
0.707
i
=


−1.0
−1.0
−1.0
−1.0
1.0
1.0
1.0
1.0


Reconstruction error:
Error = λ2 = 0.5
Summary
Reduced from 2D to 1D, preserving 80% of variance.
20

</div>
</div>
<div class="page">
<div class="page-number">Page 24 of 50</div>
<div class="page-content">
PCA Variants

</div>
</div>
<div class="page">
<div class="page-number">Page 25 of 50</div>
<div class="page-content">
Kernel PCA: Non-linear Extension
Motivation
Standard PCA finds only linear relationships. Many
real-world datasets have non-linear structure.
Key Idea:
• Map data to high-dimensional space:
ϕ : Rd →H
• Apply PCA in feature space H
• Use kernel trick: K(xi, xj) = ϕ(xi)T ϕ(xj)
• Never explicitly compute ϕ(x)
Algorithm:
1. Compute kernel matrix: Kij = K(xi, xj)
2. Center kernel matrix
3. Eigendecompose: K = UΛUT
4. Project: zi = UT k(xi)
Common Kernels:
• RBF: K(x, y) = exp(−γ∥x −y∥2)
• Polynomial: K(x, y) = (xT y + c)d
• Sigmoid: K(x, y) = tanh(αxT y + c)
Applications
• Manifold learning
• Non-linear dimensionality reduction
• Feature extraction for non-linear data
Trade-off
More flexible but computationally expensive: O(n3)
21

</div>
</div>
<div class="page">
<div class="page-number">Page 26 of 50</div>
<div class="page-content">
Sparse PCA
Motivation
Standard PCA produces components that are linear
combinations of all original features, making
interpretation difficult.
Goal: Find PCs with sparse loadings
• Most loadings are exactly zero
• Only few features contribute to each PC
• Easier interpretation
F
l
i
b il i
Comparison:
Method
Variance
Sparsity
Standard PCA
100%
0%
Sparse PCA
95-98%
70-90%
Applications:
• Gene expression analysis
• Financial data analysis
• Text mining
• Any domain requiring interpretability
22

</div>
</div>
<div class="page">
<div class="page-number">Page 27 of 50</div>
<div class="page-content">
Incremental PCA
Motivation
Standard PCA requires entire dataset in memory.
Not feasible for:
• Very large datasets
• Streaming data
• Online learning scenarios
• Limited memory systems
Key Idea: Process data in mini-batches:
1. Initialize with first batch
2. Update PCs incrementally with new batches
3. Maintain approximate PCs online
4. Never load full dataset
Algorithm Sketch:
Advantages:
• Memory efficient: Constant memory
• Scalable: Handles large data
• Online: Updates with new data
• Streaming: Real-time processing
Complexity:
• Per batch: O(bd2) where b = batch size
• Memory: O(d2) instead of O(nd)
• Nearly identical results to batch PCA
Use Cases
• Large-scale image processing
• Sensor data streams
• Online recommendation systems
23

</div>
</div>
<div class="page">
<div class="page-number">Page 28 of 50</div>
<div class="page-content">
Probabilistic PCA
Motivation
Standard PCA is deterministic. Probabilistic PCA
provides:
• Generative model of data
• Handling of missing values
• Uncertainty quantification
• Bayesian extensions
Model:
z ∼N(0, Ik)
(8)
x|z ∼N(Wz + µ, σ2Id)
(9)
where:
• z ∈Rk: Latent variables
• W ∈Rd×k: Loading matrix
• σ2: Isotropic noise
Maximum Likelihood Solution:
WML = Vk(Λk −σ2I)1/2R
EM Algorithm:
1: Initialize W, σ2
2: repeat
3:
E-step: Compute E[z|x]
4:
M-step: Update W, σ2
5: until convergence
Advantages:
• Handle missing data naturally
• Mixture of PPCAs for clustering
• Bayesian inference possible
• Uncertainty quantification
Relationship to Standard PCA:
• As σ2 →0: Recovers standard PCA
• Same subspace in limit
• Additional noise model
Extensions
• Factor Analysis: Non-isotropic noise
• Variational PCA: Variational inference
• Sparse PPCA: Sparse loadings
24

</div>
</div>
<div class="page">
<div class="page-number">Page 29 of 50</div>
<div class="page-content">
Robust PCA
Motivation
Standard PCA is sensitive to:
• Outliers: Anomalous data points
• Corrupted entries: Missing or noisy values
• Sparse errors: Localized corruption
Problem Formulation:
Decompose X into:
X = L + S
where:
• L: Low-rank (clean data)
• S: Sparse (outliers/corruption)
Optimization:
min
L,S ∥L∥∗+ λ∥S∥1
subject to X = L + S
• ∥L∥∗: Nuclear norm (rank proxy)
Applications:
Video Surveillance
• L: Static background
• S: Moving objects
• Separates foreground/background
Face Recognition
• L: Face structure
• S: Occlusions, shadows
• Robust to partial occlusion
Solution Methods:
• Principal Component Pursuit (PCP)
• Alternating Direction Method (ADMM)
• Proximal gradient methods
Complexity:
• O(nd2k) per iteration
• Slower than standard PCA
• Worth it for corrupted data
25

</div>
</div>
<div class="page">
<div class="page-number">Page 30 of 50</div>
<div class="page-content">
Evaluation &amp; Component Selection

</div>
</div>
<div class="page">
<div class="page-number">Page 31 of 50</div>
<div class="page-content">
Scree Plot: Visualizing Variance
Definition
A scree plot displays eigenvalues (or explained
variance) for each principal component in decreasing
order.
How to Read:
• X-axis: Principal component number
• Y-axis: Eigenvalue or explained variance
• Look for ”elbow” point
• Steep drop indicates important PCs
• Flat tail indicates noise
Elbow Method:
• Find point where curve bends
• Keep PCs before elbow
R
i i
PC
t ib t
littl
Example Interpretation:
• PC1: Large eigenvalue (most variance)
• PC2-3: Moderate decrease
• PC4+: Flat (noise level)
• Decision: Keep 3-4 components
Kaiser Criterion
Alternative rule: Keep components with eigenvalue
&gt; 1 (for correlation matrix).
26

</div>
</div>
<div class="page">
<div class="page-number">Page 32 of 50</div>
<div class="page-content">
Explained Variance Ratio
Individual Explained Variance:
For component i:
EVRi =
λi
Pd
j=1 λj
Cumulative Explained Variance:
For first k components:
CEVk =
Pk
i=1 λi
Pd
j=1 λj
Selection Criteria:
• Fixed threshold: CEVk ≥0.95 (95%)
• Fixed number: k = 10 components
• Elbow method: Visual inspection
• Cross-validation: Best downstream performance
Common Choice
Keep components explaining 90-95% of total
variance.
Example Calculation:
Eigenvalues: [5.2, 2.1, 0.8, 0.3, 0.1]
Total: P λi = 8.5
PC
EVR
CEV
1
61.2%
61.2%
2
24.7%
85.9%
3
9.4%
95.3%
4
3.5%
98.8%
5
1.2%
100.0%
Decision:
• For 85% variance: Keep 2 PCs
• For 95% variance: Keep 3 PCs
• For 99% variance: Keep 4 PCs
Trade-off
More components: Better reconstruction, less
compression
27

</div>
</div>
<div class="page">
<div class="page-number">Page 33 of 50</div>
<div class="page-content">
Reconstruction Error
Definition
Reconstruction error measures information lost
when using only k &lt; d principal components.
Formula:
Ek = ∥X −ˆXk∥2
F =
d
X
i=k+1
λi
Selection Strategy:
Set error tolerance ϵ:
Ek ≤ϵ · ∥X∥2
F
Choose smallest k satisfying constraint.
Example:
• Total variance: 100
28

</div>
</div>
<div class="page">
<div class="page-number">Page 34 of 50</div>
<div class="page-content">
Cross-Validation for Component Selection
Task-Specific Selection:
When PCA is used for downstream task:
1. Apply PCA with different k values
2. Train model on reduced data
3. Evaluate on validation set
4. Choose k with best performance
Procedure:
1: Split data: Train/Validation
2: for k = 1 to kmax do
3:
Fit PCA on training set
4:
Transform train &amp; validation
5:
Train classifier/regressor
6:
Evaluate performance
7: end for
8: Select k with best validation score
Important
Fit PCA only on training data to avoid data leakage!
Example: Classification Task
k
Train Acc
Val Acc
5
0.75
0.72
10
0.82
0.79
20
0.89
0.85
50
0.95
0.84
100
0.98
0.82
Analysis:
• Best validation: k = 20
• k = 50, 100: Overfitting
• Task-specific optimum
Considerations:
• Variance explained vs task performance
• Computational cost
• Interpretability needs
• Domain constraints
Best Practice
Use cross-validation when PCA is preprocessing step
for supervised learning.
29

</div>
</div>
<div class="page">
<div class="page-number">Page 35 of 50</div>
<div class="page-content">
Component Correlation Analysis
Goal: Understand relationships between original
features and principal components.
Loading Matrix:
V = [v1, v2, . . . , vd]
Interpretation:
• vij: Contribution of feature j to PC i
• Large |vij|: Feature j important for PC i
• Sign indicates direction of contribution
• Loadings are correlations (if standardized)
Biplot:
• Visualize data and loadings together
• Arrow length: Variable importance
• Arrow direction: Correlation with PCs
Example Loading Matrix:
Feature
PC1
PC2
PC3
Height
0.82
0.15
-0.05
Weight
0.79
0.20
0.08
Age
0.12
0.91
0.10
Income
-0.05
0.08
0.95
Interpretation:
• PC1: ”Body size” (height, weight)
• PC2: ”Age” dimension
• PC3: ”Income” dimension
• Clear separation of concepts
30

</div>
</div>
<div class="page">
<div class="page-number">Page 36 of 50</div>
<div class="page-content">
Applications

</div>
</div>
<div class="page">
<div class="page-number">Page 37 of 50</div>
<div class="page-content">
Application: Image Compression
Problem: Images contain redundant information.
Can we store them more efficiently?
PCA Approach:
1. Treat each image as vector (flatten pixels)
2. Build dataset: X ∈Rn×d
3. Apply PCA: Find eigenvectors
4. Project: Z = XVk
5. Store: Z and Vk instead of X
Compression Ratio:
Original: n × d values
Compressed: n × k + k × d values
Quality vs Compression:
PCs
Compression
Quality
10
95%
Poor
50
75%
Fair
100
50%
Good
200
0%
Excellent
Trade-off
Balance between file size and visual quality. Typical
choice: 80-90% variance retained.
31

</div>
</div>
<div class="page">
<div class="page-number">Page 38 of 50</div>
<div class="page-content">
Application: Face Recognition (Eigenfaces)
Eigenfaces Method:
1. Collect face images: n images, d pixels each
2. Apply PCA: Find ”eigenfaces” (principal
components)
3. Each eigenface captures facial variation
4. Represent faces in eigenface space
5. Recognition: Nearest neighbor in PC space
Advantages:
• Dimensionality reduction: 10, 000 →100
• Fast matching in low dimensional space
Typical Eigenfaces:
• PC1: Average lighting
• PC2: Left-right contrast
• PC3: Facial expression
• PC4-10: Detailed features
• PC11+: Fine details/noise
Historical Note
Eigenfaces pioneered in 1991 by Turk and Pentland.
32

</div>
</div>
<div class="page">
<div class="page-number">Page 39 of 50</div>
<div class="page-content">
Application: Noise Filtering
Principle:
Noise typically has:
• Low variance
• High frequency
• Random direction
• Captured by small eigenvalues
Signal has:
• High variance
• Low frequency
• Structured patterns
Example: Signal Denoising
Original signal + Gaussian noise
• Noise variance: 0.1
• PCA: Keep 5 components
• SNR improvement: 15 dB
Applications:
• Image denoising
• Audio signal processing
33

</div>
</div>
<div class="page">
<div class="page-number">Page 40 of 50</div>
<div class="page-content">
Application: Data Visualization
Challenge:
Cannot visualize data in d &gt; 3 dimensions.
PCA Solution:
Project to 2D or 3D for visualization:
1. Apply PCA: Get principal components
2. Keep PC1 and PC2 (or PC1, PC2, PC3)
3. Plot data in reduced space
4. Preserves maximum variance
Interpretation:
• PC1 (x-axis): Direction of most variance
• PC2 (y-axis): Second most variance
• Distances approximately preserved
• Clusters may emerge
Example: Iris Dataset
• Original: 4 dimensions
• PCA: 2D projection
• Explained variance: 95.8%
• Clear species separation visible
Comparison with Other Methods:
Method
Linear
Global
PCA
Yes
Yes
t-SNE
No
No
UMAP
No
Local
MDS
Yes/No
Yes
Limitation
34

</div>
</div>
<div class="page">
<div class="page-number">Page 41 of 50</div>
<div class="page-content">
Application: Exploratory Data Analysis
Digits Dataset Visualization:
High-dimensional handwritten digit images (64
dimensions) projected to 2D.
Insights from PCA:
• Digit clusters visible in PC space
• Similar digits closer together
• Outliers easily identified
• Confusion patterns apparent
Practical Workflow:
1. Load dataset
2. Standardize features
3. Apply PCA (keep 2-3 PCs)
4. Visualize with scatter plot
5. Color by labels (if available)
Observations:
• Digits 0 and 1 well-separated
• Digits 3, 5, 8 overlap slightly
• Some outliers (miswritten digits)
• PC1 and PC2 capture 30% variance
Feature Engineering
PCA projections can be used as features for
downstream classification:
• Reduce 64D to 20D
• Train classifier on PC scores
• Faster training
• Often better generalization
35

</div>
</div>
<div class="page">
<div class="page-number">Page 42 of 50</div>
<div class="page-content">
Application: Feature Engineering
PCA as Preprocessing:
Use PCA-transformed features for ML models.
Benefits:
• Decorrelation: Remove multicollinearity
• Dimensionality: Reduce feature count
• Noise reduction: Filter out noisy components
• Speed: Faster model training
• Regularization: Implicit regularization effect
Pipeline:
1: Train set: Fit PCA
2: Train set: Transform with PCA
3: Test set: Transform with same PCA
4: Train classifier on PC scores
5: Evaluate on test PC scores
Example Results:
Features
Accuracy
Time
Original (1000D)
0.85
120s
PCA (100D)
0.87
15s
PCA (50D)
0.86
8s
PCA (10D)
0.79
2s
Observations:
• Sweet spot: 50-100 components
• Improved accuracy with PCA
• Much faster training
• Slight degradation with too few PCs
Use Case
High-dimensional datasets where features are
correlated (genomics, text, images).
36

</div>
</div>
<div class="page">
<div class="page-number">Page 43 of 50</div>
<div class="page-content">
Best Practices &amp; Pitfalls

</div>
</div>
<div class="page">
<div class="page-number">Page 44 of 50</div>
<div class="page-content">
Best Practices: Standardization
Critical Decision
Should you standardize features before PCA?
Standardization:
zj = xj −µj
σj
When to Standardize:
Example Impact:
Dataset: Height (cm), Weight (kg), Age (years)
Without standardization:
• Height range: 150-200
• Weight range: 50-100
• Age range: 20-70
• PC1 dominated by height
37

</div>
</div>
<div class="page">
<div class="page-number">Page 45 of 50</div>
<div class="page-content">
Computational Complexity
Standard PCA Complexity:
Operation
Complexity
Centering
O(nd)
Covariance
O(nd2)
Eigendecomp
O(d3)
SVD
O(min(nd2, n2d))
Total
O(nd2 + d3)
Memory Requirements:
• Data: O(nd)
• Covariance: O(d2)
• Eigenvectors: O(d2)
• Total: O(nd + d2)
Scalability Issues:
• Large d: Covariance matrix huge
• Large n: Memory for data matrix
Solutions for Large-Scale:
Incremental PCA
• Process mini-batches
• Memory: O(bd + d2)
• Time: O(ndk)
Randomized PCA
• Approximate top-k PCs
• Time: O(ndk)
• Much faster for k ≪d
Sparse PCA
• Exploit data sparsity
• Reduce effective dimensionality
38

</div>
</div>
<div class="page">
<div class="page-number">Page 46 of 50</div>
<div class="page-content">
Common Pitfalls and How to Avoid Them
1. Data Leakage:
Mistake
Fitting PCA on entire dataset including test set.
Correct approach:
• Fit PCA only on training set
• Transform train and test separately
• Use same transformation for both
2. Forgetting to Center:
Mistake
Applying PCA without centering data.
Why it matters:
• PCA assumes zero mean
• Results will be incorrect
• Always center first!
3. Wrong Scaling Choice:
Mistake
Not standardizing when features have different
scales.
Impact:
4. Interpreting PCs as Features:
Mistake
Assuming PCs have same meaning as original
features.
Reality:
• PCs are linear combinations
• May not have intuitive interpretation
• Use loadings for understanding
5. Assuming Linearity:
Mistake
Applying PCA to manifold data with non-linear
structure.
Solution:
• Check for non-linearity first
• Consider Kernel PCA or manifold methods
6. Ignoring Outliers:
Mistake
Not handling outliers before PCA.
Impact:
39

</div>
</div>
<div class="page">
<div class="page-number">Page 47 of 50</div>
<div class="page-content">
Interpreting PCA Results
What to Report:
1. Number of components: k chosen
2. Explained variance: Per component and
cumulative
3. Scree plot: Visualize eigenvalue decay
4. Loading matrix: Top features per PC
5. Biplot: If d is small
6. Reconstruction error: If applicable
Interpreting Loadings:
• Examine largest magnitude loadings
• Group features by sign
• Name PC based on dominant features
• Example: ”Size component”, ”Age component”
Example
PC1 with high loadings on [height, weight, BMI]:
• Interpretation: ”Body size” component
• Positive values: Larger individuals
• Negative values: Smaller individuals
Statistical Significance:
Bootstrap approach:
• Resample data multiple times
• Compute PCA on each sample
• Check stability of components
• Report confidence intervals
Permutation test:
• Randomly permute features
• Compare eigenvalues to null distribution
• Test if variance is significant
Practical Checklist:
• ✓Data centered/standardized?
• ✓Scree plot shows elbow?
• ✓Enough variance explained?
• ✓PCs interpretable?
• ✓No data leakage?
• ✓Outliers addressed?
• ✓Results validated?
40

</div>
</div>
<div class="page">
<div class="page-number">Page 48 of 50</div>
<div class="page-content">
Summary

</div>
</div>
<div class="page">
<div class="page-number">Page 49 of 50</div>
<div class="page-content">
Key Takeaways
Core Concepts:
• PCA: Linear dimensionality reduction via
variance maximization
• Principal components: Orthogonal directions of
maximum variance
• Eigendecomposition: Mathematical foundation
• SVD: Practical computation method
• Variance explained: Quantifies information
retention
Key Steps:
1. Center (and optionally standardize) data
2. Compute covariance or apply SVD
3. Extract eigenvectors/singular vectors
4. Project data onto top-k components
5. Evaluate and interpret results
Variants:
• Kernel PCA: Non-linear extension
• Sparse PCA: Interpretable loadings
• Incremental PCA: Large-scale data
• Robust PCA: Handle outliers
Applications:
• Data visualization and exploration
• Image compression and processing
• Face recognition (eigenfaces)
• Noise filtering and denoising
• Feature engineering for ML
• Dimensionality reduction
Best Practices:
• Always center data, standardize if needed
• Use scree plot and explained variance
• Avoid data leakage in train/test split
• Check for outliers and non-linearity
• Validate component selection
• Document all preprocessing choices
Limitations:
• Assumes linear relationships
• Sensitive to outliers and scaling
• Loses interpretability
• May not preserve non-linear structure
41

</div>
</div>
<div class="page">
<div class="page-number">Page 50 of 50</div>
<div class="page-content">
Further Reading and Resources
Classic Papers:
• Pearson (1901): ”On lines and planes of closest
fit”
• Hotelling (1933): ”Analysis of complex
statistical variables”
• Turk &amp; Pentland (1991): ”Eigenfaces for
recognition”
• Jolliffe (2002): ”Principal Component Analysis”
(book)
Advanced Topics:
• Independent Component Analysis (ICA)
• Non-negative Matrix Factorization (NMF)
• t-SNE and UMAP for visualization
• Autoencoders for non-linear PCA
• Gaussian Process Latent Variable Models
Software Libraries:
• scikit-learn: PCA, KernelPCA,
IncrementalPCA
• numpy/scipy: Low-level linear algebra
Related Methods:
• Linear Discriminant Analysis (LDA): Supervised
dimensionality reduction
• Factor Analysis: Assumes latent variables
• Canonical Correlation Analysis: Multi-view
learning
• Isomap: Geodesic distances
• Locally Linear Embedding: Manifold learning
When to Use Alternatives:
• Non-linear structure: Kernel PCA, manifold
methods
• Labeled data: LDA, supervised methods
• Visualization only: t-SNE, UMAP
• Interpretability: Sparse methods, NMF
• Very large data: Random projections, sketching
Next Steps
• Practice on real datasets
• Compare with other methods
• Explore kernel and sparse variants
42

</div>
</div>

</body>
</html>
