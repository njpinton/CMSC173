{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header-cell",
   "metadata": {},
   "source": [
    "# Principal Component Analysis Workshop\n",
    "## Dimensionality Reduction & Feature Extraction\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/your-repo/CMSC173/blob/main/11-PCA/notebooks/workshop.ipynb)\n",
    "\n",
    "**Course:** CMSC 173 - Machine Learning  \n",
    "**Instructor:** Noel Jeffrey Pinton  \n",
    "**Topic:** Principal Component Analysis (PCA)  \n",
    "**Duration:** 60-75 minutes  \n",
    "\n",
    "### Learning Objectives\n",
    "By the end of this workshop, you will be able to:\n",
    "1. Understand the mathematical foundations of PCA\n",
    "2. Implement PCA from scratch using eigenvalue decomposition\n",
    "3. Apply PCA for dimensionality reduction and visualization\n",
    "4. Evaluate and interpret principal components\n",
    "5. Compare PCA with other dimensionality reduction techniques\n",
    "6. Understand advanced topics like Kernel PCA and computational complexity\n",
    "\n",
    "### Table of Contents\n",
    "1. [Setup & Imports](#setup)\n",
    "2. [Part 1: Motivation](#motivation)\n",
    "3. [Part 2: Core Concepts](#core-concepts)\n",
    "4. [Part 3: Implementation](#implementation)\n",
    "5. [Part 4: Evaluation](#evaluation)\n",
    "6. [Part 5: Advanced Topics](#advanced-topics)\n",
    "7. [Student Challenge](#challenge)\n",
    "8. [Solutions](#solutions)\n",
    "9. [Summary & Next Steps](#summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-header",
   "metadata": {},
   "source": [
    "<a id='setup'></a>\n",
    "## Section 1: Setup & Imports\n",
    "\n",
    "Let's import the necessary libraries and verify our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup-imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import load_digits, load_iris, make_swiss_roll\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA as SklearnPCA\n",
    "from sklearn.manifold import TSNE\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "# Version checks\n",
    "import sys\n",
    "print(\"Environment Information:\")\n",
    "print(f\"Python version: {sys.version.split()[0]}\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"Matplotlib version: {plt.matplotlib.__version__}\")\n",
    "print(f\"Seaborn version: {sns.__version__}\")\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Setup complete! All libraries imported successfully.\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "motivation-header",
   "metadata": {},
   "source": [
    "<a id='motivation'></a>\n",
    "## Section 2: Part 1 - Motivation\n",
    "\n",
    "### Why Do We Need PCA?\n",
    "\n",
    "In modern machine learning, we often face the **curse of dimensionality**:\n",
    "- High-dimensional data is hard to visualize\n",
    "- More features mean more computational cost\n",
    "- Many features may be redundant or correlated\n",
    "- Models can overfit with too many dimensions\n",
    "\n",
    "**PCA helps us by:**\n",
    "1. Reducing dimensionality while preserving information\n",
    "2. Removing redundant/correlated features\n",
    "3. Enabling visualization of high-dimensional data\n",
    "4. Improving computational efficiency\n",
    "5. Potentially improving model performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "motivation-example",
   "metadata": {},
   "source": [
    "### Real-World Example: Handwritten Digits\n",
    "\n",
    "Let's load the digits dataset - each image is 8x8 pixels (64 dimensions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-digits",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the digits dataset\n",
    "digits = load_digits()\n",
    "X_digits = digits.data\n",
    "y_digits = digits.target\n",
    "\n",
    "print(f\"Dataset shape: {X_digits.shape}\")\n",
    "print(f\"Number of samples: {X_digits.shape[0]}\")\n",
    "print(f\"Number of features (dimensions): {X_digits.shape[1]}\")\n",
    "print(f\"Number of classes: {len(np.unique(y_digits))}\")\n",
    "print(f\"\\nEach image is {int(np.sqrt(X_digits.shape[1]))}x{int(np.sqrt(X_digits.shape[1]))} pixels\")\n",
    "\n",
    "# Visualize some digits\n",
    "fig, axes = plt.subplots(2, 5, figsize=(12, 5))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow(X_digits[i].reshape(8, 8), cmap='gray')\n",
    "    ax.set_title(f'Label: {y_digits[i]}')\n",
    "    ax.axis('off')\n",
    "plt.suptitle('Sample Handwritten Digits (64 dimensions each)', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nChallenge: How can we visualize this 64-dimensional data in 2D?\")\n",
    "print(\"Answer: PCA!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "motivation-curse",
   "metadata": {},
   "source": [
    "### The Curse of Dimensionality\n",
    "\n",
    "Let's demonstrate why high dimensions are problematic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "curse-demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "# Demonstrate volume and distance in high dimensions\n",
    "dimensions = [1, 2, 5, 10, 20, 50, 100]\n",
    "n_points = 1000\n",
    "\n",
    "avg_distances = []\n",
    "for d in dimensions:\n",
    "    # Generate random points in d-dimensional unit hypercube\n",
    "    points = np.random.rand(n_points, d)\n",
    "    # Calculate average distance to origin\n",
    "    distances = np.sqrt(np.sum(points**2, axis=1))\n",
    "    avg_distances.append(np.mean(distances))\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Plot 1: Average distance growth\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(dimensions, avg_distances, 'bo-', linewidth=2, markersize=8)\n",
    "plt.xlabel('Number of Dimensions', fontweight='bold')\n",
    "plt.ylabel('Average Distance to Origin', fontweight='bold')\n",
    "plt.title('Distance Growth with Dimensionality', fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Volume of unit sphere\n",
    "plt.subplot(1, 2, 2)\n",
    "sphere_volumes = [np.pi**(d/2) / math.gamma(d/2 + 1) for d in dimensions]\n",
    "plt.semilogy(dimensions, sphere_volumes, 'ro-', linewidth=2, markersize=8)\n",
    "plt.xlabel('Number of Dimensions', fontweight='bold')\n",
    "plt.ylabel('Volume of Unit Sphere (log scale)', fontweight='bold')\n",
    "plt.title('Sphere Volume Changes with Dimensionality', fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Key Insights:\")\n",
    "print(\"1. Points become farther apart in high dimensions\")\n",
    "print(\"2. Volume concentrates at the surface (not center) of spheres\")\n",
    "print(\"3. Data becomes sparse - more data needed to maintain density\")\n",
    "print(\"4. This is why dimensionality reduction is crucial!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "core-concepts-header",
   "metadata": {},
   "source": [
    "<a id='core-concepts'></a>\n",
    "## Section 3: Part 2 - Core Concepts\n",
    "\n",
    "### The Mathematics Behind PCA\n",
    "\n",
    "PCA finds orthogonal directions (principal components) that maximize variance.\n",
    "\n",
    "**Mathematical Foundation:**\n",
    "\n",
    "Given data matrix $\\mathbf{X} \\in \\mathbb{R}^{n \\times d}$ (n samples, d features):\n",
    "\n",
    "1. **Center the data**: $\\tilde{\\mathbf{X}} = \\mathbf{X} - \\bar{\\mathbf{X}}$\n",
    "\n",
    "2. **Compute covariance matrix**: $\\mathbf{C} = \\frac{1}{n-1}\\tilde{\\mathbf{X}}^T\\tilde{\\mathbf{X}}$\n",
    "\n",
    "3. **Eigenvalue decomposition**: $\\mathbf{C}\\mathbf{v}_i = \\lambda_i\\mathbf{v}_i$\n",
    "\n",
    "4. **Principal components**: Eigenvectors $\\mathbf{v}_i$ with largest eigenvalues $\\lambda_i$\n",
    "\n",
    "5. **Project data**: $\\mathbf{Z} = \\tilde{\\mathbf{X}}\\mathbf{W}$ where $\\mathbf{W} = [\\mathbf{v}_1, \\mathbf{v}_2, ..., \\mathbf{v}_k]$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "core-concepts-visual",
   "metadata": {},
   "source": [
    "### Step-by-Step Visualization\n",
    "\n",
    "Let's understand PCA with a simple 2D example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pca-2d-demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate correlated 2D data\n",
    "np.random.seed(42)\n",
    "mean = [0, 0]\n",
    "cov = [[3, 2.5], [2.5, 3]]  # Covariance matrix with correlation\n",
    "n_samples = 300\n",
    "X_2d = np.random.multivariate_normal(mean, cov, n_samples)\n",
    "\n",
    "# Step 1: Center the data\n",
    "X_centered = X_2d - np.mean(X_2d, axis=0)\n",
    "\n",
    "# Step 2: Compute covariance matrix\n",
    "cov_matrix = np.cov(X_centered.T)\n",
    "\n",
    "# Step 3: Eigenvalue decomposition\n",
    "eigenvalues, eigenvectors = np.linalg.eig(cov_matrix)\n",
    "\n",
    "# Sort by eigenvalue\n",
    "idx = eigenvalues.argsort()[::-1]\n",
    "eigenvalues = eigenvalues[idx]\n",
    "eigenvectors = eigenvectors[:, idx]\n",
    "\n",
    "print(\"PCA Analysis Results:\")\n",
    "print(f\"Eigenvalues: {eigenvalues}\")\n",
    "print(f\"Variance explained by PC1: {eigenvalues[0]/eigenvalues.sum()*100:.1f}%\")\n",
    "print(f\"Variance explained by PC2: {eigenvalues[1]/eigenvalues.sum()*100:.1f}%\")\n",
    "print(f\"\\nEigenvector 1 (PC1): {eigenvectors[:, 0]}\")\n",
    "print(f\"Eigenvector 2 (PC2): {eigenvectors[:, 1]}\")\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "# Plot 1: Original data\n",
    "axes[0].scatter(X_2d[:, 0], X_2d[:, 1], alpha=0.5, s=30)\n",
    "axes[0].set_xlabel('Feature 1', fontweight='bold')\n",
    "axes[0].set_ylabel('Feature 2', fontweight='bold')\n",
    "axes[0].set_title('Step 1: Original Correlated Data', fontweight='bold')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].axis('equal')\n",
    "\n",
    "# Plot 2: Centered data with principal components\n",
    "axes[1].scatter(X_centered[:, 0], X_centered[:, 1], alpha=0.5, s=30, label='Data')\n",
    "\n",
    "# Draw principal components\n",
    "scale = 3\n",
    "for i, (eigenval, eigenvec) in enumerate(zip(eigenvalues, eigenvectors.T)):\n",
    "    axes[1].arrow(0, 0, eigenvec[0]*scale*np.sqrt(eigenval), \n",
    "                  eigenvec[1]*scale*np.sqrt(eigenval),\n",
    "                  head_width=0.3, head_length=0.3, fc=f'C{i+1}', ec=f'C{i+1}',\n",
    "                  linewidth=3, label=f'PC{i+1} (\u03bb={eigenval:.2f})')\n",
    "\n",
    "axes[1].set_xlabel('Feature 1 (centered)', fontweight='bold')\n",
    "axes[1].set_ylabel('Feature 2 (centered)', fontweight='bold')\n",
    "axes[1].set_title('Step 2: Principal Components', fontweight='bold')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].axis('equal')\n",
    "\n",
    "# Plot 3: Data projected onto principal components\n",
    "X_pca = X_centered @ eigenvectors\n",
    "axes[2].scatter(X_pca[:, 0], X_pca[:, 1], alpha=0.5, s=30)\n",
    "axes[2].set_xlabel('PC1 (First Principal Component)', fontweight='bold')\n",
    "axes[2].set_ylabel('PC2 (Second Principal Component)', fontweight='bold')\n",
    "axes[2].set_title('Step 3: Transformed Data', fontweight='bold')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "axes[2].axis('equal')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey Observations:\")\n",
    "print(\"1. PC1 points in the direction of maximum variance\")\n",
    "print(\"2. PC2 is orthogonal to PC1 (90 degrees)\")\n",
    "print(\"3. In the transformed space, features are uncorrelated\")\n",
    "print(\"4. We could drop PC2 with minimal information loss!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "core-concepts-variance",
   "metadata": {},
   "source": [
    "### Understanding Variance Explained\n",
    "\n",
    "How much information does each principal component capture?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "variance-explained",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply PCA to digits dataset to see variance explained\n",
    "X_digits_scaled = StandardScaler().fit_transform(X_digits)\n",
    "\n",
    "# Compute covariance matrix and eigenvalues\n",
    "cov_matrix_digits = np.cov(X_digits_scaled.T)\n",
    "eigenvalues_digits, _ = np.linalg.eig(cov_matrix_digits)\n",
    "eigenvalues_digits = np.sort(eigenvalues_digits)[::-1]\n",
    "\n",
    "# Calculate variance explained\n",
    "variance_explained = eigenvalues_digits / eigenvalues_digits.sum()\n",
    "cumulative_variance = np.cumsum(variance_explained)\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Variance per component\n",
    "axes[0].bar(range(1, 21), variance_explained[:20], alpha=0.7, color='steelblue')\n",
    "axes[0].set_xlabel('Principal Component', fontweight='bold')\n",
    "axes[0].set_ylabel('Proportion of Variance Explained', fontweight='bold')\n",
    "axes[0].set_title('Variance Explained by Each Component (First 20)', fontweight='bold')\n",
    "axes[0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Plot 2: Cumulative variance\n",
    "axes[1].plot(range(1, len(cumulative_variance)+1), cumulative_variance, \n",
    "             'ro-', linewidth=2, markersize=4)\n",
    "axes[1].axhline(y=0.9, color='g', linestyle='--', linewidth=2, label='90% threshold')\n",
    "axes[1].axhline(y=0.95, color='orange', linestyle='--', linewidth=2, label='95% threshold')\n",
    "axes[1].set_xlabel('Number of Components', fontweight='bold')\n",
    "axes[1].set_ylabel('Cumulative Variance Explained', fontweight='bold')\n",
    "axes[1].set_title('Cumulative Variance Explained', fontweight='bold')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Find number of components for different thresholds\n",
    "n_90 = np.argmax(cumulative_variance >= 0.90) + 1\n",
    "n_95 = np.argmax(cumulative_variance >= 0.95) + 1\n",
    "n_99 = np.argmax(cumulative_variance >= 0.99) + 1\n",
    "\n",
    "print(f\"\\nDimensionality Reduction Summary:\")\n",
    "print(f\"Original dimensions: {X_digits.shape[1]}\")\n",
    "print(f\"Components for 90% variance: {n_90} ({n_90/X_digits.shape[1]*100:.1f}% of original)\")\n",
    "print(f\"Components for 95% variance: {n_95} ({n_95/X_digits.shape[1]*100:.1f}% of original)\")\n",
    "print(f\"Components for 99% variance: {n_99} ({n_99/X_digits.shape[1]*100:.1f}% of original)\")\n",
    "print(f\"\\nWe can reduce from 64 to ~{n_95} dimensions with minimal information loss!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "implementation-header",
   "metadata": {},
   "source": [
    "<a id='implementation'></a>\n",
    "## Section 4: Part 3 - Implementation from Scratch\n",
    "\n",
    "Let's implement PCA from scratch to truly understand how it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pca-class",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PCA:\n",
    "    \"\"\"\n",
    "    Principal Component Analysis implementation from scratch.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    n_components : int or float, optional\n",
    "        Number of components to keep.\n",
    "        If int: keep exactly n_components\n",
    "        If float (0 < n_components < 1): keep enough components to explain\n",
    "            this proportion of variance\n",
    "        If None: keep all components\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_components=None):\n",
    "        self.n_components = n_components\n",
    "        self.components_ = None\n",
    "        self.mean_ = None\n",
    "        self.eigenvalues_ = None\n",
    "        self.explained_variance_ = None\n",
    "        self.explained_variance_ratio_ = None\n",
    "    \n",
    "    def fit(self, X):\n",
    "        \"\"\"\n",
    "        Fit the PCA model to the data.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : ndarray of shape (n_samples, n_features)\n",
    "            Training data\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        self : object\n",
    "        \"\"\"\n",
    "        # Step 1: Center the data\n",
    "        self.mean_ = np.mean(X, axis=0)\n",
    "        X_centered = X - self.mean_\n",
    "        \n",
    "        # Step 2: Compute covariance matrix\n",
    "        n_samples = X.shape[0]\n",
    "        cov_matrix = (X_centered.T @ X_centered) / (n_samples - 1)\n",
    "        \n",
    "        # Step 3: Compute eigenvalues and eigenvectors\n",
    "        eigenvalues, eigenvectors = np.linalg.eig(cov_matrix)\n",
    "        \n",
    "        # Step 4: Sort by eigenvalue (descending)\n",
    "        idx = eigenvalues.argsort()[::-1]\n",
    "        eigenvalues = eigenvalues[idx]\n",
    "        eigenvectors = eigenvectors[:, idx]\n",
    "        \n",
    "        # Step 5: Store results\n",
    "        self.eigenvalues_ = eigenvalues\n",
    "        self.explained_variance_ = eigenvalues\n",
    "        self.explained_variance_ratio_ = eigenvalues / eigenvalues.sum()\n",
    "        \n",
    "        # Step 6: Select components based on n_components\n",
    "        if self.n_components is None:\n",
    "            self.components_ = eigenvectors\n",
    "        elif isinstance(self.n_components, int):\n",
    "            self.components_ = eigenvectors[:, :self.n_components]\n",
    "        elif isinstance(self.n_components, float) and 0 < self.n_components < 1:\n",
    "            # Select components to explain desired variance\n",
    "            cumsum = np.cumsum(self.explained_variance_ratio_)\n",
    "            n_comp = np.argmax(cumsum >= self.n_components) + 1\n",
    "            self.components_ = eigenvectors[:, :n_comp]\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        Apply dimensionality reduction to X.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : ndarray of shape (n_samples, n_features)\n",
    "            Data to transform\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        X_transformed : ndarray of shape (n_samples, n_components)\n",
    "            Transformed data\n",
    "        \"\"\"\n",
    "        # Center the data using training mean\n",
    "        X_centered = X - self.mean_\n",
    "        \n",
    "        # Project onto principal components\n",
    "        return X_centered @ self.components_\n",
    "    \n",
    "    def fit_transform(self, X):\n",
    "        \"\"\"\n",
    "        Fit the model and apply dimensionality reduction.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : ndarray of shape (n_samples, n_features)\n",
    "            Training data\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        X_transformed : ndarray of shape (n_samples, n_components)\n",
    "            Transformed data\n",
    "        \"\"\"\n",
    "        self.fit(X)\n",
    "        return self.transform(X)\n",
    "    \n",
    "    def inverse_transform(self, X_transformed):\n",
    "        \"\"\"\n",
    "        Transform data back to original space.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X_transformed : ndarray of shape (n_samples, n_components)\n",
    "            Transformed data\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        X_original : ndarray of shape (n_samples, n_features)\n",
    "            Data in original space\n",
    "        \"\"\"\n",
    "        return (X_transformed @ self.components_.T) + self.mean_\n",
    "\n",
    "print(\"PCA class implemented successfully!\")\n",
    "print(\"\\nKey methods:\")\n",
    "print(\"  - fit(X): Learn principal components from data\")\n",
    "print(\"  - transform(X): Project data onto principal components\")\n",
    "print(\"  - fit_transform(X): Fit and transform in one step\")\n",
    "print(\"  - inverse_transform(X): Reconstruct original space\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "implementation-test",
   "metadata": {},
   "source": [
    "### Testing Our Implementation\n",
    "\n",
    "Let's test our PCA implementation and compare with sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test-pca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "X_test = StandardScaler().fit_transform(X_digits)\n",
    "\n",
    "# Our implementation\n",
    "pca_ours = PCA(n_components=2)\n",
    "X_pca_ours = pca_ours.fit_transform(X_test)\n",
    "\n",
    "# Sklearn implementation\n",
    "pca_sklearn = SklearnPCA(n_components=2)\n",
    "X_pca_sklearn = pca_sklearn.fit_transform(X_test)\n",
    "\n",
    "# Compare results\n",
    "print(\"Comparison: Our Implementation vs Sklearn\\n\" + \"=\"*50)\n",
    "print(f\"\\nExplained variance ratio:\")\n",
    "print(f\"  Ours:    {pca_ours.explained_variance_ratio_[:2]}\")\n",
    "print(f\"  Sklearn: {pca_sklearn.explained_variance_ratio_}\")\n",
    "\n",
    "print(f\"\\nTotal variance explained:\")\n",
    "print(f\"  Ours:    {pca_ours.explained_variance_ratio_[:2].sum():.4f}\")\n",
    "print(f\"  Sklearn: {pca_sklearn.explained_variance_ratio_.sum():.4f}\")\n",
    "\n",
    "# Visualize both results\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Our implementation\n",
    "scatter1 = axes[0].scatter(X_pca_ours[:, 0], X_pca_ours[:, 1], \n",
    "                           c=y_digits, cmap='tab10', alpha=0.6, s=20)\n",
    "axes[0].set_xlabel('First Principal Component', fontweight='bold')\n",
    "axes[0].set_ylabel('Second Principal Component', fontweight='bold')\n",
    "axes[0].set_title('Our PCA Implementation', fontweight='bold')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "plt.colorbar(scatter1, ax=axes[0], label='Digit')\n",
    "\n",
    "# Sklearn implementation\n",
    "scatter2 = axes[1].scatter(X_pca_sklearn[:, 0], X_pca_sklearn[:, 1], \n",
    "                           c=y_digits, cmap='tab10', alpha=0.6, s=20)\n",
    "axes[1].set_xlabel('First Principal Component', fontweight='bold')\n",
    "axes[1].set_ylabel('Second Principal Component', fontweight='bold')\n",
    "axes[1].set_title('Sklearn PCA', fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "plt.colorbar(scatter2, ax=axes[1], label='Digit')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Check reconstruction error\n",
    "X_reconstructed = pca_ours.inverse_transform(X_pca_ours)\n",
    "reconstruction_error = np.mean((X_test - X_reconstructed)**2)\n",
    "\n",
    "print(f\"\\nReconstruction Error (MSE): {reconstruction_error:.6f}\")\n",
    "print(\"\\nSuccess! Our implementation matches sklearn closely.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "implementation-reconstruction",
   "metadata": {},
   "source": [
    "### Visualizing Reconstruction\n",
    "\n",
    "Let's see how well we can reconstruct images with different numbers of components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reconstruction-visual",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a sample digit\n",
    "sample_idx = 0\n",
    "sample_digit = X_test[sample_idx:sample_idx+1]\n",
    "\n",
    "# Test different numbers of components\n",
    "n_components_list = [1, 2, 5, 10, 20, 64]\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(12, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, n_comp in enumerate(n_components_list):\n",
    "    # Apply PCA with n_comp components\n",
    "    pca_temp = PCA(n_components=n_comp)\n",
    "    X_reduced = pca_temp.fit_transform(X_test)\n",
    "    X_reconstructed = pca_temp.inverse_transform(X_reduced[sample_idx:sample_idx+1])\n",
    "    \n",
    "    # Calculate reconstruction error\n",
    "    mse = np.mean((sample_digit - X_reconstructed)**2)\n",
    "    var_explained = pca_temp.explained_variance_ratio_[:n_comp].sum()\n",
    "    \n",
    "    # Display\n",
    "    axes[i].imshow(X_reconstructed.reshape(8, 8), cmap='gray')\n",
    "    axes[i].set_title(f'{n_comp} components\\nVar: {var_explained:.1%}, MSE: {mse:.4f}', \n",
    "                      fontsize=10)\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.suptitle(f'Image Reconstruction with Different Numbers of Components\\nOriginal Digit: {y_digits[sample_idx]}', \n",
    "             fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Observations:\")\n",
    "print(\"1. With just 5 components, the digit is recognizable!\")\n",
    "print(\"2. 20 components gives excellent reconstruction\")\n",
    "print(\"3. We can reduce from 64 to 20 dimensions (69% reduction) with minimal quality loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "evaluation-header",
   "metadata": {},
   "source": [
    "<a id='evaluation'></a>\n",
    "## Section 5: Part 4 - Evaluation & Performance\n",
    "\n",
    "### How do we evaluate PCA?\n",
    "\n",
    "Key metrics:\n",
    "1. **Explained variance ratio**: How much information is retained?\n",
    "2. **Reconstruction error**: How well can we reconstruct original data?\n",
    "3. **Visualization quality**: Are classes well-separated?\n",
    "4. **Downstream task performance**: Does it help our ML model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "evaluation-metrics",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive evaluation\n",
    "def evaluate_pca(X, y, n_components_range):\n",
    "    \"\"\"\n",
    "    Evaluate PCA performance across different numbers of components.\n",
    "    \"\"\"\n",
    "    results = {\n",
    "        'n_components': [],\n",
    "        'variance_explained': [],\n",
    "        'reconstruction_error': [],\n",
    "        'compression_ratio': []\n",
    "    }\n",
    "    \n",
    "    for n_comp in n_components_range:\n",
    "        pca = PCA(n_components=n_comp)\n",
    "        X_reduced = pca.fit_transform(X)\n",
    "        X_reconstructed = pca.inverse_transform(X_reduced)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        var_exp = pca.explained_variance_ratio_[:n_comp].sum()\n",
    "        recon_error = np.mean((X - X_reconstructed)**2)\n",
    "        compression = 1 - (n_comp / X.shape[1])\n",
    "        \n",
    "        results['n_components'].append(n_comp)\n",
    "        results['variance_explained'].append(var_exp)\n",
    "        results['reconstruction_error'].append(recon_error)\n",
    "        results['compression_ratio'].append(compression)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Evaluate\n",
    "n_components_range = [1, 2, 5, 10, 15, 20, 30, 40, 50, 64]\n",
    "eval_results = evaluate_pca(X_test, y_digits, n_components_range)\n",
    "\n",
    "# Visualize results\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Plot 1: Variance explained\n",
    "axes[0, 0].plot(eval_results['n_components'], eval_results['variance_explained'], \n",
    "                'bo-', linewidth=2, markersize=8)\n",
    "axes[0, 0].axhline(y=0.9, color='r', linestyle='--', label='90% threshold')\n",
    "axes[0, 0].axhline(y=0.95, color='orange', linestyle='--', label='95% threshold')\n",
    "axes[0, 0].set_xlabel('Number of Components', fontweight='bold')\n",
    "axes[0, 0].set_ylabel('Cumulative Variance Explained', fontweight='bold')\n",
    "axes[0, 0].set_title('Variance Explained vs Components', fontweight='bold')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Reconstruction error\n",
    "axes[0, 1].plot(eval_results['n_components'], eval_results['reconstruction_error'], \n",
    "                'ro-', linewidth=2, markersize=8)\n",
    "axes[0, 1].set_xlabel('Number of Components', fontweight='bold')\n",
    "axes[0, 1].set_ylabel('Reconstruction Error (MSE)', fontweight='bold')\n",
    "axes[0, 1].set_title('Reconstruction Error vs Components', fontweight='bold')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Compression ratio\n",
    "axes[1, 0].plot(eval_results['n_components'], \n",
    "                [c*100 for c in eval_results['compression_ratio']], \n",
    "                'go-', linewidth=2, markersize=8)\n",
    "axes[1, 0].set_xlabel('Number of Components', fontweight='bold')\n",
    "axes[1, 0].set_ylabel('Compression Ratio (%)', fontweight='bold')\n",
    "axes[1, 0].set_title('Data Compression vs Components', fontweight='bold')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Trade-off (variance vs compression)\n",
    "axes[1, 1].scatter([c*100 for c in eval_results['compression_ratio']], \n",
    "                   eval_results['variance_explained'],\n",
    "                   c=eval_results['n_components'], cmap='viridis', s=100)\n",
    "for i, n in enumerate(eval_results['n_components']):\n",
    "    if n in [2, 10, 20, 40]:\n",
    "        axes[1, 1].annotate(f'{n}', \n",
    "                           (eval_results['compression_ratio'][i]*100, \n",
    "                            eval_results['variance_explained'][i]),\n",
    "                           fontsize=10, fontweight='bold')\n",
    "axes[1, 1].set_xlabel('Compression Ratio (%)', fontweight='bold')\n",
    "axes[1, 1].set_ylabel('Variance Explained', fontweight='bold')\n",
    "axes[1, 1].set_title('Variance-Compression Trade-off', fontweight='bold')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "cbar = plt.colorbar(axes[1, 1].collections[0], ax=axes[1, 1])\n",
    "cbar.set_label('Number of Components', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Find optimal number of components\n",
    "idx_90 = next(i for i, v in enumerate(eval_results['variance_explained']) if v >= 0.90)\n",
    "optimal_n = eval_results['n_components'][idx_90]\n",
    "\n",
    "print(\"\\nPerformance Summary:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Optimal components (90% variance): {optimal_n}\")\n",
    "print(f\"Compression ratio: {eval_results['compression_ratio'][idx_90]*100:.1f}%\")\n",
    "print(f\"Reconstruction error: {eval_results['reconstruction_error'][idx_90]:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "evaluation-comparison",
   "metadata": {},
   "source": [
    "### Comparison with Other Dimensionality Reduction Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compare-methods",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare PCA with t-SNE\n",
    "print(\"Applying dimensionality reduction methods...\")\n",
    "print(\"(t-SNE may take a minute...)\\n\")\n",
    "\n",
    "# PCA\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_test)\n",
    "\n",
    "# t-SNE\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=30)\n",
    "X_tsne = tsne.fit_transform(X_test[:500])  # Use subset for speed\n",
    "\n",
    "# Visualize comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# PCA\n",
    "scatter1 = axes[0].scatter(X_pca[:, 0], X_pca[:, 1], \n",
    "                          c=y_digits, cmap='tab10', alpha=0.6, s=20)\n",
    "axes[0].set_xlabel('First Component', fontweight='bold')\n",
    "axes[0].set_ylabel('Second Component', fontweight='bold')\n",
    "axes[0].set_title('PCA (Linear)', fontweight='bold')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "plt.colorbar(scatter1, ax=axes[0], label='Digit')\n",
    "\n",
    "# t-SNE\n",
    "scatter2 = axes[1].scatter(X_tsne[:, 0], X_tsne[:, 1], \n",
    "                          c=y_digits[:500], cmap='tab10', alpha=0.6, s=20)\n",
    "axes[1].set_xlabel('First Component', fontweight='bold')\n",
    "axes[1].set_ylabel('Second Component', fontweight='bold')\n",
    "axes[1].set_title('t-SNE (Nonlinear)', fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "plt.colorbar(scatter2, ax=axes[1], label='Digit')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nComparison:\")\n",
    "print(\"\\nPCA (Principal Component Analysis):\")\n",
    "print(\"  + Fast and deterministic\")\n",
    "print(\"  + Linear, easy to interpret\")\n",
    "print(\"  + Can reconstruct original data\")\n",
    "print(\"  - May not capture nonlinear structure\")\n",
    "print(f\"  Variance explained: {pca.explained_variance_ratio_.sum():.1%}\")\n",
    "\n",
    "print(\"\\nt-SNE (t-Distributed Stochastic Neighbor Embedding):\")\n",
    "print(\"  + Better for visualization\")\n",
    "print(\"  + Captures local structure well\")\n",
    "print(\"  + Nonlinear\")\n",
    "print(\"  - Slower, non-deterministic\")\n",
    "print(\"  - Cannot reconstruct or transform new data\")\n",
    "print(\"  - Mainly for visualization, not preprocessing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advanced-header",
   "metadata": {},
   "source": [
    "<a id='advanced-topics'></a>\n",
    "## Section 6: Part 5 - Advanced Topics\n",
    "\n",
    "### Kernel PCA\n",
    "\n",
    "What if data has nonlinear structure? Enter **Kernel PCA**!\n",
    "\n",
    "Kernel PCA applies the kernel trick to perform PCA in a high-dimensional feature space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kernel-pca-demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import KernelPCA\n",
    "\n",
    "# Create nonlinear data (Swiss roll)\n",
    "X_swiss, color_swiss = make_swiss_roll(n_samples=1000, random_state=42)\n",
    "\n",
    "# Apply different methods\n",
    "pca_linear = PCA(n_components=2)\n",
    "X_pca_linear = pca_linear.fit_transform(X_swiss)\n",
    "\n",
    "kpca_rbf = KernelPCA(n_components=2, kernel='rbf', gamma=0.1)\n",
    "X_kpca_rbf = kpca_rbf.fit_transform(X_swiss)\n",
    "\n",
    "kpca_poly = KernelPCA(n_components=2, kernel='poly', degree=3)\n",
    "X_kpca_poly = kpca_poly.fit_transform(X_swiss)\n",
    "\n",
    "# Visualize\n",
    "fig = plt.figure(figsize=(16, 4))\n",
    "\n",
    "# Original 3D data\n",
    "ax1 = fig.add_subplot(141, projection='3d')\n",
    "ax1.scatter(X_swiss[:, 0], X_swiss[:, 1], X_swiss[:, 2], \n",
    "           c=color_swiss, cmap='viridis', s=20, alpha=0.6)\n",
    "ax1.set_title('Original Swiss Roll (3D)', fontweight='bold')\n",
    "ax1.set_xlabel('X')\n",
    "ax1.set_ylabel('Y')\n",
    "ax1.set_zlabel('Z')\n",
    "\n",
    "# Linear PCA\n",
    "ax2 = fig.add_subplot(142)\n",
    "scatter2 = ax2.scatter(X_pca_linear[:, 0], X_pca_linear[:, 1], \n",
    "                       c=color_swiss, cmap='viridis', s=20, alpha=0.6)\n",
    "ax2.set_title('Linear PCA', fontweight='bold')\n",
    "ax2.set_xlabel('PC1')\n",
    "ax2.set_ylabel('PC2')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Kernel PCA (RBF)\n",
    "ax3 = fig.add_subplot(143)\n",
    "scatter3 = ax3.scatter(X_kpca_rbf[:, 0], X_kpca_rbf[:, 1], \n",
    "                       c=color_swiss, cmap='viridis', s=20, alpha=0.6)\n",
    "ax3.set_title('Kernel PCA (RBF)', fontweight='bold')\n",
    "ax3.set_xlabel('KPC1')\n",
    "ax3.set_ylabel('KPC2')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Kernel PCA (Polynomial)\n",
    "ax4 = fig.add_subplot(144)\n",
    "scatter4 = ax4.scatter(X_kpca_poly[:, 0], X_kpca_poly[:, 1], \n",
    "                       c=color_swiss, cmap='viridis', s=20, alpha=0.6)\n",
    "ax4.set_title('Kernel PCA (Polynomial)', fontweight='bold')\n",
    "ax4.set_xlabel('KPC1')\n",
    "ax4.set_ylabel('KPC2')\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Key Insights:\")\n",
    "print(\"1. Linear PCA struggles with nonlinear manifold (swiss roll)\")\n",
    "print(\"2. Kernel PCA can 'unroll' the swiss roll better\")\n",
    "print(\"3. Different kernels capture different nonlinear patterns\")\n",
    "print(\"4. Trade-off: Kernel PCA is more expensive computationally\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advanced-complexity",
   "metadata": {},
   "source": [
    "### Computational Complexity\n",
    "\n",
    "Understanding the computational cost of PCA is crucial for large datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "complexity-analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def measure_pca_time(n_samples, n_features, n_components):\n",
    "    \"\"\"Measure time for PCA computation.\"\"\"\n",
    "    X = np.random.randn(n_samples, n_features)\n",
    "    \n",
    "    start = time.time()\n",
    "    pca = PCA(n_components=n_components)\n",
    "    pca.fit_transform(X)\n",
    "    elapsed = time.time() - start\n",
    "    \n",
    "    return elapsed\n",
    "\n",
    "# Test scaling with number of samples\n",
    "print(\"Testing computational complexity...\\n\")\n",
    "\n",
    "n_samples_list = [100, 200, 500, 1000, 2000]\n",
    "n_features = 50\n",
    "n_components = 10\n",
    "\n",
    "times_samples = []\n",
    "for n in n_samples_list:\n",
    "    t = measure_pca_time(n, n_features, n_components)\n",
    "    times_samples.append(t)\n",
    "    print(f\"n_samples={n:4d}, n_features={n_features}, time={t:.4f}s\")\n",
    "\n",
    "# Test scaling with number of features\n",
    "print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "\n",
    "n_samples = 500\n",
    "n_features_list = [10, 20, 50, 100, 200]\n",
    "\n",
    "times_features = []\n",
    "for n in n_features_list:\n",
    "    t = measure_pca_time(n_samples, n, min(10, n))\n",
    "    times_features.append(t)\n",
    "    print(f\"n_samples={n_samples}, n_features={n:3d}, time={t:.4f}s\")\n",
    "\n",
    "# Visualize complexity\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Scaling with samples\n",
    "axes[0].plot(n_samples_list, times_samples, 'bo-', linewidth=2, markersize=8)\n",
    "axes[0].set_xlabel('Number of Samples', fontweight='bold')\n",
    "axes[0].set_ylabel('Time (seconds)', fontweight='bold')\n",
    "axes[0].set_title('Time Complexity vs Number of Samples', fontweight='bold')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Scaling with features\n",
    "axes[1].plot(n_features_list, times_features, 'ro-', linewidth=2, markersize=8)\n",
    "axes[1].set_xlabel('Number of Features', fontweight='bold')\n",
    "axes[1].set_ylabel('Time (seconds)', fontweight='bold')\n",
    "axes[1].set_title('Time Complexity vs Number of Features', fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Complexity Analysis:\")\n",
    "print(\"=\"*50)\n",
    "print(\"\\nTime Complexity: O(min(n*d\u00b2, d*n\u00b2)) where:\")\n",
    "print(\"  n = number of samples\")\n",
    "print(\"  d = number of features\")\n",
    "print(\"\\nSpace Complexity: O(d\u00b2) for covariance matrix\")\n",
    "print(\"\\nPractical Tips:\")\n",
    "print(\"  \u2022 For n >> d: Compute covariance matrix (d x d)\")\n",
    "print(\"  \u2022 For d >> n: Use SVD or randomized PCA\")\n",
    "print(\"  \u2022 For very large datasets: Use incremental PCA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advanced-incremental",
   "metadata": {},
   "source": [
    "### Incremental PCA for Large Datasets\n",
    "\n",
    "When data doesn't fit in memory, use Incremental PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incremental-pca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import IncrementalPCA\n",
    "\n",
    "# Simulate large dataset in batches\n",
    "n_samples = 2000\n",
    "n_features = 50\n",
    "batch_size = 200\n",
    "n_components = 10\n",
    "\n",
    "# Generate full dataset\n",
    "X_large = np.random.randn(n_samples, n_features)\n",
    "\n",
    "# Regular PCA (requires all data at once)\n",
    "print(\"Standard PCA:\")\n",
    "start = time.time()\n",
    "pca_standard = PCA(n_components=n_components)\n",
    "X_pca_standard = pca_standard.fit_transform(X_large)\n",
    "time_standard = time.time() - start\n",
    "print(f\"  Time: {time_standard:.4f}s\")\n",
    "\n",
    "# Incremental PCA (processes data in batches)\n",
    "print(\"\\nIncremental PCA:\")\n",
    "start = time.time()\n",
    "ipca = IncrementalPCA(n_components=n_components, batch_size=batch_size)\n",
    "\n",
    "# Fit in batches\n",
    "for i in range(0, n_samples, batch_size):\n",
    "    batch = X_large[i:i+batch_size]\n",
    "    ipca.partial_fit(batch)\n",
    "\n",
    "# Transform\n",
    "X_ipca = ipca.transform(X_large)\n",
    "time_incremental = time.time() - start\n",
    "print(f\"  Time: {time_incremental:.4f}s\")\n",
    "\n",
    "# Compare explained variance\n",
    "print(\"\\nExplained Variance Comparison:\")\n",
    "print(f\"  Standard PCA:    {pca_standard.explained_variance_ratio_.sum():.4f}\")\n",
    "print(f\"  Incremental PCA: {ipca.explained_variance_ratio_.sum():.4f}\")\n",
    "\n",
    "# Compare projections\n",
    "correlation = np.corrcoef(X_pca_standard[:, 0], X_ipca[:, 0])[0, 1]\n",
    "print(f\"\\nFirst PC correlation: {abs(correlation):.4f}\")\n",
    "print(\"\\nIncremental PCA gives nearly identical results!\")\n",
    "print(\"Use it for datasets that don't fit in memory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "challenge-header",
   "metadata": {},
   "source": [
    "<a id='challenge'></a>\n",
    "## Section 7: Student Challenge (15-20 minutes)\n",
    "\n",
    "### Your Task: PCA Analysis on Iris Dataset\n",
    "\n",
    "The famous Iris dataset has 4 features. Your mission:\n",
    "\n",
    "1. Load the Iris dataset and standardize it\n",
    "2. Apply PCA to reduce to 2 components\n",
    "3. Visualize the results with proper labels\n",
    "4. Analyze which features contribute most to each PC\n",
    "5. Determine how many components are needed for 95% variance\n",
    "6. Create a biplot showing both data points and feature vectors\n",
    "\n",
    "**Starter code is provided below. Fill in the missing parts!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "challenge-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Iris dataset\n",
    "iris = load_iris()\n",
    "X_iris = iris.data\n",
    "y_iris = iris.target\n",
    "feature_names = iris.feature_names\n",
    "target_names = iris.target_names\n",
    "\n",
    "print(\"Iris Dataset Information:\")\n",
    "print(f\"Number of samples: {X_iris.shape[0]}\")\n",
    "print(f\"Number of features: {X_iris.shape[1]}\")\n",
    "print(f\"Features: {feature_names}\")\n",
    "print(f\"Classes: {target_names}\")\n",
    "print(\"\\nYour tasks are below!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "challenge-task1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK 1: Standardize the data\n",
    "# Hint: Use StandardScaler or implement yourself\n",
    "\n",
    "# YOUR CODE HERE\n",
    "X_iris_scaled = None  # Replace with your code\n",
    "\n",
    "print(\"Task 1: Standardization\")\n",
    "print(f\"Mean of scaled data: {np.mean(X_iris_scaled, axis=0)}\")\n",
    "print(f\"Std of scaled data: {np.std(X_iris_scaled, axis=0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "challenge-task2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK 2: Apply PCA to reduce to 2 components\n",
    "# Use your PCA class or sklearn's\n",
    "\n",
    "# YOUR CODE HERE\n",
    "pca_iris = None  # Create PCA instance\n",
    "X_iris_pca = None  # Fit and transform the data\n",
    "\n",
    "print(\"Task 2: PCA Transformation\")\n",
    "print(f\"Explained variance ratio: {pca_iris.explained_variance_ratio_[:2]}\")\n",
    "print(f\"Total variance explained: {pca_iris.explained_variance_ratio_[:2].sum():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "challenge-task3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK 3: Visualize the PCA results\n",
    "# Create a scatter plot with different colors for each species\n",
    "\n",
    "# YOUR CODE HERE\n",
    "plt.figure(figsize=(10, 6))\n",
    "# Create your visualization\n",
    "\n",
    "print(\"Task 3: Check your visualization above!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "challenge-task4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK 4: Analyze feature contributions to principal components\n",
    "# Which features contribute most to PC1? To PC2?\n",
    "\n",
    "# YOUR CODE HERE\n",
    "# Hint: Look at pca_iris.components_\n",
    "\n",
    "print(\"Task 4: Feature Contributions\")\n",
    "print(\"Your analysis here...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "challenge-task5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK 5: Determine components needed for 95% variance\n",
    "# Fit PCA with all 4 components and analyze cumulative variance\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "print(\"Task 5: Components for 95% Variance\")\n",
    "print(\"Your answer here...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "challenge-task6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK 6: Create a biplot\n",
    "# Show both the transformed data points AND the original feature vectors\n",
    "\n",
    "# YOUR CODE HERE\n",
    "# Hint: Plot data points as scatter, feature vectors as arrows\n",
    "\n",
    "print(\"Task 6: Check your biplot above!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "solutions-header",
   "metadata": {},
   "source": [
    "<a id='solutions'></a>\n",
    "## Section 8: Solutions\n",
    "\n",
    "<details>\n",
    "<summary><b>Click to reveal solutions (try the challenge first!)</b></summary>\n",
    "\n",
    "### Solution Code Below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "solution-task1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION - TASK 1: Standardize the data\n",
    "print(\"SOLUTION - Task 1: Standardization\\n\" + \"=\"*50)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_iris_scaled = scaler.fit_transform(X_iris)\n",
    "\n",
    "print(f\"Mean of scaled data: {np.mean(X_iris_scaled, axis=0)}\")\n",
    "print(f\"Std of scaled data: {np.std(X_iris_scaled, axis=0)}\")\n",
    "print(\"\\nStandardization ensures all features have mean=0 and std=1\")\n",
    "print(\"This is crucial for PCA to avoid bias toward large-scale features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "solution-task2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION - TASK 2: Apply PCA\n",
    "print(\"SOLUTION - Task 2: PCA Transformation\\n\" + \"=\"*50)\n",
    "\n",
    "pca_iris = PCA(n_components=2)\n",
    "X_iris_pca = pca_iris.fit_transform(X_iris_scaled)\n",
    "\n",
    "print(f\"Explained variance ratio: {pca_iris.explained_variance_ratio_[:2]}\")\n",
    "print(f\"PC1 explains: {pca_iris.explained_variance_ratio_[0]*100:.2f}% of variance\")\n",
    "print(f\"PC2 explains: {pca_iris.explained_variance_ratio_[1]*100:.2f}% of variance\")\n",
    "print(f\"Total variance explained: {pca_iris.explained_variance_ratio_[:2].sum()*100:.2f}%\")\n",
    "print(\"\\nWith just 2 components, we retain most of the information!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "solution-task3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION - TASK 3: Visualize PCA results\n",
    "print(\"SOLUTION - Task 3: Visualization\\n\" + \"=\"*50)\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "\n",
    "colors = ['red', 'blue', 'green']\n",
    "markers = ['o', 's', '^']\n",
    "\n",
    "for i, (color, marker, name) in enumerate(zip(colors, markers, target_names)):\n",
    "    mask = y_iris == i\n",
    "    plt.scatter(X_iris_pca[mask, 0], X_iris_pca[mask, 1],\n",
    "               c=color, marker=marker, label=name, \n",
    "               alpha=0.7, s=100, edgecolors='black', linewidth=0.5)\n",
    "\n",
    "plt.xlabel(f'PC1 ({pca_iris.explained_variance_ratio_[0]*100:.1f}% variance)', \n",
    "          fontweight='bold', fontsize=12)\n",
    "plt.ylabel(f'PC2 ({pca_iris.explained_variance_ratio_[1]*100:.1f}% variance)', \n",
    "          fontweight='bold', fontsize=12)\n",
    "plt.title('Iris Dataset - PCA Projection', fontweight='bold', fontsize=14)\n",
    "plt.legend(title='Species', fontsize=10)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nObservations:\")\n",
    "print(\"\u2022 Setosa (red) is clearly separated from the others\")\n",
    "print(\"\u2022 Versicolor (blue) and Virginica (green) overlap somewhat\")\n",
    "print(\"\u2022 PC1 is the main discriminator between species\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "solution-task4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION - TASK 4: Feature contributions\n",
    "print(\"SOLUTION - Task 4: Feature Contributions\\n\" + \"=\"*50)\n",
    "\n",
    "# Get the components (loadings)\n",
    "components = pca_iris.components_\n",
    "\n",
    "# Create a heatmap\n",
    "plt.figure(figsize=(10, 4))\n",
    "sns.heatmap(components, annot=True, fmt='.3f', cmap='RdBu_r',\n",
    "           xticklabels=feature_names, yticklabels=['PC1', 'PC2'],\n",
    "           center=0, vmin=-1, vmax=1, cbar_kws={'label': 'Loading'})\n",
    "plt.title('PCA Component Loadings (Feature Contributions)', fontweight='bold', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Detailed analysis\n",
    "print(\"\\nPC1 Contributions:\")\n",
    "for i, feature in enumerate(feature_names):\n",
    "    print(f\"  {feature:25s}: {components[0, i]:+.3f}\")\n",
    "\n",
    "print(\"\\nPC2 Contributions:\")\n",
    "for i, feature in enumerate(feature_names):\n",
    "    print(f\"  {feature:25s}: {components[1, i]:+.3f}\")\n",
    "\n",
    "print(\"\\nKey Insights:\")\n",
    "print(\"\u2022 PC1: All features contribute positively (especially petal features)\")\n",
    "print(\"  \u2192 PC1 represents overall flower size\")\n",
    "print(\"\u2022 PC2: Sepal vs petal contrast (sepal +, petal -)\")\n",
    "print(\"  \u2192 PC2 represents sepal-petal size balance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "solution-task5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION - TASK 5: Components for 95% variance\n",
    "print(\"SOLUTION - Task 5: Components for 95% Variance\\n\" + \"=\"*50)\n",
    "\n",
    "# Fit PCA with all components\n",
    "pca_full = PCA(n_components=4)\n",
    "pca_full.fit(X_iris_scaled)\n",
    "\n",
    "# Calculate cumulative variance\n",
    "cumulative_variance = np.cumsum(pca_full.explained_variance_ratio_)\n",
    "\n",
    "# Find number of components for 95%\n",
    "n_components_95 = np.argmax(cumulative_variance >= 0.95) + 1\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(range(1, 5), pca_full.explained_variance_ratio_, \n",
    "       alpha=0.6, label='Individual', color='steelblue')\n",
    "plt.step(range(1, 5), cumulative_variance, where='mid',\n",
    "        label='Cumulative', color='red', linewidth=2)\n",
    "plt.axhline(y=0.95, color='green', linestyle='--', \n",
    "           linewidth=2, label='95% threshold')\n",
    "plt.xlabel('Principal Component', fontweight='bold')\n",
    "plt.ylabel('Variance Explained', fontweight='bold')\n",
    "plt.title('Variance Explained by Each Component', fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "plt.xticks([1, 2, 3, 4])\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nVariance by component:\")\n",
    "for i in range(4):\n",
    "    print(f\"  PC{i+1}: {pca_full.explained_variance_ratio_[i]*100:.2f}% \"\n",
    "          f\"(cumulative: {cumulative_variance[i]*100:.2f}%)\")\n",
    "\n",
    "print(f\"\\nAnswer: {n_components_95} component(s) needed for 95% variance\")\n",
    "print(f\"With {n_components_95} component(s), we explain {cumulative_variance[n_components_95-1]*100:.2f}% of variance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "solution-task6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION - TASK 6: Biplot\n",
    "print(\"SOLUTION - Task 6: Biplot\\n\" + \"=\"*50)\n",
    "\n",
    "def biplot(X_pca, pca, feature_names, y, target_names):\n",
    "    \"\"\"\n",
    "    Create a biplot showing both data points and feature vectors.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Plot data points\n",
    "    colors = ['red', 'blue', 'green']\n",
    "    markers = ['o', 's', '^']\n",
    "    \n",
    "    for i, (color, marker, name) in enumerate(zip(colors, markers, target_names)):\n",
    "        mask = y == i\n",
    "        plt.scatter(X_pca[mask, 0], X_pca[mask, 1],\n",
    "                   c=color, marker=marker, label=name,\n",
    "                   alpha=0.6, s=80, edgecolors='black', linewidth=0.5)\n",
    "    \n",
    "    # Plot feature vectors\n",
    "    scale = 3.5  # Scale factor for visibility\n",
    "    for i, feature in enumerate(feature_names):\n",
    "        arrow_x = pca.components_[0, i] * scale\n",
    "        arrow_y = pca.components_[1, i] * scale\n",
    "        \n",
    "        plt.arrow(0, 0, arrow_x, arrow_y,\n",
    "                 head_width=0.15, head_length=0.15,\n",
    "                 fc='orange', ec='darkorange', linewidth=2.5, alpha=0.8)\n",
    "        \n",
    "        # Add feature name at arrow tip\n",
    "        plt.text(arrow_x * 1.15, arrow_y * 1.15, feature,\n",
    "                fontsize=11, fontweight='bold',\n",
    "                ha='center', va='center',\n",
    "                bbox=dict(boxstyle='round,pad=0.3', facecolor='yellow', alpha=0.7))\n",
    "    \n",
    "    plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]*100:.1f}% variance)',\n",
    "              fontweight='bold', fontsize=12)\n",
    "    plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]*100:.1f}% variance)',\n",
    "              fontweight='bold', fontsize=12)\n",
    "    plt.title('PCA Biplot: Data Points and Feature Vectors',\n",
    "             fontweight='bold', fontsize=14)\n",
    "    plt.legend(title='Species', loc='best', fontsize=10)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.axhline(y=0, color='k', linewidth=0.5)\n",
    "    plt.axvline(x=0, color='k', linewidth=0.5)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Create the biplot\n",
    "biplot(X_iris_pca, pca_iris, feature_names, y_iris, target_names)\n",
    "\n",
    "print(\"\\nBiplot Interpretation:\")\n",
    "print(\"\u2022 Orange arrows = original features projected onto PC1-PC2 plane\")\n",
    "print(\"\u2022 Arrow length = importance of feature in that PC direction\")\n",
    "print(\"\u2022 Arrow angle = correlation between features\")\n",
    "print(\"  - Small angle = positive correlation\")\n",
    "print(\"  - 90 degrees = uncorrelated\")\n",
    "print(\"  - 180 degrees = negative correlation\")\n",
    "print(\"\\n\u2022 Petal features point right \u2192 contribute positively to PC1\")\n",
    "print(\"\u2022 Sepal width points up \u2192 contributes positively to PC2\")\n",
    "print(\"\u2022 Petal and sepal features are correlated (small angles between them)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "solutions-footer",
   "metadata": {},
   "source": [
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary-header",
   "metadata": {},
   "source": [
    "<a id='summary'></a>\n",
    "## Section 9: Summary & Next Steps\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "#### 1. What is PCA?\n",
    "- **Unsupervised** dimensionality reduction technique\n",
    "- Finds **orthogonal** directions of maximum variance\n",
    "- Based on **eigenvalue decomposition** of covariance matrix\n",
    "- **Linear** transformation of data\n",
    "\n",
    "#### 2. When to Use PCA?\n",
    "**Use PCA when:**\n",
    "- You have high-dimensional data (curse of dimensionality)\n",
    "- Features are correlated/redundant\n",
    "- You need to visualize high-dimensional data\n",
    "- Computational efficiency is important\n",
    "- You want interpretable components\n",
    "\n",
    "**Don't use PCA when:**\n",
    "- Features are already uncorrelated\n",
    "- You need to preserve exact values\n",
    "- Data has strong nonlinear structure (use Kernel PCA instead)\n",
    "- Feature interpretability is crucial (consider feature selection instead)\n",
    "\n",
    "#### 3. PCA Workflow\n",
    "1. **Standardize** data (mean=0, std=1)\n",
    "2. **Compute** covariance matrix\n",
    "3. **Find** eigenvalues and eigenvectors\n",
    "4. **Sort** by eigenvalue (descending)\n",
    "5. **Select** top k components\n",
    "6. **Project** data onto selected components\n",
    "\n",
    "#### 4. Evaluation Metrics\n",
    "- **Explained variance ratio**: Information retained\n",
    "- **Reconstruction error**: Quality of reconstruction\n",
    "- **Scree plot**: Choose number of components\n",
    "- **Downstream performance**: Does it help your ML task?\n",
    "\n",
    "#### 5. Advanced Techniques\n",
    "- **Kernel PCA**: For nonlinear patterns\n",
    "- **Incremental PCA**: For data that doesn't fit in memory\n",
    "- **Sparse PCA**: For sparse loadings\n",
    "- **Randomized PCA**: For faster computation on large datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary-practical",
   "metadata": {},
   "source": [
    "### Practical Guidelines\n",
    "\n",
    "#### Choosing Number of Components\n",
    "\n",
    "**Rule of thumb:**\n",
    "- **Visualization**: 2-3 components\n",
    "- **Preprocessing**: Keep 90-95% variance\n",
    "- **Compression**: Balance variance vs. compression ratio\n",
    "- **Exploratory analysis**: Use scree plot\n",
    "\n",
    "#### Common Pitfalls to Avoid\n",
    "\n",
    "1. **Not standardizing data first**\n",
    "   - Features with larger scales will dominate\n",
    "   - Always standardize unless you have good reason not to\n",
    "\n",
    "2. **Applying PCA to categorical features**\n",
    "   - PCA assumes continuous data\n",
    "   - One-hot encode first, or use correspondence analysis\n",
    "\n",
    "3. **Using PCA for feature selection**\n",
    "   - PCA creates new features (combinations of originals)\n",
    "   - For feature selection, use L1 regularization or other methods\n",
    "\n",
    "4. **Forgetting to apply same transformation to test data**\n",
    "   - Use `fit()` on training data\n",
    "   - Use `transform()` on test data (don't `fit()` again!)\n",
    "\n",
    "5. **Over-interpreting principal components**\n",
    "   - PCs are mathematical constructs, not always meaningful\n",
    "   - Be cautious with causal interpretations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary-comparison",
   "metadata": {},
   "source": [
    "### Comparison with Other Techniques\n",
    "\n",
    "| Method | Type | Preserves | Speed | Use Case |\n",
    "|--------|------|-----------|-------|----------|\n",
    "| **PCA** | Linear | Global structure | Fast | General dimensionality reduction |\n",
    "| **t-SNE** | Nonlinear | Local structure | Slow | Visualization only |\n",
    "| **UMAP** | Nonlinear | Local + global | Medium | Visualization + preprocessing |\n",
    "| **Kernel PCA** | Nonlinear | Depends on kernel | Medium | Nonlinear patterns |\n",
    "| **Autoencoders** | Nonlinear | Learned | Slow | Complex patterns, large data |\n",
    "| **LDA** | Linear | Class separation | Fast | Supervised dim. reduction |\n",
    "| **ICA** | Linear | Independence | Medium | Signal separation |\n",
    "| **NMF** | Linear | Non-negativity | Medium | Parts-based representation |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary-resources",
   "metadata": {},
   "source": [
    "### Next Steps\n",
    "\n",
    "#### Further Learning\n",
    "\n",
    "**Topics to Explore:**\n",
    "1. **Probabilistic PCA (PPCA)**: Probabilistic framework for PCA\n",
    "2. **Factor Analysis**: Related technique for latent variable modeling\n",
    "3. **Independent Component Analysis (ICA)**: For signal separation\n",
    "4. **Autoencoders**: Neural network-based dimensionality reduction\n",
    "5. **Manifold learning**: t-SNE, UMAP, Isomap, LLE\n",
    "\n",
    "**Resources:**\n",
    "- **Books**: \n",
    "  - \"Pattern Recognition and Machine Learning\" by Bishop (Chapter 12)\n",
    "  - \"The Elements of Statistical Learning\" by Hastie et al. (Chapter 14)\n",
    "- **Papers**:\n",
    "  - Jolliffe & Cadima (2016): \"Principal Component Analysis: A Review\"\n",
    "- **Online**:\n",
    "  - Scikit-learn documentation on decomposition methods\n",
    "  - StatQuest videos on PCA\n",
    "\n",
    "#### Practice Exercises\n",
    "\n",
    "1. **Apply PCA to image data** (MNIST, CIFAR-10)\n",
    "   - Visualize principal components as images\n",
    "   - Reconstruct images with different numbers of components\n",
    "\n",
    "2. **Face recognition with Eigenfaces**\n",
    "   - Apply PCA to face images\n",
    "   - Build a face recognition system\n",
    "\n",
    "3. **Compare PCA with other methods**\n",
    "   - Try t-SNE, UMAP on the same dataset\n",
    "   - Compare visualization quality\n",
    "\n",
    "4. **PCA for preprocessing**\n",
    "   - Train classifiers with/without PCA\n",
    "   - Compare accuracy and training time\n",
    "\n",
    "5. **Kernel PCA exploration**\n",
    "   - Test different kernels (RBF, polynomial, sigmoid)\n",
    "   - Apply to nonlinear datasets\n",
    "\n",
    "---\n",
    "\n",
    "### Congratulations!\n",
    "\n",
    "You've completed the Principal Component Analysis workshop. You now have:\n",
    "\n",
    "- Deep understanding of PCA mathematics and intuition\n",
    "- Hands-on experience implementing PCA from scratch\n",
    "- Skills to apply and evaluate PCA on real datasets\n",
    "- Knowledge of advanced topics and when to use them\n",
    "- Practical guidelines for using PCA effectively\n",
    "\n",
    "**Keep exploring and applying PCA to your own projects!**\n",
    "\n",
    "---\n",
    "\n",
    "*Workshop created for CMSC 173 - Machine Learning*  \n",
    "*Instructor: Noel Jeffrey Pinton*  \n",
    "*University of the Philippines - Cebu*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}