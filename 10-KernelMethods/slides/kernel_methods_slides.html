<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>kernel_methods_slides</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            max-width: 1200px;
            margin: 0 auto;
            padding: 20px;
            background-color: #f5f5f5;
        }
        .page {
            background-color: white;
            margin: 20px 0;
            padding: 40px;
            box-shadow: 0 2px 8px rgba(0,0,0,0.1);
            border-radius: 4px;
            min-height: 400px;
        }
        .page-number {
            color: #666;
            font-size: 14px;
            margin-bottom: 20px;
            border-bottom: 1px solid #ddd;
            padding-bottom: 10px;
        }
        h1, h2, h3 {
            color: #333;
        }
        .page-content {
            line-height: 1.8;
            white-space: pre-wrap;
            font-family: 'Courier New', monospace;
            font-size: 12pt;
        }
        .note {
            color: #666;
            font-style: italic;
            margin: 10px 0;
            padding: 10px;
            background-color: #f9f9f9;
            border-left: 3px solid #ccc;
        }
    </style>
</head>
<body>
<div class="note">
    This is a text-only version of the slides. For the full visual experience with images and formatting, please refer to the PDF version.
</div>

<div class="page">
<div class="page-number">Page 1 of 52</div>
<div class="page-content">
Kernel Methods
CMSC 173 - Machine Learning
Noel Jeffrey Pinton
October 3, 2025
Department of Computer Science
University of the Philippines - Cebu
1

</div>
</div>
<div class="page">
<div class="page-number">Page 2 of 52</div>
<div class="page-content">
Outline
Introduction &amp; Motivation
Historical Context: From Perceptron to SVM
Support Vector Machines
Large-Margin Classifiers
Quadratic Optimization Problem
Nonlinear SVM using Kernels
Mercer’s Theorem
Multiple Kernel Learning
Multi-class Classification
Kernel Methods for Regression
Support Vector Regression
Kernel Ridge Regression
Parametric vs Non-parametric Models
2

</div>
</div>
<div class="page">
<div class="page-number">Page 3 of 52</div>
<div class="page-content">
Review: Supervised Learning Framework
Supervised Learning: Learning from Labeled Examples
Core Concept
Given labeled training data, learn f : X →Y.
Regression Tasks
• Continuous output
Classification Tasks
• Discrete labels
3

</div>
</div>
<div class="page">
<div class="page-number">Page 4 of 52</div>
<div class="page-content">
Comparison of Supervised Learning Methods
Method
Model
Loss Function
Linear Regression
y = wT x + b
PN
i=1(yi −wT xi −b)2
Sum-of-squares loss
Logistic Regression
P(y = 1|x) = σ(wT x + b)
−PN
i=1 yi log(σ(wT xi + b))
σ(z) =
1
1+e−z
+(1 −yi ) log(1 −σ(wT xi + b))
Cross-entropy loss
Support Vector Machine
y = sign(wT x + b)
1
2 ∥w∥2 + C P
i max(0, 1 −yi (wT xi + b))
Hinge loss + L2 regularization
Today’s Focus
We’ll explore how Support Vector Machines use the kernel trick to solve non-linear problems while maintaining computational efficiency.
4

</div>
</div>
<div class="page">
<div class="page-number">Page 5 of 52</div>
<div class="page-content">
Introduction &amp; Motivation

</div>
</div>
<div class="page">
<div class="page-number">Page 6 of 52</div>
<div class="page-content">
What are Kernel Methods?
Definition
Kernel methods are a class of algorithms that use
kernel functions to operate in high-dimensional
feature spaces without explicitly computing the
coordinates in that space.
Key Idea:
• Transform data to higher dimensions where it
becomes linearly separable
• Use kernel trick to avoid explicit transformation
• Compute inner products efficiently in feature
space
Why Kernel Methods?
Many real-world problems are not linearly separable
in their original feature space.
Common Applications:
• Classification: Support Vector Machines
• Regression: Support Vector Regression
• Dimensionality Reduction: Kernel PCA
• Clustering: Kernel K-means
Advantages:
• Handle non-linear relationships
• Computational efficiency via kernel trick
• Strong theoretical foundations
• Flexible and powerful
Examples:
• Image classification
• Text analysis
• Bioinformatics
• Time series analysis
5

</div>
</div>
<div class="page">
<div class="page-number">Page 7 of 52</div>
<div class="page-content">
Linear vs Non-linear Separability
Linearly Separable Data
Characteristics:
• Classes form distinct clusters
• Linear boundary separates perfectly
• Decision rule: wT x + b = 0
Advantages:
• Simple and interpretable
Non-linearly Separable Data
Characteristics:
• Complex data patterns (e.g., concentric circles)
• No linear boundary can separate
• Requires non-linear decision boundary
Solution: Kernel Methods
• Transform to higher dimensions
6

</div>
</div>
<div class="page">
<div class="page-number">Page 8 of 52</div>
<div class="page-content">
Historical Context: From Perceptron
to SVM

</div>
</div>
<div class="page">
<div class="page-number">Page 9 of 52</div>
<div class="page-content">
Rosenblatt’s Perceptron (1957)
The Perceptron Algorithm
Frank Rosenblatt introduced the first learning
algorithm for binary classification in 1957.
Perceptron Model:
f (x) = sign(wT x + b)
(1)
Output =
(
+1
if wT x + b ≥0
−1
if wT x + b &lt; 0
(2)
Learning Rule:
w(t+1) = w(t) + η · yi · xi
(3)
b(t+1) = b(t) + η · yi
(4)
Update only when misclassified
Perceptron Convergence Theorem
If data is linearly separable, the perceptron
algorithm will converge to a solution in finite steps.
Historical Impact:
• 1957: Perceptron introduced
• 1969: Limitations exposed (XOR problem)
• 1980s-1990s: SVM development
• Key insight: Maximum margin principle
Motivation for SVMs:
• Address perceptron limitations
• Better generalization bounds
• Handle non-linearly separable data
7

</div>
</div>
<div class="page">
<div class="page-number">Page 10 of 52</div>
<div class="page-content">
Perceptron vs SVM: A Visual Comparison
Perceptron Approach
• Goal: Find any separating hyperplane
• Method: Iterative weight updates
• Result: Multiple valid solutions
• Issue: No optimality criterion
SVM Approach
• Goal: Find optimal separating hyperplane
• Method: Maximize margin width
• Result: Unique optimal solution
• Benefit: Better generalization
Key Insight
SVMs choose the hyperplane that maximizes the distance to the nearest data points (support vectors),
8

</div>
</div>
<div class="page">
<div class="page-number">Page 11 of 52</div>
<div class="page-content">
From Linear to Non-linear: Evolution of Ideas
Timeline of Development:
• 1957: Rosenblatt’s Perceptron
• 1969: Minsky &amp; Papert limitations
• 1979: Least squares SVM ideas
• 1992: Boser, Guyon, Vapnik kernel trick
• 1995: Cortes &amp; Vapnik soft margin
• 1996: Sch¨olkopf kernel methods
Minsky &amp; Papert (1969)
Showed that perceptrons cannot solve non-linearly
separable problems like XOR, leading to the ”AI
winter.”
The Kernel Revolution
The kernel trick (1992) revived interest by enabling
non-linear classification while maintaining
computational efficiency.
The XOR Problem:
Problem Statement
• Input: (0, 0) →0, (0, 1) →1
• Input: (1, 0) →1, (1, 1) →0
• No linear separator exists
Kernel Solution: Transform to 3D space:
9

</div>
</div>
<div class="page">
<div class="page-number">Page 12 of 52</div>
<div class="page-content">
Support Vector Machines

</div>
</div>
<div class="page">
<div class="page-number">Page 13 of 52</div>
<div class="page-content">
Support Vector Machines: Overview
Core Concept
SVM finds the optimal hyperplane that separates
classes with the maximum margin.
Key Components:
• Decision boundary: Hyperplane separating
classes
• Margin: Distance between boundary and closest
points
• Support vectors: Points defining the margin
Mathematical Formulation:
Hyperplane:
wT x + b = 0
(5)
Margin:
2
∥w∥
(6)
Goal
Maximize margin while correctly classifying all
training points.
Why Maximum Margin?
• Generalization: Better performance on unseen
data
• Robustness: Less sensitive to noise
• Uniqueness: Single optimal solution
10

</div>
</div>
<div class="page">
<div class="page-number">Page 14 of 52</div>
<div class="page-content">
Large-Margin Classifiers

</div>
</div>
<div class="page">
<div class="page-number">Page 15 of 52</div>
<div class="page-content">
Geometric Interpretation of Margin
Margin Definition:
For a hyperplane wT x + b = 0:
Distance from point xi to hyperplane:
(7)
di = |wT xi + b|
∥w∥
(8)
Margin Width:
Margin = min
i
di =
1
∥w∥
(9)
Canonical Form
Scale w and b so that for the closest points:
wT xi + b = ±1
Then margin becomes
2
∥w∥.
Visual Interpretation:
Key Elements
• Decision boundary: wT x + b = 0
• Margin boundaries: wT x + b = ±1
• Support vectors: Closest points
• Margin width:
2
∥w∥
11

</div>
</div>
<div class="page">
<div class="page-number">Page 16 of 52</div>
<div class="page-content">
Quadratic Optimization Problem

</div>
</div>
<div class="page">
<div class="page-number">Page 17 of 52</div>
<div class="page-content">
SVM Optimization: Primal Problem
Hard Margin SVM:
Primal Problem
min
w,b
1
2 ∥w∥2
(10)
subject to
yi(wT xi + b) ≥1,
i = 1, . . . , n
(11)
Interpretation:
• Objective: Minimize ∥w∥2 ⇒Maximize margin
2
∥w∥
• Constraints: All points correctly classified with
margin ≥1
Problem Type:
• Quadratic objective function
• Linear constraints
• Convex optimization problem
• Unique global optimum
Soft Margin SVM:
Primal Problem with Slack Variables
min
w,b,ξ
1
2 ∥w∥2 + C
n
X
i=1
ξi
(12)
subject to
yi(wT xi + b) ≥1 −ξi
(13)
ξi ≥0,
i = 1, . . . , n
(14)
Slack Variables ξi:
• ξi = 0: Point correctly classified with margin ≥1
• 0 &lt; ξi &lt; 1: Point correctly classified but within
margin
• ξi ≥1: Point misclassified
Regularization Parameter C:
• Large C: Penalty for violations (hard margin)
• Small C: Allow more violations (soft margin)
• Controls bias-variance tradeoff
12

</div>
</div>
<div class="page">
<div class="page-number">Page 18 of 52</div>
<div class="page-content">
SVM Optimization: Dual Problem
Lagrangian Formulation:
Lagrangian
L = 1
2 ∥w∥2 + C
n
X
i=1
ξi
(15)
−
n
X
i=1
αi[yi(wT xi + b) −1 + ξi]
(16)
−
n
X
i=1
µiξi
(17)
KKT Conditions:
∂L
∂w = 0 ⇒w =
n
X
i=1
αiyixi
(18)
∂L
∂b = 0 ⇒
n
X
i=1
αiyi = 0
(19)
∂L
∂ξi
= 0 ⇒αi + µi = C
(20)
Dual Problem:
Dual Formulation
max
α
n
X
i=1
αi −1
2
n
X
i,j=1
αiαjyiyjxT
i xj
(21)
subject to
n
X
i=1
αiyi = 0
(22)
0 ≤αi ≤C,
i = 1, . . . , n
(23)
Key Insights:
• Only depends on inner products xT
i xj
• Sparse solution: many αi = 0
• Support vectors: αi &gt; 0
Kernel Trick Preview
Replace xT
i xj with K(xi, xj) to work in higher
dimensions!
13

</div>
</div>
<div class="page">
<div class="page-number">Page 19 of 52</div>
<div class="page-content">
Hard vs Soft Margin SVMs
Hard Margin (C = 1000)
• No training errors allowed
• Requires linearly separable data
• May overfit to training data
• Complex decision boundary
Soft Margin (C = 0.1)
• Allows some training errors
• Works with non-separable data
• Better generalization
• Smoother decision boundary
14

</div>
</div>
<div class="page">
<div class="page-number">Page 20 of 52</div>
<div class="page-content">
Nonlinear SVM using Kernels

</div>
</div>
<div class="page">
<div class="page-number">Page 21 of 52</div>
<div class="page-content">
The Kernel Trick
Feature Mapping:
Transform input space X to feature space H:
ϕ : X →H
Example: Polynomial Features
x = (x1, x2)
(24)
ϕ(x) = (1, x1, x2, x2
1 , x2
2 ,
√
2x1x2)
(25)
Kernel Function
Instead of computing ϕ(xi)T ϕ(xj) explicitly:
K(xi, xj) = ϕ(xi)T ϕ(xj)
Key Insight
We can compute inner products in high-dimensional
space without explicitly mapping to that space!
Computational Advantage:
Without Kernel Trick:
• Map: O(d′) where d′ is feature space dimension
• Inner product: O(d′)
• Total: O(d′) per pair
With Kernel Trick:
• Direct kernel computation: O(d) where d is
input dimension
• No explicit mapping needed
• Total: O(d) per pair
Example: Polynomial Kernel
K(x, x′) = (xT x′ + 1)2
(26)
= 1 + 2xT x′ + (xT x′)2
(27)
This implicitly computes inner product in a
(d + 1)(d + 2)/2 dimensional space using only O(d)
operations!
15

</div>
</div>
<div class="page">
<div class="page-number">Page 22 of 52</div>
<div class="page-content">
Kernel Trick Visualization
Original Space
Non-linearly separable data in 2D
cannot be separated by a straight
line.
Transformed Space
Data becomes linearly separable in
3D after polynomial
transformation
ϕ(x) = (x1, x2, x2
1 + x2
2 ).
Kernel Result
RBF kernel achieves non-linear
separation directly in original 2D
space.
16

</div>
</div>
<div class="page">
<div class="page-number">Page 23 of 52</div>
<div class="page-content">
Common Kernel Functions
17

</div>
</div>
<div class="page">
<div class="page-number">Page 24 of 52</div>
<div class="page-content">
Common Kernel Functions: Definitions
Linear Kernel
K(x, x′) = xT x′
• Equivalent to no transformation
• Fastest computation
• Good baseline
Polynomial Kernel
K(x, x′) = (γxT x′ + r)d
• Captures interactions to degree d
• Parameters: γ, r, d
RBF (Gaussian) Kernel
K(x, x′) = exp
 −γ∥x −x′∥2
• Most popular kernel
• Infinite-dimensional feature space
• Smooth, localized similarity
Sigmoid Kernel
K(x, x′) = tanh(γxT x′ + r)
• Neural network inspired
• Less commonly used
18

</div>
</div>
<div class="page">
<div class="page-number">Page 25 of 52</div>
<div class="page-content">
Comparing Different Kernels
19

</div>
</div>
<div class="page">
<div class="page-number">Page 26 of 52</div>
<div class="page-content">
RBF Kernel Parameter Effects
20

</div>
</div>
<div class="page">
<div class="page-number">Page 27 of 52</div>
<div class="page-content">
Mercer’s Theorem

</div>
</div>
<div class="page">
<div class="page-number">Page 28 of 52</div>
<div class="page-content">
Mercer’s Theorem: Valid Kernels
Mercer’s Theorem
A function K(x, x′) is a valid kernel (corresponds to
an inner product in some feature space) if and only
if for any finite set of points {x1, . . . , xn}, the kernel
matrix K is positive semi-definite.
Kernel Matrix:
Kij = K(xi, xj)
Positive Semi-definite:
K ⪰0 ⇐⇒
X
i,j
cicjK(xi, xj) ≥0
for all real numbers c1, . . . , cn.
Practical Implication
Not every function can be used as a kernel! Only
those satisfying Mercer’s condition.
Properties of Valid Kernels:
• Symmetry: K(x, x′) = K(x′, x)
• Positive semi-definiteness: Kernel matrix K ⪰0
Constructing New Kernels:
If K1 and K2 are valid kernels, then:
K(x, x′) = K1(x, x′) + K2(x, x′)
(28)
K(x, x′) = c · K1(x, x′),
c &gt; 0
(29)
K(x, x′) = K1(x, x′) · K2(x, x′)
(30)
K(x, x′) = exp(K1(x, x′))
(31)
K(x, x′) = f (x)K1(x, x′)f (x′)
(32)
Domain-Specific Kernels:
• String kernels for text
• Graph kernels for networks
• Tree kernels for structured data
21

</div>
</div>
<div class="page">
<div class="page-number">Page 29 of 52</div>
<div class="page-content">
Examples of Valid and Invalid Kernels
Valid Kernels:
Linear
K(x, x′) = xT x′
Polynomial
K(x, x′) = (xT x′ + 1)d,
d ≥1
RBF/Gaussian
K(x, x′) = exp

−∥x −x′∥2
2σ2

Exponential
K(x, x′) = exp(−γ∥x −x′∥),
γ &gt; 0
Why Valid? All can be shown to correspond to inner
products in (possibly infinite-dimensional) feature
spaces.
Invalid Kernels:
Negative Power
K(x, x′) = (xT x′)−1
Not positive semi-definite.
Logarithmic
K(x, x′) = log(xT x′ + 1)
Can produce negative eigenvalues.
Checking Validity:
1. Theoretical: Prove positive semi-definiteness
2. Computational: Check eigenvalues of kernel
matrix
3. Construction: Build from known valid kernels
Practical Note
Most standard kernels in ML libraries are guaranteed
to be valid. Custom kernels need verification.
22

</div>
</div>
<div class="page">
<div class="page-number">Page 30 of 52</div>
<div class="page-content">
Multiple Kernel Learning

</div>
</div>
<div class="page">
<div class="page-number">Page 31 of 52</div>
<div class="page-content">
Multiple Kernel Learning (MKL)
Motivation
Different kernels capture different aspects of data:
• RBF kernel: local similarities
• Linear kernel: global structure
• Polynomial kernel: feature interactions
Idea: Combine multiple kernels to get better
performance than any single kernel.
Linear Combination:
K(x, x′) =
M
X
m=1
βmKm(x, x′)
where βm ≥0 and PM
m=1 βm = 1.
Applications:
• Multi-modal data (text + images)
• Feature selection
• Domain adaptation
MKL Optimization:
Joint Optimization
min
w,b,ξ,β
1
2
M
X
m=1
∥wm∥2
βm
+ C
n
X
i=1
ξi
(33)
subject to
yi
 M
X
m=1
wT
m ϕm(xi) + b
!
≥1 −ξi
(34)
ξi ≥0,
βm ≥0,
M
X
m=1
βm = 1
(35)
Solution Methods:
• Alternating optimization: Fix β, solve for w; fix
w, solve for β
• Semi-definite programming: Convex formulation
• Gradient-based methods: Efficient for
large-scale
Kernel Weight Interpretation:
• βm close to 1: Kernel m is most important
23

</div>
</div>
<div class="page">
<div class="page-number">Page 32 of 52</div>
<div class="page-content">
MKL Example: Combining Kernels
Example Setup:
Consider combining three kernels:
K1(x, x′) = xT x′
(Linear)
(36)
K2(x, x′) = (xT x′ + 1)2
(Polynomial)
(37)
K3(x, x′) = exp(−∥x −x′∥2)
(RBF)
(38)
Combined Kernel:
K(x, x′) = β1K1(x, x′) + β2K2(x, x′) + β3K3(x, x′)
Learning Process:
1. Start with equal weights: β1 = β2 = β3 = 1
3
2. Iteratively optimize weights and SVM parameters
3. Converge to optimal combination
Sample Results:
Kernel
Weight
Accuracy
Linear
0.1
0.78
Polynomial
0.3
0.82
RBF
0.6
0.85
Combined MKL
-
0.89
Advantages:
• Better performance than individual kernels
• Automatic selection of relevant kernels
• Interpretability through weights
Challenges:
• Increased computational complexity
• More hyperparameters to tune
• Risk of overfitting with many kernels
24

</div>
</div>
<div class="page">
<div class="page-number">Page 33 of 52</div>
<div class="page-content">
Multi-class Classification

</div>
</div>
<div class="page">
<div class="page-number">Page 34 of 52</div>
<div class="page-content">
Multi-class SVM Strategies
One-vs-Rest (OvR)
• Train k binary classifiers
• Each separates one class from
all others
• Prediction: class with highest
score
One-vs-One (OvO)
• Train
 k
2

binary classifiers
• Each separates pair of classes
• Prediction: majority voting
Direct Multiclass
• Single optimization problem
• Simultaneous separation
• More complex but unified
25

</div>
</div>
<div class="page">
<div class="page-number">Page 35 of 52</div>
<div class="page-content">
One-vs-Rest Detailed Analysis
OvR Strategy:
For k classes:
Advantages:
Si
l
i
l
t ti
26

</div>
</div>
<div class="page">
<div class="page-number">Page 36 of 52</div>
<div class="page-content">
Multi-class Kernel Comparison
Performance:
• Linear: High-dim data
• Polynomial: Feature interactions
• RBF: Most flexible
Selection Criteria:
• Dataset size
• Computational cost
• Cross-validation
Best Practice
Start with RBF kernel and tune C, γ via cross-validation.
27

</div>
</div>
<div class="page">
<div class="page-number">Page 37 of 52</div>
<div class="page-content">
Kernel Methods for Regression

</div>
</div>
<div class="page">
<div class="page-number">Page 38 of 52</div>
<div class="page-content">
Support Vector Regression (SVR)
SVR Concept
Extend SVM to regression by finding a function that
deviates from target values by at most ϵ, while
being as flat as possible.
Linear SVR:
f (x) = wT x + b
Optimization Problem:
min
w,b,ξ,ξ∗
1
2 ∥w∥2 + C
n
X
i=1
(ξi + ξ∗
i )
(39)
subject to
yi −wT xi −b ≤ϵ + ξi
(40)
wT xi + b −yi ≤ϵ + ξ∗
i
(41)
ξi, ξ∗
i ≥0
(42)
ϵ-insensitive Loss:
Lϵ(y, f (x)) = max(0, |y −f (x)| −ϵ)
Key Parameters:
• ϵ: Width of insensitive zone
• C: Regularization parameter
• Kernel parameters: γ for RBF, etc.
Dual Formulation:
f (x) =
n
X
i=1
(αi −α∗
i )K(xi, x) + b
(43)
where αi, α∗
i ≥0 are Lagrange multipliers.
Support Vectors:
• Points outside ϵ-tube
• αi &gt; 0 or α∗
i &gt; 0
• Determine the regression function
Sparsity
Many αi = α∗
i = 0, leading to sparse solutions.
28

</div>
</div>
<div class="page">
<div class="page-number">Page 39 of 52</div>
<div class="page-content">
SVR Demonstration
Linear SVR
• Simple linear relationship
• Good for linear trends
• Fast computation
Polynomial SVR
• Captures polynomial trends
• Risk of overfitting
• Degree selection important
RBF SVR
• Most flexible
• Handles non-linear patterns
• Requires parameter tuning
29

</div>
</div>
<div class="page">
<div class="page-number">Page 40 of 52</div>
<div class="page-content">
Effect of ϵ Parameter in SVR
ϵ Parameter Effects:
• Small ϵ (0.01): Tight fit, many support vectors
• Medium ϵ (0.1): Balanced complexity
• Large ϵ (0.5): Loose fit, fewer support vectors
Trade-offs:
• Small ϵ: Low bias, high variance
• Large ϵ: High bias, low variance
• Sparsity: Larger ϵ ⇒fewer support vectors
Selection Guidelines:
• Cross-validation for optimal ϵ
• Consider noise level in data
• Balance accuracy vs complexity
Practical Values:
• Start with ϵ = 0.1
• Scale with target variable range
• Grid search with C and kernel parameters
Rule of Thumb
30

</div>
</div>
<div class="page">
<div class="page-number">Page 41 of 52</div>
<div class="page-content">
Support Vector Regression

</div>
</div>
<div class="page">
<div class="page-number">Page 42 of 52</div>
<div class="page-content">
Kernel Ridge Regression vs SVR
31

</div>
</div>
<div class="page">
<div class="page-number">Page 43 of 52</div>
<div class="page-content">
Kernel Ridge Regression vs SVR: Comparison
Kernel Ridge
Objective:
min
α ∥Kα −y∥2 + λαT Kα
Solution:
α = (K + λI)−1y
Properties:
• Non-sparse
• Closed-form
• Fast for small data
SVR
Objective:
min
w,b,ξ
1
2 ∥w∥2 + C
X
i
(ξi + ξ∗
i )
Constraints:
|yi −f (xi)| ≤ϵ + ξi
Properties:
• Sparse (SVs)
• Robust to outliers
• Quadratic program
32

</div>
</div>
<div class="page">
<div class="page-number">Page 44 of 52</div>
<div class="page-content">
Kernel Ridge Regression

</div>
</div>
<div class="page">
<div class="page-number">Page 45 of 52</div>
<div class="page-content">
Regularization in Kernel Regression
Kernel Ridge (α):
• Small: Overfitting risk
• Medium: Balanced
• Large: Underfitting risk
min
f
X
i
(yi −f (xi))2 + α∥f ∥2
SVR (C):
• High: Complex model
• Medium: Balanced
• Low: Simple model
Relationship: C ≈1
α
33

</div>
</div>
<div class="page">
<div class="page-number">Page 46 of 52</div>
<div class="page-content">
Worked Example: RBF Kernel Computation
Problem Setup:
Given two points:
x1 = (1, 2)
(44)
x2 = (3, 1)
(45)
Compute RBF kernel with γ = 0.5:
K(x1, x2) = exp(−γ∥x1 −x2∥2)
Step 1: Compute distance
x1 −x2 = (1, 2) −(3, 1) = (−2, 1)
(46)
∥x1 −x2∥2 = (−2)2 + 12 = 4 + 1 = 5
(47)
Step 2: Apply kernel
K(x1, x2) = exp(−0.5 × 5)
(48)
= exp(−2.5)
(49)
≈0.082
(50)
Interpretation:
• Points are moderately far apart
• Kernel value is small (0.082)
• Indicates low similarity
Compare with closer points:
For x1 = (1, 2) and x3 = (1.1, 2.1):
∥x1 −x3∥2 = (0.1)2 + (0.1)2 = 0.02
(51)
K(x1, x3) = exp(−0.5 × 0.02) = exp(−0.01)
(52)
≈0.99
(53)
Effect of γ:
• Large γ: Rapid decay, local influence
• Small γ: Slow decay, global influence
Key Insight
RBF kernel measures similarity through Euclidean
distance in input space.
34

</div>
</div>
<div class="page">
<div class="page-number">Page 47 of 52</div>
<div class="page-content">
Practical Implementation Tips
Data Preprocessing:
• Feature Scaling: Critical for RBF kernels
• Normalization: StandardScaler or MinMaxScaler
• Missing Values: Handle before kernel
computation
Hyperparameter Tuning:
Grid Search Example
param grid = {
’C’: [0.1, 1, 10, 100],
’gamma’:
[0.001, 0.01, 0.1, 1],
’kernel’:
[’rbf’, ’poly’, ’linear’]
}
Performance Considerations:
• Linear kernel: O(n × d)
• RBF kernel: O(n × d) per evaluation
• Training complexity: O(n2) to O(n3)
Model Selection:
1. Start with RBF kernel
2. Use cross-validation
3. Compare with linear kernel
4. Consider computational constraints
Common Pitfalls:
Avoid These
• Forgetting to scale features
• Using default parameters
• Ignoring class imbalance
• Overfitting with complex kernels
Debugging Tips:
• Check kernel matrix properties
• Visualize decision boundaries
• Monitor support vector counts
• Validate on holdout set
Software Libraries:
35

</div>
</div>
<div class="page">
<div class="page-number">Page 48 of 52</div>
<div class="page-content">
Parametric vs Non-parametric
Models

</div>
</div>
<div class="page">
<div class="page-number">Page 49 of 52</div>
<div class="page-content">
Understanding Model Types
Parametric Models:
Definition
Fixed number of parameters independent of training
set size. Make strong assumptions about functional
form.
Examples:
• Linear Regression: f (x) = wT x + b
• Logistic Regression: p = σ(wT x + b)
• Perceptron: Fixed decision boundary
• Neural Networks: Fixed architecture
Characteristics:
• Fast training and prediction
• Strong inductive bias
• May underfit complex data
• Interpretable parameters
Non-parametric Models:
Definition
Number of parameters grows with training data size.
Make minimal assumptions about functional form.
Examples:
• k-NN: Stores all training data
• Decision Trees: Adaptive structure
• Kernel Methods: Support vector representation
• Gaussian Processes: Infinite parameters
Characteristics:
• Flexible representation
• Can fit complex patterns
• Risk of overfitting
• Higher computational cost
36

</div>
</div>
<div class="page">
<div class="page-number">Page 50 of 52</div>
<div class="page-content">
Kernel Methods: The Non-parametric Perspective
Why SVMs are Non-parametric:
Key Insight
SVM decision function depends on support vectors,
whose number grows with data complexity, not fixed
in advance.
Decision Function:
f (x) =
X
i∈SV
αiyiK(xi, x) + b
• Number of support vectors |SV | varies
• Complex data ⇒more support vectors
• Simple data ⇒fewer support vectors
Adaptive Complexity:
• Model complexity adapts to data
• Automatic feature selection
• Sparse representation via support vectors
Comparison with Other Methods:
Parametric Linear Classifier
f (x) = wT x + b
Fixed d + 1 parameters regardless of training set
size.
Non-parametric SVM
f (x) =
nsv
X
i=1
αiyiK(xi, x) + b
nsv support vectors determined by data.
Benefits of Non-parametric Approach:
• Flexibility: No assumptions about decision
boundary shape
• Universality: Can approximate any function
(with appropriate kernel)
• Robustness: Less sensitive to model specification
37

</div>
</div>
<div class="page">
<div class="page-number">Page 51 of 52</div>
<div class="page-content">
Kernel Functions and Function Spaces
Reproducing Kernel Hilbert Space (RKHS):
Mathematical Framework
Kernel K defines an infinite-dimensional feature
space H where linear methods become non-linear in
original space.
Key Properties:
ϕ : X →H
(54)
K(x, x′) = ⟨ϕ(x), ϕ(x′)⟩H
(55)
Universal Approximation:
• RBF kernels are universal approximators
• Can represent any continuous function
• Given sufficient training data
Non-parametric Power
Kernel methods can learn arbitrarily complex
decision boundaries without specifying the form in
advance.
Practical Implications:
Model Selection Strategy:
• Start Simple: Linear kernel first
• Add Complexity: Polynomial →RBF
• Cross-validate: Choose optimal kernel and
parameters
Trade-offs:
Parametric Advantage
• Fast training and prediction
• Lower memory requirements
• Better interpretability
Non-parametric Advantage
• Higher representational power
• Better fit to complex data
• Fewer modeling assumptions
When to Use Kernel Methods:
• Non-linear relationships in data
38

</div>
</div>
<div class="page">
<div class="page-number">Page 52 of 52</div>
<div class="page-content">
Summary and Key Takeaways
Core Concepts Learned:
• Kernel Trick: Implicit high-dimensional mapping
• Support Vectors: Sparse representation
• Margin Maximization: Generalization principle
• Non-linear Separation: Via kernels
Main Algorithms:
• SVM: Classification with maximum margin
• SVR: Regression with ϵ-insensitive loss
• Kernel Ridge: Regularized regression
• Multi-class: Extensions to multiple classes
Key Kernels:
• Linear, Polynomial, RBF, Sigmoid
• Mercer’s theorem for validity
• Multiple kernel learning
Practical Guidelines:
When to Use Kernel Methods
• Non-linear relationships in data
• Need for sparse solutions
• Strong theoretical guarantees required
• Medium-sized datasets
Parameter Selection:
• C: Start with 1.0, tune via CV
• γ: Start with
1
n features
• ϵ: Start with 0.1 for SVR
Limitations:
• Computational complexity: O(n2) to O(n3)
• Memory requirements: Store kernel matrix
• Parameter sensitivity
• Not suitable for very large datasets
Next Steps
Explore deep learning for automatic feature learning
i
hi h di
i
l
39

</div>
</div>

</body>
</html>
