{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yourusername/CMSC173/blob/main/tex/eda_lecture/eda_companion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "title"
      },
      "source": [
        "# Exploratory Data Analysis (EDA) - Companion Notebook\n",
        "\n",
        "**Course**: CMSC 173 - Data Science for Computer Scientists  \n",
        "**Topic**: Comprehensive EDA with the Titanic Dataset  \n",
        "**Companion to**: EDA Lecture Slides\n",
        "\n",
        "---\n",
        "\n",
        "This notebook provides hands-on implementation of concepts covered in the EDA lecture slides. We'll work through the complete data analysis pipeline using the famous Titanic dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "setup"
      },
      "source": [
        "## üìö Setup and Imports\n",
        "\n",
        "First, let's import all necessary libraries for our EDA journey."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "imports"
      },
      "outputs": [],
      "source": [
        "# Data manipulation and analysis\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Visualization libraries\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "# Statistical analysis\n",
        "from scipy import stats\n",
        "from scipy.stats import chi2_contingency\n",
        "\n",
        "# Machine learning libraries\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.feature_selection import SelectKBest, f_classif\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "# Ignore warnings for cleaner output\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set plotting style\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "print(\"‚úÖ All libraries imported successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "data_loading"
      },
      "source": [
        "## üö¢ Data Loading and Initial Exploration\n",
        "\n",
        "Let's load the Titanic dataset and perform our initial exploration as discussed in **Slide 2** of the lecture."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "load_data"
      },
      "outputs": [],
      "source": [
        "# Load the Titanic dataset\n",
        "# Using seaborn's built-in dataset for convenience\n",
        "titanic = sns.load_dataset('titanic')\n",
        "\n",
        "print(\"üìä Dataset loaded successfully!\")\n",
        "print(f\"Dataset shape: {titanic.shape}\")\n",
        "print(f\"Columns: {list(titanic.columns)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "first_look"
      },
      "outputs": [],
      "source": [
        "# First look at the data\n",
        "print(\"üîç First 5 rows of the dataset:\")\n",
        "display(titanic.head())\n",
        "\n",
        "print(\"\\nüìà Dataset information:\")\n",
        "titanic.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "data_types"
      },
      "source": [
        "## üè∑Ô∏è Data Types Analysis (Slide 3)\n",
        "\n",
        "Let's examine and categorize our data types as shown in the lecture slides."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "analyze_types"
      },
      "outputs": [],
      "source": [
        "# Create a comprehensive data type analysis\n",
        "def analyze_data_types(df):\n",
        "    analysis = []\n",
        "    \n",
        "    for col in df.columns:\n",
        "        dtype = df[col].dtype\n",
        "        unique_count = df[col].nunique()\n",
        "        null_count = df[col].isnull().sum()\n",
        "        \n",
        "        # Determine data type category\n",
        "        if dtype in ['int64', 'float64']:\n",
        "            if unique_count == 2:\n",
        "                category = 'Binary'\n",
        "            elif unique_count < 10:\n",
        "                category = 'Discrete'\n",
        "            else:\n",
        "                category = 'Continuous'\n",
        "        else:\n",
        "            if unique_count < 10:\n",
        "                category = 'Nominal'\n",
        "            else:\n",
        "                category = 'Text/Identifier'\n",
        "        \n",
        "        analysis.append({\n",
        "            'Column': col,\n",
        "            'Data Type': str(dtype),\n",
        "            'Category': category,\n",
        "            'Unique Values': unique_count,\n",
        "            'Missing Values': null_count,\n",
        "            'Missing %': round(null_count/len(df)*100, 2)\n",
        "        })\n",
        "    \n",
        "    return pd.DataFrame(analysis)\n",
        "\n",
        "# Analyze our Titanic dataset\n",
        "type_analysis = analyze_data_types(titanic)\n",
        "print(\"üè∑Ô∏è Data Types Analysis:\")\n",
        "display(type_analysis)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "missing_data"
      },
      "source": [
        "## üîç Missing Data Analysis (Slide 8)\n",
        "\n",
        "Understanding and handling missing data is crucial for effective EDA."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "missing_analysis"
      },
      "outputs": [],
      "source": [
        "# Missing data visualization\n",
        "def plot_missing_data(df):\n",
        "    # Calculate missing data\n",
        "    missing_data = df.isnull().sum()\n",
        "    missing_percent = (missing_data / len(df)) * 100\n",
        "    \n",
        "    # Create visualization\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
        "    \n",
        "    # Missing data counts\n",
        "    missing_data[missing_data > 0].plot(kind='bar', ax=ax1, color='coral')\n",
        "    ax1.set_title('Missing Data Count by Column')\n",
        "    ax1.set_ylabel('Number of Missing Values')\n",
        "    ax1.tick_params(axis='x', rotation=45)\n",
        "    \n",
        "    # Missing data percentages\n",
        "    missing_percent[missing_percent > 0].plot(kind='bar', ax=ax2, color='lightblue')\n",
        "    ax2.set_title('Missing Data Percentage by Column')\n",
        "    ax2.set_ylabel('Percentage of Missing Values')\n",
        "    ax2.tick_params(axis='x', rotation=45)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    return missing_data[missing_data > 0], missing_percent[missing_percent > 0]\n",
        "\n",
        "print(\"üîç Missing Data Analysis:\")\n",
        "missing_counts, missing_percentages = plot_missing_data(titanic)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "missing_patterns"
      },
      "outputs": [],
      "source": [
        "# Missing data patterns analysis\n",
        "print(\"üìä Missing Data Summary:\")\n",
        "for col in missing_counts.index:\n",
        "    count = missing_counts[col]\n",
        "    percentage = missing_percentages[col]\n",
        "    print(f\"{col}: {count} missing values ({percentage:.1f}%)\")\n",
        "    \n",
        "    # Suggest handling strategy\n",
        "    if percentage < 5:\n",
        "        strategy = \"Consider removing rows or simple imputation\"\n",
        "    elif percentage < 20:\n",
        "        strategy = \"Use advanced imputation methods\"\n",
        "    else:\n",
        "        strategy = \"Consider dropping column or creating 'missing' indicator\"\n",
        "    print(f\"  ‚Üí Suggested strategy: {strategy}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "univariate"
      },
      "source": [
        "## üìä Univariate Analysis (Slides 4-5)\n",
        "\n",
        "Let's explore individual variables through various visualization techniques."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "numerical_univariate"
      },
      "outputs": [],
      "source": [
        "# Numerical variables analysis\n",
        "numerical_cols = ['age', 'fare', 'sibsp', 'parch']\n",
        "\n",
        "fig, axes = plt.subplots(2, 4, figsize=(20, 10))\n",
        "fig.suptitle('Univariate Analysis - Numerical Variables', fontsize=16, fontweight='bold')\n",
        "\n",
        "for i, col in enumerate(numerical_cols):\n",
        "    # Histogram\n",
        "    axes[0, i].hist(titanic[col].dropna(), bins=30, alpha=0.7, color='skyblue', edgecolor='black')\n",
        "    axes[0, i].set_title(f'Distribution of {col.title()}')\n",
        "    axes[0, i].set_xlabel(col.title())\n",
        "    axes[0, i].set_ylabel('Frequency')\n",
        "    \n",
        "    # Box plot\n",
        "    axes[1, i].boxplot(titanic[col].dropna(), patch_artist=True,\n",
        "                      boxprops=dict(facecolor='lightgreen', alpha=0.7))\n",
        "    axes[1, i].set_title(f'Box Plot of {col.title()}')\n",
        "    axes[1, i].set_ylabel(col.title())\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "categorical_univariate"
      },
      "outputs": [],
      "source": [
        "# Categorical variables analysis\n",
        "categorical_cols = ['survived', 'pclass', 'sex', 'embarked']\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "fig.suptitle('Univariate Analysis - Categorical Variables', fontsize=16, fontweight='bold')\n",
        "\n",
        "for i, col in enumerate(categorical_cols):\n",
        "    row = i // 2\n",
        "    col_idx = i % 2\n",
        "    \n",
        "    # Count plot\n",
        "    data_counts = titanic[col].value_counts()\n",
        "    axes[row, col_idx].bar(data_counts.index, data_counts.values, \n",
        "                          color=['#FF6B6B', '#4ECDC4', '#45B7D1', '#FFA07A'][:len(data_counts)])\n",
        "    axes[row, col_idx].set_title(f'Distribution of {col.title()}')\n",
        "    axes[row, col_idx].set_xlabel(col.title())\n",
        "    axes[row, col_idx].set_ylabel('Count')\n",
        "    \n",
        "    # Add value labels on bars\n",
        "    for j, v in enumerate(data_counts.values):\n",
        "        axes[row, col_idx].text(j, v + 5, str(v), ha='center', va='bottom')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bivariate"
      },
      "source": [
        "## üîó Bivariate Analysis (Slide 6)\n",
        "\n",
        "Exploring relationships between variables to uncover insights."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "correlation_matrix"
      },
      "outputs": [],
      "source": [
        "# Correlation matrix for numerical variables\n",
        "numerical_data = titanic[['survived', 'pclass', 'age', 'sibsp', 'parch', 'fare']].corr()\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "mask = np.triu(np.ones_like(numerical_data, dtype=bool))\n",
        "sns.heatmap(numerical_data, mask=mask, annot=True, cmap='RdYlBu_r', center=0,\n",
        "            square=True, linewidths=0.5, cbar_kws={\"shrink\": .8})\n",
        "plt.title('Correlation Matrix - Numerical Variables', fontsize=14, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"üîç Key Correlations with Survival:\")\n",
        "survival_corr = numerical_data['survived'].sort_values(key=abs, ascending=False)[1:]\n",
        "for var, corr in survival_corr.items():\n",
        "    print(f\"{var.title()}: {corr:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "survival_analysis"
      },
      "outputs": [],
      "source": [
        "# Survival analysis by different factors\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "fig.suptitle('Survival Analysis by Key Factors', fontsize=16, fontweight='bold')\n",
        "\n",
        "# Survival by Gender\n",
        "survival_by_sex = titanic.groupby(['sex', 'survived']).size().unstack()\n",
        "survival_by_sex.plot(kind='bar', ax=axes[0,0], color=['#FF6B6B', '#4ECDC4'])\n",
        "axes[0,0].set_title('Survival by Gender')\n",
        "axes[0,0].set_xlabel('Gender')\n",
        "axes[0,0].set_ylabel('Count')\n",
        "axes[0,0].legend(['Did not survive', 'Survived'])\n",
        "axes[0,0].tick_params(axis='x', rotation=0)\n",
        "\n",
        "# Survival by Class\n",
        "survival_by_class = titanic.groupby(['pclass', 'survived']).size().unstack()\n",
        "survival_by_class.plot(kind='bar', ax=axes[0,1], color=['#FF6B6B', '#4ECDC4'])\n",
        "axes[0,1].set_title('Survival by Passenger Class')\n",
        "axes[0,1].set_xlabel('Passenger Class')\n",
        "axes[0,1].set_ylabel('Count')\n",
        "axes[0,1].legend(['Did not survive', 'Survived'])\n",
        "axes[0,1].tick_params(axis='x', rotation=0)\n",
        "\n",
        "# Age distribution by survival\n",
        "survived = titanic[titanic['survived'] == 1]['age'].dropna()\n",
        "not_survived = titanic[titanic['survived'] == 0]['age'].dropna()\n",
        "axes[1,0].hist([not_survived, survived], bins=30, alpha=0.7, \n",
        "              label=['Did not survive', 'Survived'], color=['#FF6B6B', '#4ECDC4'])\n",
        "axes[1,0].set_title('Age Distribution by Survival')\n",
        "axes[1,0].set_xlabel('Age')\n",
        "axes[1,0].set_ylabel('Frequency')\n",
        "axes[1,0].legend()\n",
        "\n",
        "# Fare distribution by survival\n",
        "survived_fare = titanic[titanic['survived'] == 1]['fare'].dropna()\n",
        "not_survived_fare = titanic[titanic['survived'] == 0]['fare'].dropna()\n",
        "axes[1,1].hist([not_survived_fare, survived_fare], bins=50, alpha=0.7,\n",
        "              label=['Did not survive', 'Survived'], color=['#FF6B6B', '#4ECDC4'])\n",
        "axes[1,1].set_title('Fare Distribution by Survival')\n",
        "axes[1,1].set_xlabel('Fare')\n",
        "axes[1,1].set_ylabel('Frequency')\n",
        "axes[1,1].legend()\n",
        "axes[1,1].set_xlim(0, 200)  # Limit x-axis for better visualization\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "feature_engineering"
      },
      "source": [
        "## üîß Feature Engineering (Slide 9)\n",
        "\n",
        "Creating new features and transforming existing ones to improve our analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "create_features"
      },
      "outputs": [],
      "source": [
        "# Create a copy for feature engineering\n",
        "titanic_fe = titanic.copy()\n",
        "\n",
        "# 1. Family size feature\n",
        "titanic_fe['family_size'] = titanic_fe['sibsp'] + titanic_fe['parch'] + 1\n",
        "\n",
        "# 2. Is alone feature\n",
        "titanic_fe['is_alone'] = (titanic_fe['family_size'] == 1).astype(int)\n",
        "\n",
        "# 3. Age groups\n",
        "def categorize_age(age):\n",
        "    if pd.isna(age):\n",
        "        return 'Unknown'\n",
        "    elif age < 12:\n",
        "        return 'Child'\n",
        "    elif age < 18:\n",
        "        return 'Teen'\n",
        "    elif age < 65:\n",
        "        return 'Adult'\n",
        "    else:\n",
        "        return 'Senior'\n",
        "\n",
        "titanic_fe['age_group'] = titanic_fe['age'].apply(categorize_age)\n",
        "\n",
        "# 4. Fare categories\n",
        "titanic_fe['fare_category'] = pd.cut(titanic_fe['fare'], \n",
        "                                    bins=[0, 7.9, 14.5, 31.0, float('inf')],\n",
        "                                    labels=['Low', 'Medium', 'High', 'Very High'])\n",
        "\n",
        "# 5. Title extraction from name\n",
        "titanic_fe['title'] = titanic_fe['who'].str.title()\n",
        "\n",
        "print(\"üîß New Features Created:\")\n",
        "new_features = ['family_size', 'is_alone', 'age_group', 'fare_category', 'title']\n",
        "for feature in new_features:\n",
        "    print(f\"‚úÖ {feature}\")\n",
        "\n",
        "print(\"\\nüìä Sample of engineered features:\")\n",
        "display(titanic_fe[['survived'] + new_features].head(10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "feature_impact"
      },
      "outputs": [],
      "source": [
        "# Analyze impact of new features on survival\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "fig.suptitle('Impact of Engineered Features on Survival', fontsize=16, fontweight='bold')\n",
        "\n",
        "# Family size vs survival\n",
        "family_survival = titanic_fe.groupby('family_size')['survived'].mean()\n",
        "axes[0,0].bar(family_survival.index, family_survival.values, color='lightcoral')\n",
        "axes[0,0].set_title('Survival Rate by Family Size')\n",
        "axes[0,0].set_xlabel('Family Size')\n",
        "axes[0,0].set_ylabel('Survival Rate')\n",
        "\n",
        "# Age group vs survival\n",
        "age_survival = titanic_fe.groupby('age_group')['survived'].mean()\n",
        "axes[0,1].bar(age_survival.index, age_survival.values, color='lightblue')\n",
        "axes[0,1].set_title('Survival Rate by Age Group')\n",
        "axes[0,1].set_xlabel('Age Group')\n",
        "axes[0,1].set_ylabel('Survival Rate')\n",
        "axes[0,1].tick_params(axis='x', rotation=45)\n",
        "\n",
        "# Fare category vs survival\n",
        "fare_survival = titanic_fe.groupby('fare_category')['survived'].mean()\n",
        "axes[1,0].bar(fare_survival.index, fare_survival.values, color='lightgreen')\n",
        "axes[1,0].set_title('Survival Rate by Fare Category')\n",
        "axes[1,0].set_xlabel('Fare Category')\n",
        "axes[1,0].set_ylabel('Survival Rate')\n",
        "\n",
        "# Title vs survival\n",
        "title_survival = titanic_fe.groupby('title')['survived'].mean()\n",
        "axes[1,1].bar(title_survival.index, title_survival.values, color='gold')\n",
        "axes[1,1].set_title('Survival Rate by Title')\n",
        "axes[1,1].set_xlabel('Title')\n",
        "axes[1,1].set_ylabel('Survival Rate')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "feature_selection"
      },
      "source": [
        "## üéØ Feature Selection (Slide 7)\n",
        "\n",
        "Identifying the most important features for predicting survival."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "prepare_ml_data"
      },
      "outputs": [],
      "source": [
        "# Prepare data for machine learning\n",
        "# Handle missing values and encode categorical variables\n",
        "\n",
        "ml_data = titanic_fe.copy()\n",
        "\n",
        "# Fill missing ages with median\n",
        "ml_data['age'].fillna(ml_data['age'].median(), inplace=True)\n",
        "\n",
        "# Fill missing embarked with mode\n",
        "ml_data['embarked'].fillna(ml_data['embarked'].mode()[0], inplace=True)\n",
        "\n",
        "# Encode categorical variables\n",
        "le = LabelEncoder()\n",
        "categorical_columns = ['sex', 'embarked', 'age_group', 'fare_category', 'title']\n",
        "\n",
        "for col in categorical_columns:\n",
        "    ml_data[col + '_encoded'] = le.fit_transform(ml_data[col].astype(str))\n",
        "\n",
        "# Select features for analysis\n",
        "feature_columns = ['pclass', 'age', 'sibsp', 'parch', 'fare', 'family_size', 'is_alone'] + \\\n",
        "                 [col + '_encoded' for col in categorical_columns]\n",
        "\n",
        "X = ml_data[feature_columns]\n",
        "y = ml_data['survived']\n",
        "\n",
        "print(f\"üéØ Feature matrix shape: {X.shape}\")\n",
        "print(f\"üìä Target variable shape: {y.shape}\")\n",
        "print(f\"‚úÖ Features selected: {len(feature_columns)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "feature_importance"
      },
      "outputs": [],
      "source": [
        "# Feature importance using Random Forest\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf.fit(X, y)\n",
        "\n",
        "# Get feature importances\n",
        "feature_importance = pd.DataFrame({\n",
        "    'Feature': X.columns,\n",
        "    'Importance': rf.feature_importances_\n",
        "}).sort_values('Importance', ascending=False)\n",
        "\n",
        "# Plot feature importance\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.barplot(data=feature_importance.head(10), x='Importance', y='Feature', palette='viridis')\n",
        "plt.title('Top 10 Most Important Features (Random Forest)', fontsize=14, fontweight='bold')\n",
        "plt.xlabel('Feature Importance')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"üèÜ Top 10 Most Important Features:\")\n",
        "for i, (_, row) in enumerate(feature_importance.head(10).iterrows(), 1):\n",
        "    print(f\"{i:2d}. {row['Feature']:20s}: {row['Importance']:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "statistical_selection"
      },
      "outputs": [],
      "source": [
        "# Statistical feature selection\n",
        "selector = SelectKBest(score_func=f_classif, k=10)\n",
        "X_selected = selector.fit_transform(X, y)\n",
        "\n",
        "# Get selected features\n",
        "selected_features = X.columns[selector.get_support()]\n",
        "feature_scores = pd.DataFrame({\n",
        "    'Feature': X.columns,\n",
        "    'Score': selector.scores_,\n",
        "    'Selected': selector.get_support()\n",
        "}).sort_values('Score', ascending=False)\n",
        "\n",
        "print(\"üìä Statistical Feature Selection (F-score):\")\n",
        "print(f\"Selected {len(selected_features)} features out of {len(X.columns)}\")\n",
        "print(\"\\nüèÜ Top features by F-score:\")\n",
        "for i, (_, row) in enumerate(feature_scores.head(10).iterrows(), 1):\n",
        "    status = \"‚úÖ\" if row['Selected'] else \"‚ùå\"\n",
        "    print(f\"{i:2d}. {status} {row['Feature']:20s}: {row['Score']:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "normalization"
      },
      "source": [
        "## üìè Data Normalization (Slide 10)\n",
        "\n",
        "Scaling features to ensure they're on similar scales for machine learning algorithms."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "normalization_comparison"
      },
      "outputs": [],
      "source": [
        "# Compare different normalization techniques\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler\n",
        "\n",
        "# Select numerical features for normalization\n",
        "numerical_features = ['age', 'fare', 'family_size']\n",
        "original_data = ml_data[numerical_features]\n",
        "\n",
        "# Apply different scaling techniques\n",
        "scalers = {\n",
        "    'Original': None,\n",
        "    'StandardScaler': StandardScaler(),\n",
        "    'MinMaxScaler': MinMaxScaler(),\n",
        "    'RobustScaler': RobustScaler()\n",
        "}\n",
        "\n",
        "fig, axes = plt.subplots(len(numerical_features), len(scalers), figsize=(20, 12))\n",
        "fig.suptitle('Comparison of Normalization Techniques', fontsize=16, fontweight='bold')\n",
        "\n",
        "for col_idx, feature in enumerate(numerical_features):\n",
        "    for scaler_idx, (scaler_name, scaler) in enumerate(scalers.items()):\n",
        "        if scaler is None:\n",
        "            data_to_plot = original_data[feature]\n",
        "        else:\n",
        "            scaled_data = scaler.fit_transform(original_data[[feature]])\n",
        "            data_to_plot = scaled_data.flatten()\n",
        "        \n",
        "        axes[col_idx, scaler_idx].hist(data_to_plot, bins=30, alpha=0.7, \n",
        "                                      color=plt.cm.Set3(scaler_idx))\n",
        "        axes[col_idx, scaler_idx].set_title(f'{feature.title()} - {scaler_name}')\n",
        "        axes[col_idx, scaler_idx].set_ylabel('Frequency')\n",
        "        \n",
        "        # Add statistics\n",
        "        mean_val = np.mean(data_to_plot)\n",
        "        std_val = np.std(data_to_plot)\n",
        "        axes[col_idx, scaler_idx].axvline(mean_val, color='red', linestyle='--', alpha=0.7)\n",
        "        axes[col_idx, scaler_idx].text(0.02, 0.95, f'Œº={mean_val:.2f}\\nœÉ={std_val:.2f}', \n",
        "                                     transform=axes[col_idx, scaler_idx].transAxes, \n",
        "                                     verticalalignment='top',\n",
        "                                     bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ml_pipeline"
      },
      "source": [
        "## üöÄ Machine Learning Pipeline (Slide 11)\n",
        "\n",
        "Putting it all together in a complete ML pipeline from EDA insights to model evaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ml_pipeline_implementation"
      },
      "outputs": [],
      "source": [
        "# Complete ML Pipeline based on EDA insights\n",
        "\n",
        "# 1. Use top features identified in feature selection\n",
        "top_features = feature_importance.head(8)['Feature'].tolist()\n",
        "X_final = X[top_features]\n",
        "\n",
        "# 2. Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_final, y, test_size=0.2, \n",
        "                                                    random_state=42, stratify=y)\n",
        "\n",
        "# 3. Scale the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# 4. Train models\n",
        "models = {\n",
        "    'Logistic Regression': LogisticRegression(random_state=42),\n",
        "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "}\n",
        "\n",
        "results = {}\n",
        "for model_name, model in models.items():\n",
        "    # Train model\n",
        "    if model_name == 'Logistic Regression':\n",
        "        model.fit(X_train_scaled, y_train)\n",
        "        y_pred = model.predict(X_test_scaled)\n",
        "    else:\n",
        "        model.fit(X_train, y_train)\n",
        "        y_pred = model.predict(X_test)\n",
        "    \n",
        "    # Calculate accuracy\n",
        "    accuracy = (y_pred == y_test).mean()\n",
        "    results[model_name] = {'accuracy': accuracy, 'predictions': y_pred}\n",
        "    \n",
        "    print(f\"üéØ {model_name} Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "print(f\"\\nüìä Training set size: {len(X_train)}\")\n",
        "print(f\"üìä Test set size: {len(X_test)}\")\n",
        "print(f\"üéØ Features used: {len(top_features)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "model_evaluation"
      },
      "outputs": [],
      "source": [
        "# Detailed model evaluation\n",
        "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
        "\n",
        "for idx, (model_name, result) in enumerate(results.items()):\n",
        "    y_pred = result['predictions']\n",
        "    \n",
        "    # Confusion Matrix\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[idx])\n",
        "    axes[idx].set_title(f'Confusion Matrix - {model_name}')\n",
        "    axes[idx].set_xlabel('Predicted')\n",
        "    axes[idx].set_ylabel('Actual')\n",
        "    \n",
        "    # Print classification report\n",
        "    print(f\"\\nüìä {model_name} - Detailed Results:\")\n",
        "    print(classification_report(y_test, y_pred, target_names=['Did not survive', 'Survived']))\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "insights"
      },
      "source": [
        "## üí° Key Insights and Takeaways\n",
        "\n",
        "Based on our comprehensive EDA of the Titanic dataset, here are the main findings:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "summary_insights"
      },
      "outputs": [],
      "source": [
        "# Generate summary insights\n",
        "print(\"üö¢ TITANIC DATASET - KEY INSIGHTS FROM EDA\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# 1. Survival statistics\n",
        "survival_rate = titanic['survived'].mean()\n",
        "print(f\"\\nüìä Overall Survival Rate: {survival_rate:.1%}\")\n",
        "\n",
        "# 2. Gender impact\n",
        "gender_survival = titanic.groupby('sex')['survived'].mean()\n",
        "print(f\"\\nüë• Survival by Gender:\")\n",
        "for gender, rate in gender_survival.items():\n",
        "    print(f\"   {gender.title()}: {rate:.1%}\")\n",
        "\n",
        "# 3. Class impact\n",
        "class_survival = titanic.groupby('pclass')['survived'].mean()\n",
        "print(f\"\\nüé´ Survival by Class:\")\n",
        "for pclass, rate in class_survival.items():\n",
        "    class_name = ['First', 'Second', 'Third'][pclass-1]\n",
        "    print(f\"   {class_name} Class: {rate:.1%}\")\n",
        "\n",
        "# 4. Age insights\n",
        "child_survival = titanic[titanic['age'] < 16]['survived'].mean()\n",
        "adult_survival = titanic[titanic['age'] >= 16]['survived'].mean()\n",
        "print(f\"\\nüë∂ Children (<16) Survival Rate: {child_survival:.1%}\")\n",
        "print(f\"üë® Adult (‚â•16) Survival Rate: {adult_survival:.1%}\")\n",
        "\n",
        "# 5. Family size impact\n",
        "alone_survival = titanic_fe[titanic_fe['is_alone'] == 1]['survived'].mean()\n",
        "family_survival = titanic_fe[titanic_fe['is_alone'] == 0]['survived'].mean()\n",
        "print(f\"\\nüë§ Traveling Alone Survival Rate: {alone_survival:.1%}\")\n",
        "print(f\"üë®‚Äçüë©‚Äçüëß‚Äçüë¶ Traveling with Family Survival Rate: {family_survival:.1%}\")\n",
        "\n",
        "# 6. Top predictive features\n",
        "print(f\"\\nüèÜ Most Predictive Features:\")\n",
        "for i, feature in enumerate(feature_importance.head(5)['Feature'], 1):\n",
        "    print(f\"   {i}. {feature}\")\n",
        "\n",
        "print(f\"\\nüéØ Best Model Performance:\")\n",
        "best_model = max(results.keys(), key=lambda x: results[x]['accuracy'])\n",
        "best_accuracy = results[best_model]['accuracy']\n",
        "print(f\"   {best_model}: {best_accuracy:.1%} accuracy\")\n",
        "\n",
        "print(\"\\n‚ú® EDA PROCESS COMPLETE! ‚ú®\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "conclusion"
      },
      "source": [
        "## üéì Conclusion\n",
        "\n",
        "This notebook demonstrated a comprehensive EDA workflow that includes:\n",
        "\n",
        "1. **Data Understanding**: Loading and initial exploration\n",
        "2. **Data Types Analysis**: Categorizing variables appropriately\n",
        "3. **Missing Data Handling**: Identifying patterns and strategies\n",
        "4. **Univariate Analysis**: Understanding individual variable distributions\n",
        "5. **Bivariate Analysis**: Exploring relationships between variables\n",
        "6. **Feature Engineering**: Creating meaningful new features\n",
        "7. **Feature Selection**: Identifying the most predictive variables\n",
        "8. **Data Normalization**: Preparing data for machine learning\n",
        "9. **ML Pipeline**: Implementing a complete predictive model\n",
        "\n",
        "### üöÄ Next Steps\n",
        "\n",
        "- Try advanced feature engineering techniques\n",
        "- Experiment with different machine learning algorithms\n",
        "- Perform hyperparameter tuning\n",
        "- Create ensemble models for better prediction accuracy\n",
        "- Apply similar EDA techniques to other datasets\n",
        "\n",
        "### üìö Additional Resources\n",
        "\n",
        "- [Pandas Documentation](https://pandas.pydata.org/docs/)\n",
        "- [Seaborn Gallery](https://seaborn.pydata.org/examples/index.html)\n",
        "- [Scikit-learn User Guide](https://scikit-learn.org/stable/user_guide.html)\n",
        "- [Matplotlib Tutorials](https://matplotlib.org/stable/tutorials/index.html)\n",
        "\n",
        "---\n",
        "\n",
        "**Happy Data Exploring! üéâ**"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}